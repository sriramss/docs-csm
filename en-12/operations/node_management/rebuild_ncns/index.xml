<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rebuild NCNs on Cray System Management (CSM)</title>
    <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/</link>
    <description>Recent content in Rebuild NCNs on Cray System Management (CSM)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 24 Apr 2022 03:13:38 +0000</lastBuildDate><atom:link href="/docs-csm/en-12/operations/node_management/rebuild_ncns/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rebuild NCNs</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/rebuild_ncns/</link>
      <pubDate>Sun, 24 Apr 2022 03:12:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/rebuild_ncns/</guid>
      <description>Rebuild NCNs Rebuild a master, worker, or storage non-compute node (NCN). Use this procedure in the event that a node has a hardware failure, or some other issue with the node has occurred that warrants rebuilding the node.
The following is a high-level overview of the NCN rebuild workflow:
 Prepare Node  There is a different procedure for each type of node (worker, master, and storage)   Identify Node and Update Metadata  Same procedure for all node types   Wipe Disks  Same for master and worker nodes, but different for storage nodes   Power Cycle Node  Same procedure for all node types   Rebuild Storage Node  Only needed for storage nodes   Validate BOOTRAID Artifacts  Run from ncn-m001   Validation  There is a different procedure for each type of node (worker, master, and storage)    Prerequisites The system is fully installed and has transitioned off of the LiveCD.</description>
    </item>
    
    <item>
      <title>Validate Bootraid Artifacts</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/validate_boot_raid/</link>
      <pubDate>Sun, 24 Apr 2022 03:12:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/validate_boot_raid/</guid>
      <description>Validate BOOTRAID Artifacts Perform the following steps on ncn-m001.
  Initialize the cray command and follow the prompts (required for the next step).
ncn-m001# cray init   Run the script to ensure the local BOOTRAID has a valid kernel and initrd.
ncn-m001# /opt/cray/tests/install/ncn/scripts/validate-bootraid-artifacts.sh   Workaround: CASMINST-2015 As a result of rebuilding any NCN(s), remove any dynamically assigned interface IP addresses that did not get released automatically by running the CASMINST-2015 script:</description>
    </item>
    
    <item>
      <title>Wipe Drives</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/wipe_drives/</link>
      <pubDate>Sun, 24 Apr 2022 03:12:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/wipe_drives/</guid>
      <description>Wipe Drives WARNING: This is the point of no return. Once the disks are wiped, the node must be rebuilt.
All commands in this section must be run on the node being rebuilt (unless otherwise indicated). These commands can be done from the ConMan console window.
Only follow the steps in the section for the node type that is being rebuilt:
 Wipe Disks: Master Wipe Disks: Worker Node Wipe Disks: Utility Storage Node  Wipe Disks: Master   Unmount the etcd volume and remove the volume group.</description>
    </item>
    
    <item>
      <title>Final Validation Steps</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/final_validation_steps/</link>
      <pubDate>Sun, 24 Apr 2022 03:12:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/final_validation_steps/</guid>
      <description>Final Validation Steps Use this procedure to finish validating the success of a rebuilt NCN(s).
Procedure   Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.
NOTE: The following command will indicate if a CFS job is currently in progress for this node.
IMPORTANT: The following command assumes that the variables from the prerequisites section have been set.
ncn# cray cfs components describe $XNAME --format json Example output:</description>
    </item>
    
    <item>
      <title>Identify Nodes And Update Metadata</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/identify_nodes_and_update_metadata/</link>
      <pubDate>Sun, 24 Apr 2022 03:12:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/identify_nodes_and_update_metadata/</guid>
      <description>Identify Nodes and Update Metadata Use the following procedure to inspect and modify the Boot Script Service (BSS) boot parameters JSON file.
This section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.
Procedure   Generate the BSS boot parameters JSON file.
Run the following commands from a node that has cray CLI initialized:
ncn# cray bss bootparameters list --name $XNAME --format=json | jq .</description>
    </item>
    
    <item>
      <title>Post Rebuild Storage Node Validation</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/post_rebuild_storage_node_validation/</link>
      <pubDate>Sun, 24 Apr 2022 03:12:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/post_rebuild_storage_node_validation/</guid>
      <description>Post Rebuild Storage Node Validation Validate the storage node rebuilt successfully.
Skip this section if a master or worker node was rebuilt.
Procedure   Verify there are 3 mons, 3 mds, 3 mgr processes, and rgws.
ncn-m# ceph -s Example output:
 cluster: id: 22d01fcd-a75b-4bfc-b286-2ed8645be2b5 health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 4m) mgr: ncn-s001(active, since 19h), standbys: ncn-s002, ncn-s003 mds: cephfs:1 {0=ncn-s001=up:active} 2 up:standby osd: 12 osds: 12 up (since 2m), 12 in (since 2m) rgw: 3 daemons active (ncn-s001.</description>
    </item>
    
    <item>
      <title>Power Cycle And Rebuild Nodes</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/power_cycle_and_rebuild_nodes/</link>
      <pubDate>Sun, 24 Apr 2022 03:12:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/power_cycle_and_rebuild_nodes/</guid>
      <description>Power Cycle and Rebuild Nodes This section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.
Procedure   Open and Watch the console for the node being rebuilt.
  Log in to a second session to use it to watch the console using the instructions at the link below:
Please open this link in a new tab or page Log in to a Node Using ConMan</description>
    </item>
    
    <item>
      <title>Prepare Storage Nodes</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/prepare_storage_nodes/</link>
      <pubDate>Sun, 24 Apr 2022 03:12:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/prepare_storage_nodes/</guid>
      <description>Prepare Storage Nodes Prepare a storage node before rebuilding it.
IMPORTANT: All of the output examples may not reflect the cluster status where this operation is being performed. For example, if this is a rebuild in place, then Ceph components will not be reporting down, in contrast to a failed node rebuild.
Prerequisites If rebuilding ncn-s001, it is critical that the storage-ceph-cloudinit.sh has been removed from the runcmd in BSS.</description>
    </item>
    
    <item>
      <title>Re-add A Storage Node To Ceph</title>
      <link>/docs-csm/en-12/operations/node_management/rebuild_ncns/re-add_storage_node_to_ceph/</link>
      <pubDate>Sun, 24 Apr 2022 03:12:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/node_management/rebuild_ncns/re-add_storage_node_to_ceph/</guid>
      <description>Re-Add a Storage Node to Ceph Use the following procedure to re-add a Ceph node to the Ceph cluster.
NOTE: This operation can be done to add more than one node at the same time.
Add Join Script   Copy and paste the below script into /srv/cray/scripts/common/join_ceph_cluster.sh.
NOTE: This script may also available in the /usr/share/doc/csm/scripts directory where the latest docs-csm RPM is installed. If so, it can be copied from that node to the new storage node being rebuilt and skip to step 2.</description>
    </item>
    
  </channel>
</rss>
