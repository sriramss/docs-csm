<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cray System Management Documentation on Cray System Management (CSM)</title>
    <link>/docs-csm/en-12/</link>
    <description>Recent content in Cray System Management Documentation on Cray System Management (CSM)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sat, 13 Nov 2021 03:15:35 +0000</lastBuildDate><atom:link href="/docs-csm/en-12/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>802.1x</title>
      <link>/docs-csm/en-12/operations/network/network_management_install_guide/aruba/8021x/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/network_management_install_guide/aruba/8021x/</guid>
      <description>802.1X IEEE 802.1X is an IEEE standard for port-based network access control (PNAC). This standard provides an authentication mechanism to devices wishing to attach to a LAN or WLAN. IEEE 802.1X defines the encapsulation of the Extensible Authentication Protocol (EAP) over IEEE 802, which is known as EAP over LAN (EAPOL).
 Port security is feature of &amp;ldquo;edge&amp;rdquo; switches such as 63/6400 and not available on 83xx.  Relevant Configuration</description>
    </item>
    
    <item>
      <title>Update BGP Neighbors</title>
      <link>/docs-csm/en-12/operations/network/metallb_bgp/update_bgp_neighbors/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/metallb_bgp/update_bgp_neighbors/</guid>
      <description>Update BGP Neighbors This page will detail the manual procedure to configure and verify BGP neighbors on the management switches.
You will not have BGP peers until CSM install.sh has run. This is where MetalLB is deployed.
 How do I check the status of the BGP neighbors?  Log into the spine switches and run show bgp ipv4 unicast summary for Aruba/HPE switches and show ip bgp summary for Mellanox.</description>
    </item>
    
    <item>
      <title>Metallb In BGP-mode</title>
      <link>/docs-csm/en-12/operations/network/metallb_bgp/metallb_in_bgp-mode/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/metallb_bgp/metallb_in_bgp-mode/</guid>
      <description>MetalLB in BGP-Mode MetalLB is a component in Kubernetes that manages access to LoadBalancer services from outside the Kubernetes cluster. There are LoadBalancer services on the Node Management Network (NMN), Hardware Management Network (HMN), and Customer Access Network (CAN).
MetalLB can run in either Layer2-mode or BGP-mode for each address pool it manages. BGP-mode is used for the NMN, HMN, and CAN. This enables true load balancing (Layer2-mode does failover, not load balancing) and allows for a more robust layer 3 configuration for these networks.</description>
    </item>
    
    <item>
      <title>Metallb In BGP-mode Configuration</title>
      <link>/docs-csm/en-12/operations/network/metallb_bgp/metallb_in_bgp-mode_configuration/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/metallb_bgp/metallb_in_bgp-mode_configuration/</guid>
      <description>MetalLB in BGP-Mode Configuration MetalLB in BGP-mode provides a more robust configuration for the Node Management Network (NMN), Hardware Management Network (HMN), and Customer Access Network (CAN). This configuration is generated from the csi config init input values. BGP-mode is enabled by updating the protocols in these configuration files.
MetalLB Peer Configuration The content for metallb_bgp_peers is generated by the csi config init command. In addition to the MetalLB configuration, there is configuration needed on the spine switches to set up the BGP router on these switches.</description>
    </item>
    
    <item>
      <title>Troubleshoot BGP Not Accepting Routes From Metallb</title>
      <link>/docs-csm/en-12/operations/network/metallb_bgp/troubleshoot_bgp_not_accepting_routes_from_metallb/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/metallb_bgp/troubleshoot_bgp_not_accepting_routes_from_metallb/</guid>
      <description>Troubleshoot BGP not Accepting Routes from MetalLB Check the number of routes that the Border Gateway Protocol (BGP) Router is accepting in the peering session. This procedure is useful if Kubernetes LoadBalancer services in the NMN, HMN, or CAN address pools are not accessible from outside the cluster.
Regain access to Kubernetes LoadBalancer services from outside the cluster.
Prerequisites This procedure requires administrative privileges.
Procedure   Log into the spine or aggregate switch.</description>
    </item>
    
    <item>
      <title>Troubleshoot Services Without An Allocated Ip Address</title>
      <link>/docs-csm/en-12/operations/network/metallb_bgp/troubleshoot_services_without_an_allocated_ip_address/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/metallb_bgp/troubleshoot_services_without_an_allocated_ip_address/</guid>
      <description>Troubleshoot Services without an Allocated IP Address Check if a given service has an IP address allocated for it if the Kubernetes LoadBalancer services in the NMN, HMN, or CAN address pools are not accessible from outside the cluster.
Regain access to Kubernetes LoadBalancer services from outside the cluster.
Prerequisites This procedure requires administrative privileges.
Procedure   Check the status of the services with the kubectl command to see the External-IP of the service.</description>
    </item>
    
    <item>
      <title>Check BGP Status And Reset Sessions</title>
      <link>/docs-csm/en-12/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/</guid>
      <description>Check BGP Status and Reset Sessions Check the Border Gateway Protocol (BGP) status on the Aruba and Mellanox switches and verify that all sessions are in an Established state. If the state of any session in the table is Idle, the BGP sessions needs to be reset.
Prerequisites This procedure requires administrative privileges.
Procedure MELLANOX   Verify that all BGP sessions are in an Established state for the Mellanox spine switches.</description>
    </item>
    
    <item>
      <title>Management Network CAN Setup</title>
      <link>/docs-csm/en-12/operations/network/management_network/management_network_can_setup/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/management_network_can_setup/</guid>
      <description>Management Network CAN Setup Access from the customer site to the system over shared networks is known as the Customer Access Network (CAN).
Requirements  Access to switches SHCD  Configuration The CAN configuration is highly dependent on customer requirements and may not meet the specifications below.
To access the HPE Cray EX nodes and services from the customer network, there is minimal configuration needed on the spine switch and the customer switch connected upstream from the spine switch to allow the customer_access_network subnet to be routed to the HPE Cray EX system.</description>
    </item>
    
    <item>
      <title>Management Network Flow Control Settings</title>
      <link>/docs-csm/en-12/operations/network/management_network/management_network_flow_control_settings/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/management_network_flow_control_settings/</guid>
      <description>Management Network Flow Control Settings This page is designed to go over all the flow control settings for Dell/Mellanox systems.
Leaf Switch Node Connections For the node connections to a leaf switch, disable the transmit flowcontrol and enable receive flowcontrol with the following commands:
NOTE: If using a TDS system involving a Hill cabinet, make sure to confirm that no CMM nor CEC components are connected to any leaf switches in your system.</description>
    </item>
    
    <item>
      <title>Management Network Switch Rename</title>
      <link>/docs-csm/en-12/operations/network/management_network/management_network_switch_rename/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/management_network_switch_rename/</guid>
      <description>Management Network Switch Rename Any system moving from Shasta v1.3 to Shasta v1.4 software needs to adjust the hostnames and IP addresses for all switches to match the new standard. There is now a virtual IP address ending in .1 which is used by spine switches. In Shasta v1.3, the first spine switch used the .1 address. In Shasta v1.4, the ordering of the switches has changed with spine switches being grouped first.</description>
    </item>
    
    <item>
      <title>Update Management Network Firmware</title>
      <link>/docs-csm/en-12/operations/network/management_network/update_management_network_firmware/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/update_management_network_firmware/</guid>
      <description>Update Management Network Firmware This page describes how to update firmware on the management network switches.
Requirements Access to the switches from the LiveCD/ncn-m001.
Configuration All firmware will be located at /var/www/fw/network on the LiveCD. It should contain the following files:
ncn-m001-pit:/var/www/network/firmware # ls -lh total 2.7G -rw-rw-r--+ 1 root root 658353828 Jan 7 2021 ArubaOS-CX_6400-6300_10_06_0010.stable.swi -rw-rw-r--+ 1 root root 384156519 May 3 22:18 ArubaOS-CX_8320_10_06_0110.stable.swi -rw-rw-r--+ 1 root root 444610797 Jan 7 2021 ArubaOS-CX_8325_10_06_0010.</description>
    </item>
    
    <item>
      <title>Management Network Access Port Configurations</title>
      <link>/docs-csm/en-12/operations/network/management_network/management_network_access_port_configurations/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/management_network_access_port_configurations/</guid>
      <description>Management Network Access Port Configurations Requirements  Access to switches SHCD  Configuration   This configuration describes the edge port configuration. This configuration is in the NMN/HMN/Mountain-TDS Management Tab of the SHCD.
  Typically, these are ports that are connected to iLOs (BMCs), gateway nodes, or compute nodes/CMM switches.
sw-leaf-001(config)# interface 1/1/35 no shutdown no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge   This configuration describes the ports that go to the Node Management Network (NMN/VLAN2).</description>
    </item>
    
    <item>
      <title>Management Network ACL Configuration</title>
      <link>/docs-csm/en-12/operations/network/management_network/management_network_acl_configuration/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/management_network_acl_configuration/</guid>
      <description>Management Network ACL Configuration This page describes the purpose of the ACLs and how they are configured.
Requirements  Access to the switches  Aruba Configuration These ACLs are designed to block traffic from the Node Management Network (NMN) to and from the Hardware Management Network (HMN).
These need to be set where the Layer3 interface is located, this will most likely be a VSX pair of switches. These ACLs are required on both switches in the pair.</description>
    </item>
    
    <item>
      <title>Troubleshoot Connectivity To Services With External Ip Addresses</title>
      <link>/docs-csm/en-12/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/</guid>
      <description>Troubleshoot Connectivity to Services with External IP addresses Systems that do not support CAN will not have services provisioned with external IP addresses on CAN. Kubernetes will report a &amp;lt;pending&amp;gt; status for the external IP address of the service experiencing connectivity issues.
If SSH access to a non-compute node (NCN) is available, it is possible to override resolution of external hostnames and forward local ports into the cluster for the cluster IP address of the corresponding service.</description>
    </item>
    
    <item>
      <title>Update The Cmn-external-DNS Value Post-installation</title>
      <link>/docs-csm/en-12/operations/network/external_dns/update_the_cmn-external-dns_value_post-installation/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/update_the_cmn-external-dns_value_post-installation/</guid>
      <description>Update the cmn-external-dns Value Post-Installation By default, the services/cray-externaldns-coredns-tcp and services/cray-externaldns-coredns-udp services both share the same Customer Management Network (CMN) external IP as defined by the cmn-external-dns value. This value is specified during the csi config init input.
It is expected to be in the static range reserved in MetalLB&amp;rsquo;s cmn-static-pool subnet. Theoretically, this is the only CMN IP address that must be known external to the system so IT DNS cmn delegate the system-name.</description>
    </item>
    
    <item>
      <title>Update The System-name.site-domain Value Post-installation</title>
      <link>/docs-csm/en-12/operations/network/external_dns/update_the_system-name_site-domain_value_post-installation/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/update_the_system-name_site-domain_value_post-installation/</guid>
      <description>Update the system-name.site-domain Value Post-Installation Update the domain name specified by the csi config init input. Updating the system-name.site-domain value without reinstalling the platform involves the following actions:
 Edit the Service object definitions and update all external-dns.alpha.kubernetes.io/hostname annotations. Edit the VirtualService (and possibly Gateway) object definitions and update spec.hosts and spec.http[].match[].authority settings.  Enables access to services that are accessible from the Customer Access Network (CAN) by updating the external domain for external hostnames.</description>
    </item>
    
    <item>
      <title>External DNS Csi Config Init Input Values</title>
      <link>/docs-csm/en-12/operations/network/external_dns/external_dns_csi_config_init_input_values/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/external_dns_csi_config_init_input_values/</guid>
      <description>External DNS csi config init Input Values External DNS requires the system-name, site-domain, and cmn-external-dns values that are defined with the csi config init command. These values are used to customize the External DNS configuration during installation.
The system-name and site-domain Values The system-name and site-domain values specified as part of the csi config init are used together in the system-name.site-domain format, creating the external domain for external hostnames for services accessible from the Customer Management Network (CMN).</description>
    </item>
    
    <item>
      <title>External DNS Failing To Discover Services Workaround</title>
      <link>/docs-csm/en-12/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/</guid>
      <description>External DNS Failing to Discover Services Workaround Many external DNS issues can be worked around by directly connecting to the desired backend service. This can circumvent authentication and authorization protections, but it may be necessary to access specific services when mitigating critical issues.
Istio&amp;rsquo;s ingress gateway uses Gateway and VirtualService objects to configure how traffic is routed to backend services. Currently, there is only one Gateway supporting the Customer Access Network (CAN), which is services/services-gateway.</description>
    </item>
    
    <item>
      <title>Ingress Routing</title>
      <link>/docs-csm/en-12/operations/network/external_dns/ingress_routing/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/ingress_routing/</guid>
      <description>Ingress Routing Ingress routing to services via Istio&amp;rsquo;s ingress gateway is configured by VirtualService custom resource definitions (CRD). When using external hostnames, there needs to be a VirtualService CRD that matches the external hostname to the desired destination.
For example, the configuration below controls the ingress routing for prometheus.SYSTEM_DOMAIN_NAME:
ncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus NAME GATEWAYS HOSTS AGE cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.SYSTEM_DOMAIN_NAME] 22h ncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus -o yaml apiVersion: networking.</description>
    </item>
    
    <item>
      <title>Troubleshoot DNS Configuration Issues</title>
      <link>/docs-csm/en-12/operations/network/external_dns/troubleshoot_dns_configuration_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/troubleshoot_dns_configuration_issues/</guid>
      <description>Troubleshoot DNS Configuration Issues Troubleshoot issues when DNS is not properly configured to delegate name resolution to the core DNS instance on a specific cluster. Although the CAN IP address may still be routable using the IP address directly, it may not work because Istio&amp;rsquo;s ingress gateway depends on the hostname (or SNI) to route traffic. For command line tools like cURL, using the &amp;ndash;resolve option to force correct resolution can be used to work around this issue.</description>
    </item>
    
    <item>
      <title>Add NCNs And UANs To External DNS</title>
      <link>/docs-csm/en-12/operations/network/external_dns/add_ncns_and_uans_to_external_dns/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:09 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/add_ncns_and_uans_to_external_dns/</guid>
      <description>Add NCNs and UANs to External DNS Edit the cray-externaldns-coredns ConfigMap to associate names with the Customer Access Network (CAN) IP addresses for non-compute nodes (NCNs) and User Access Nodes (UANs) in external DNS.
The cray-externaldns-coredns file contains the configuration files for external DNS&#39; CoreDNS instance.
Prerequisites This procedure requires administrative privileges.
Procedure   View the existing cray-externaldns-coredns ConfigMap.
ncn-w001# kubectl -n services get configmap cray-externaldns-coredns -o jsonpath=&amp;#39;{.data}&amp;#39; map[Corefile:.:53 { errors health log ready kubernetes cluster.</description>
    </item>
    
    <item>
      <title>External DNS</title>
      <link>/docs-csm/en-12/operations/network/external_dns/external_dns/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:09 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/external_dns/</guid>
      <description>External DNS External DNS, along with the Customer Access Network (CAN), Border Gateway Protocol (BGP), and MetalLB, makes it simpler to access the HPE Cray EX API and system management services. Services are accessible directly from a laptop without needing to tunnel into a non-compute node (NCN) or override /etc/hosts settings. Some services may require a JSON Web Token (JWT) to access them, while others may require Keycloak to login using a DC LDAP password.</description>
    </item>
    
    <item>
      <title>Troubleshoot PowerDNS</title>
      <link>/docs-csm/en-12/operations/network/dns/troubleshoot_powerdns/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:09 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/troubleshoot_powerdns/</guid>
      <description>Troubleshoot PowerDNS List DNS Zone Contents The PowerDNS zone database is populated with data from two sources:
 The cray-powerdns-manager service creates the zones and DNS records based on data sourced from the System Layout Service (SLS) The external DNS records are populated by the cray-externaldns-external-dns service using data sourced from Kubernetes annotations and virtual service definitions  Use the cray-powerdns-visualizer command to view the zone structure that cray-powerdns-manager will create.</description>
    </item>
    
    <item>
      <title>Enable Ncsd On UANs</title>
      <link>/docs-csm/en-12/operations/network/dns/enable_ncsd_on_uans/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/enable_ncsd_on_uans/</guid>
      <description>Enable ncsd on UANs Configure User Access Nodes (UANs) to start the ncsd service at boot time.
The nscd service is not currently enabled by default and systemd does not start it at boot time. There are two ways to start nscd on UAN nodes: manually starting the service or enabling the service in the UAN image. While restarting nscd manually has to be performed each time the UAN is rebooted, enabling nscd in the image only has to be done once.</description>
    </item>
    
    <item>
      <title>Manage The DNS Unbound Resolver</title>
      <link>/docs-csm/en-12/operations/network/dns/manage_the_dns_unbound_resolver/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/manage_the_dns_unbound_resolver/</guid>
      <description>Manage the DNS Unbound Resolver The unbound DNS instance is used to resolve names for the physical equipment on the management networks within the system, such as NCNs, UANs, switches, compute nodes, and more. This instance is accessible only within the HPE Cray EX system.
Check the Status of the cray-dns-unbound Pods Use the kubectl command to check the status of the pods:
ncn-w001# kubectl get -n services pods | grep unbound cray-dns-unbound-696c58647f-26k4c 2/2 Running 0 121m cray-dns-unbound-696c58647f-rv8h6 2/2 Running 0 121m cray-dns-unbound-coredns-q9lbg 0/2 Completed 0 121m cray-dns-unbound-manager-1596149400-5rqxd 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-8ppv4 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-cwksv 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-dtm9p 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-hckmp 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-t24w6 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-vzxnp 0/2 Completed 0 20h cray-dns-unbound-manager-1596222000-bcsk7 0/2 Completed 0 2m48s cray-dns-unbound-manager-1596222060-8pjx6 0/2 Completed 0 118s cray-dns-unbound-manager-1596222120-hrgbr 0/2 Completed 0 67s cray-dns-unbound-manager-1596222180-sf46q 1/2 NotReady 0 7s For more information about the pods displayed in the output above:</description>
    </item>
    
    <item>
      <title>PowerDNS Configuration</title>
      <link>/docs-csm/en-12/operations/network/dns/powerdns_configuration/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/powerdns_configuration/</guid>
      <description>PowerDNS Configuration External DNS PowerDNS replaces the CoreDNS server that earlier versions of CSM used to provide External DNS services.
The cray-dns-powerdns-can-tcp and cray-dns-powerdns-can-udp LoadBalancer resources are configured to service external DNS requests using the IP address specified by the CSI --cmn-external-dns command line argument.
The CSI --system-name and --site-domain command line arguments are combined to form the subdomain used for External DNS.
Site setup In the following example, the IP address 10.</description>
    </item>
    
    <item>
      <title>Troubleshoot Common DNS Issues</title>
      <link>/docs-csm/en-12/operations/network/dns/troubleshoot_common_dns_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/troubleshoot_common_dns_issues/</guid>
      <description>Troubleshoot Common DNS Issues The Domain Name Service (DNS) is part of an integrated infrastructure set designed to provide dynamic host discovery, addressing, and naming. There are several different place to look for troubleshooting as DNS interacts with Dynamic Host Configuration Protocol (DHCP), the Hardware Management Service (HMS), the System Layout Service (SLS), and the State Manager Daemon (SMD).
The information below describes what to check when experiencing issues with DNS.</description>
    </item>
    
    <item>
      <title>DHCP</title>
      <link>/docs-csm/en-12/operations/network/dhcp/dhcp/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:07 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dhcp/dhcp/</guid>
      <description>DHCP The Dynamic Host Configuration Protocol (DHCP) service on the HPE Cray EX system uses the Internet Systems Consortium (ISC) Kea tool. Kea provides more robust management capabilities for DHCP servers.
For more information: https://www.isc.org/kea/.
The following improvements to the DHCP service are included:
 Persistent and resilient data store for DHCP leases in Postgres API access to manage DHCP Scalable pod that uses metalLB instead of host networking Options for updates to HPE Cray EX management system IP addresses  DHCP Helper Workflow The DHCP-Helper uses the following workflow:</description>
    </item>
    
    <item>
      <title>Domain Name Service (DNS) Overview</title>
      <link>/docs-csm/en-12/operations/network/dns/dns/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:07 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/dns/</guid>
      <description>Domain Name Service (DNS) Overview DNS Architecture This diagram shows how the various components of the DNS infrastructure interact.
DNS Components The DNS infrastructure is comprised of a number of components.
Unbound (cray-dns-unbound) Unbound is a caching DNS resolver which is also used as the primary DNS server.
The DNS records served by Unbound include system component xnames, node hostnames, and service names and these records are read from the cray-dns-unbound ConfigMap which is populated by cray-dns-unbound-manager.</description>
    </item>
    
    <item>
      <title>Troubleshoot DHCP Issues</title>
      <link>/docs-csm/en-12/operations/network/dhcp/troubleshoot_dhcp_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:07 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dhcp/troubleshoot_dhcp_issues/</guid>
      <description>Troubleshoot DHCP Issues There are several things to check for when troubleshooting issues with Dynamic Host Configuration Protocol (DHCP) servers.
Incorrect DHCP IP Addresses One of the most common issues is when the DHCP IP addresses are not matching in the Domain Name Service (DNS).
Check to make sure cray-dhcp is not running in Kubernetes:
ncn-w001# kubectl get pods -A | grep cray-dhcp services cray-dhcp-5f8c8767db-hg6ch 1/1 Running 0 35d If the cray-dhcp pod is running, use the following command to shut down the pod:</description>
    </item>
    
    <item>
      <title>Externally Exposed Services</title>
      <link>/docs-csm/en-12/operations/network/customer_access_network/externally_exposed_services/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_access_network/externally_exposed_services/</guid>
      <description>Externally Exposed Services The following services are exposed on the Customer Access Network (CAN). Each of these services requires an IP address on the CAN subnet so they are reachable on the CAN. This IP address is allocated by the MetalLB component.
Services under Istio Ingress Gateway and Keycloak Gatekeeper Ingress share an ingress, so they all use the IP allocated to the Ingress.
Each service is given a DNS name that is served by the External DNS service to make them resolvable from the site network.</description>
    </item>
    
    <item>
      <title>Required Labels If CAN Is Not Configured</title>
      <link>/docs-csm/en-12/operations/network/customer_access_network/required_labels_if_can_is_not_configured/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_access_network/required_labels_if_can_is_not_configured/</guid>
      <description>Required Labels if CAN is Not Configured Some services on the system are required to access services outside of the HPE Cray EX system. If the Customer Access Network (CAN) is not configured on the system, these services will need to be pinned to ncn-m001 because that is the only node that has external access. See Customer Access Network (CAN) for more implications if CAN is not configured.
The label used for scheduling these services is no_external_access:</description>
    </item>
    
    <item>
      <title>Troubleshoot CAN Issues</title>
      <link>/docs-csm/en-12/operations/network/customer_access_network/troubleshoot_can_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_access_network/troubleshoot_can_issues/</guid>
      <description>Troubleshoot CAN Issues Various connection points to check when using the CAN and how to fix any issues that arise.
The most frequent issue with the Customer Access Network (CAN) is trouble accessing IP addresses outside of the HPE Cray EX system from a node or pod inside the system.
The best way to resolve this issue is to try to ping an outside IP address from one of the NCNs other than ncn-m001, which has a direct connection that it can use instead of the Customer Access Network (CAN).</description>
    </item>
    
    <item>
      <title>CAN With Dual-spine Configuration</title>
      <link>/docs-csm/en-12/operations/network/customer_access_network/can_with_dual-spine_configuration/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_access_network/can_with_dual-spine_configuration/</guid>
      <description>CAN with Dual-Spine Configuration The Customer Access Network (CAN) needs to be connected to both spines in a dual-spine configuration so that each spine can access the outside network. However, the NCNs should only have one default gateway. Therefore, the multi-active gateway protocol (MAGP) on the Mellanox spines can be used to create a virtual router gateway IP address that can direct to either of the spines, depending on the state of the spines.</description>
    </item>
    
    <item>
      <title>Connect To The CAN</title>
      <link>/docs-csm/en-12/operations/network/customer_access_network/connect_to_the_can/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_access_network/connect_to_the_can/</guid>
      <description>Connect to the CAN How to connect to the CAN physically and via layer 3.
There are multiple ways to connect to the Customer Access Network (CAN), both physically and via a layer 3 connection.
Physical Connection to the CAN The physical connection to the CAN is made via the load balancer or the spine switches. The uplink connection from the system to the customer network is achieved by using the highest numbered port(s).</description>
    </item>
    
    <item>
      <title>Customer Access Network</title>
      <link>/docs-csm/en-12/operations/network/customer_access_network/customer_access_network_can/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_access_network/customer_access_network_can/</guid>
      <description>Customer Access Network The Customer Access Network (CAN) provides access from outside the customer network to services, non-compute nodes (NCNs), and User Access Nodes (UANs) in the system. This allows for the following:
 Clients outside of the system:  Log in to each of the NCNs and UANs. Access web UIs within the system (e.g. Prometheus, Grafana, and more). Access the Rest APIs within the system. Access a DNS server within the system for resolution of names for the webUI and REST API services.</description>
    </item>
    
    <item>
      <title>Network</title>
      <link>/docs-csm/en-12/operations/network/network/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/network/</guid>
      <description>Network There are several different networks supported by the HPE Cray EX system. The following are the available internal and external networks, as well as the devices that connect to each network:
 Networks external to the system:  Customer Network (Data Center)  ncn-m001 BMC is connected by the customer network switch to the customer management network All NCNs (worker, master, and storage) are connected ClusterStor System Management Unit (SMU) interfaces User Access Nodes (UANs)     System networks:  Customer Network (Data Center)  ncn-m001 BMC is connected by the customer network switch to the customer management network ClusterStor SMU interfaces User Access Nodes (UANs)   Hardware Management Network (HMN)  BMCs for Admin tasks Power distribution units (PDU) Keyboard/video/mouse (KVM)   Node Management Network (NMN)  All NCNs and compute nodes User Access Nodes (UANs)   ClusterStor Management Network  ClusterStor controller management interfaces of all ClusterStor components (SMU, Metadata Management Unit (MMU), and Scalable Storage Unit (SSU))   High-Speed Network (HSN), which connects the following devices:  Kubernetes worker nodes UANs ClusterStor controller data interfaces of all ClusterStor components (SMU, MMU, and SSU) There must be at least two NCN&amp;rsquo;s whose BMCs are on the HMN.</description>
    </item>
    
    <item>
      <title>Access To System Management Services</title>
      <link>/docs-csm/en-12/operations/network/access_to_system_management_services/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/access_to_system_management_services/</guid>
      <description>Access to System Management Services The standard configuration for System Management Services (SMS) is the containerized REST micro-service with a public API. All of the micro-services provide an HTTP interface and are collectively exposed through a single gateway URL. The API gateway for the system is available at a well known URL based on the domain name of the system. It acts as a single HTTPS endpoint for terminating Transport Layer Security (TLS) using the configured certificate authority.</description>
    </item>
    
    <item>
      <title>Connect To The HPE Cray Ex Environment</title>
      <link>/docs-csm/en-12/operations/network/connect_to_the_hpe_cray_ex_environment/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/connect_to_the_hpe_cray_ex_environment/</guid>
      <description>Connect to the HPE Cray EX Environment The HPE Cray EX Management Network (SMNet) has multiple separate physical and logical links that are used to segregate traffic.
The diagram below shows the available connections from within the SMNet, as well as the connections to the customer network:
There are multiple ways to connect to the HPE Cray EX environment. The various methods are described in the following table:
   Role Description     Administrative External customer network connection to the worker node&amp;rsquo;s hardware management and administrative port   Application node access External customer network connection to an Application Node   Customer Access Network (CAN) Customer connection to the CAN gateway to access the HPE Cray EX CAN    There are also several ways to physically connect to the nodes on the system.</description>
    </item>
    
    <item>
      <title>Default Ip Address Ranges</title>
      <link>/docs-csm/en-12/operations/network/default_ip_address_ranges/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/default_ip_address_ranges/</guid>
      <description>Default IP Address Ranges The initial installation of the system creates default networks with default settings and with no external exposure. These IP address default ranges ensure that no nodes in the system attempt to use the same IP address as a Kubernetes service or pod, which would result in undefined behavior that is extremely difficult to reproduce or debug.
The following table shows the default IP address ranges:
   Network IP Address Range     Kubernetes service network 10.</description>
    </item>
    
    <item>
      <title>Troubleshoot Postgres Database</title>
      <link>/docs-csm/en-12/operations/kubernetes/troubleshoot_postgres_database/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/troubleshoot_postgres_database/</guid>
      <description>Troubleshoot Postgres Database General Postgres Troubleshooting Topics
 Is the Database Unavailable? Is the Database Disk Full? Is Replication Lagging? Is the Postgres status SyncFailed? Is a Cluster Member missing? Is the Postgres Leader missing?  The patronictl Tool The patronictl tool is used to call a REST API that interacts with Postgres databases. It handles a variety of tasks, such as listing cluster members and the replication status, configuring and restarting databases, and more.</description>
    </item>
    
    <item>
      <title>View Postgres Information For System Databases</title>
      <link>/docs-csm/en-12/operations/kubernetes/view_postgres_information_for_system_databases/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/view_postgres_information_for_system_databases/</guid>
      <description>View Postgres Information for System Databases Postgres uses SQL language to store and manage databases on the system. This procedure describes how to view and obtain helpful information about system databases, as well as the types of data being stored.
Prerequisites This procedure requires administrative privileges.
Procedure   Log in to the Postgres container.
ncn-w001# kubectl -n services exec -it cray-smd-postgres-0 -- bash Defaulting container name to postgres. Use &amp;#39;kubectl describe pod/cray-smd-postgres-0 -n services&amp;#39; to see all of the containers in this pod.</description>
    </item>
    
    <item>
      <title>Restore An Etcd Cluster From A Backup</title>
      <link>/docs-csm/en-12/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/</guid>
      <description>Restore an etcd Cluster from a Backup Use an existing backup of a healthy etcd cluster to restore an unhealthy cluster to a healthy state.
The commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.
 NOTE
Etcd Clusters can be restored using the automation script or the manual procedure below. The automation script follows the same steps as the manual procedure.</description>
    </item>
    
    <item>
      <title>Restore Bare-metal Etcd Clusters From An S3 Snapshot</title>
      <link>/docs-csm/en-12/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/</guid>
      <description>Restore Bare-Metal etcd Clusters from an S3 Snapshot The etcd cluster that serves Kubernetes on master nodes is backed up every 10 minutes. These backups are pushed to Ceph Rados Gateway (S3).
Restoring the etcd cluster from backup is only meant to be used in a catastrophic scenario, whereby the Kubernetes cluster and master nodes are being rebuilt. This procedure shows how to restore the bare-metal etcd cluster from an Simple Storage Service (S3) snapshot.</description>
    </item>
    
    <item>
      <title>Restore Postgres</title>
      <link>/docs-csm/en-12/operations/kubernetes/restore_postgres/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/restore_postgres/</guid>
      <description>Restore Postgres Below are the service specific steps required to restore data to a Postgres cluster.
Restore Postgres Procedures by Service:
 Restore Postgres for Spire Restore Postgres for Keycloak Restore Postgres for HSM (Hardware State Manager) Restore Postgres for SLS (System Layout Service) Restore Postgres for VCS Restore Postgres for Capsules Services  
Restore Postgres for Spire In the event that the spire Postgres cluster is in a state that the cluster must be rebuilt and the data restored, the following procedures are recommended.</description>
    </item>
    
    <item>
      <title>Retrieve Cluster Health Information Using Kubernetes</title>
      <link>/docs-csm/en-12/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/</guid>
      <description>Retrieve Cluster Health Information Using Kubernetes The kubectl CLI commands can be used to retrieve information about the Kubernetes cluster components.
Retrieve Node Status ncn# kubectl get nodes NAME STATUS ROLES AGE VERSION ncn-m001 Ready master 19d v1.14.3 ncn-m002 Ready master 19d v1.14.3 ncn-m003 Ready master 19d v1.14.3 ncn-w001 Ready &amp;lt;none&amp;gt; 19d v1.14.3 ncn-w002 Ready &amp;lt;none&amp;gt; 19d v1.14.3 ncn-w003 Ready &amp;lt;none&amp;gt; 19d v1.14.3 Retrieve Pod Status ncn# kubectl get pods options NAME READY STATUS RESTARTS AGE api-gateway-6fffd4d854-btp4v 1/1 Running 2 11d api-gateway-6fffd4d854-l7z8m 1/1 Running 1 11d api-gateway-6fffd4d854-tqx8v 1/1 Running 1 11d api-gateway-database-8555685975-xw5gm 1/1 Running 1 11d cray-ars-6466bcf77d-s99rq 1/1 Running 1 11d cray-bss-7589d7459f-ttwf4 1/1 Running 1 11d cray-capmc-6bf5855b78-np72d 1/1 Running 1 11d cray-datastore-768576677d-gpnl7 1/1 Running 1 11d cray-dhcp-5fccf85696-wrqxb 1/1 Running 1 11d cray-tftp-885cc65c4-568jz 2/2 Running 2 11d kvstore-1-56fdb6574c-ts79v 1/1 Running 1 11d kvstore-2-694bc7567b-k99tl 1/1 Running 1 11d kvstore-3-5b658b9bd9-n9dgt 1/1 Running 1 11d Retrieve Information about Individual Pods ncn# kubectl describe pod POD_NAME -n NAMESPACE_NAME Retrieve a List of Healthy Pods ncn# kubectl get pods -A | grep -e &amp;#39;Completed|Running&amp;#39; ceph-cephfs cephfs-provisioner-74599ccfcd-bzf5b 1/1 Running 3 2d11h ceph-rbd rbd-provisioner-76c464c567-jvvk6 1/1 Running 3 2d11h cert-manager cray-certmanager-cainjector-8487d996d7-phbfr 1/1 Running 0 2d11h cert-manager cray-certmanager-cert-manager-d5fb67664-v9sgf 1/1 Running 0 2d11h cert-manager cray-certmanager-webhook-6b58c6bb79-dtpl7 1/1 Running 0 2d11h default kube-keepalived-vip-mgmt-plane-nmn-local-pkbxj 1/1 Running 0 2d11h default kube-keepalived-vip-mgmt-plane-nmn-local-v8w6k 1/1 Running 0 2d11h default kube-keepalived-vip-mgmt-plane-nmn-local-zzc4n 1/1 Running 1 2d11h istio-system grafana-ff8b4b964-grg8m 1/1 Running 0 2d11h istio-system istio-citadel-699fc7bcf-rn99p 1/1 Running 0 2d11h istio-system istio-ingressgateway-59bf97fbc5-h6x4w 1/1 Running 0 2d11h istio-system istio-ingressgateway-hmn-5d7777c75-2c5nz 1/1 Running 0 2d11h istio-system istio-ingressgateway-hmn-5d7777c75-dh4fl 1/1 Running 0 2d11h istio-system istio-init-crd-10-1.</description>
    </item>
    
    <item>
      <title>Sealed Secrets Procedures</title>
      <link>/docs-csm/en-12/operations/kubernetes/sealed_secrets_procedures/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/sealed_secrets_procedures/</guid>
      <description>Sealed Secrets Procedures Tracked Sealed Secrets Tracked sealed secrets are regenerated every time secrets are seeded (see the use of utils/secrets-seed-customizations.sh above).
To view currently tracked sealed secrets:
linux# yq read /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets Expected output looks similar to the following:
- cray_reds_credentials - cray_meds_credentials - cray_hms_rts_credentials In order to prevent tracked sealed secrets from being regenerated, they MUST BE REMOVED from the spec.kubernetes.tracked_sealed_secrets list in /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml prior to executing the Generate Sealed Secrets section of Prepare Site Init.</description>
    </item>
    
    <item>
      <title>Rebuild Unhealthy Etcd Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/rebuild_unhealthy_etcd_clusters/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/rebuild_unhealthy_etcd_clusters/</guid>
      <description>Rebuild Unhealthy etcd Clusters Rebuild any cluster that does not have healthy pods by deleting and redeploying unhealthy pods. This procedure includes examples for rebuilding etcd clusters in the services namespace. This procedure must be used for each unhealthy cluster, not just the services used in the following examples.
This process also applies when etcd is not visible when running the kubectl get pods command.
A special use case is also included for the Content Projection Service (CPS) as the process for rebuilding the cluster is slightly different.</description>
    </item>
    
    <item>
      <title>Recover From Postgres Wal Event</title>
      <link>/docs-csm/en-12/operations/kubernetes/recover_from_postgres_wal_event/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/recover_from_postgres_wal_event/</guid>
      <description>Recover from Postgres WAL Event A WAL event can occur because of lag, network communication, or bandwidth issues. This can cause the PVC hosted by Ceph and mounted inside the container on /home/postgres/pgdata to fill and the database to stop running. If no database dump exists, the disk space issue needs to be fixed so that a dump can be taken. Then the dump can be restored to a newly created postgresql cluster.</description>
    </item>
    
    <item>
      <title>Repopulate Data In Etcd Clusters When Rebuilding Them</title>
      <link>/docs-csm/en-12/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/</guid>
      <description>Repopulate Data in etcd Clusters When Rebuilding Them When an etcd cluster is not healthy, it needs to be rebuilt. During that process, the pods that rely on etcd clusters lose data. That data needs to be repopulated in order for the cluster to go back to a healthy state.
The following services need their data repopulated in the etcd cluster:
 Boot Orchestration Service (BOS) Boot Script Service (BSS) Content Projection Service (CPS) Compute Rolling Upgrade Service (CRUS) External DNS Firmware Action Service (FAS) HMS Notification Fanout Daemon (HMNFD) Mountain Endpoint Discovery Service (MEDS) River Endpoint Discovery Service (REDS)  Prerequisites A etcd cluster was rebuilt.</description>
    </item>
    
    <item>
      <title>Report The Endpoint Status For Etcd Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/</guid>
      <description>Report the Endpoint Status for etcd Clusters Report etcd cluster end point status. The report includes a cluster&amp;rsquo;s endpoint, database size, and leader status.
This procedure provides the ability to view the etcd cluster endpoint status.
Prerequisites  This procedure requires root privileges. The etcd clusters are in a healthy state.  Procedure   Report the endpoint status for all etcd clusters in a namespace.
The following example is for the services namespace.</description>
    </item>
    
    <item>
      <title>Kubernetes Networking</title>
      <link>/docs-csm/en-12/operations/kubernetes/kubernetes_networking/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/kubernetes_networking/</guid>
      <description>Kubernetes Networking Every Kubernetes pod has an IP address in the pod network that is reachable within the cluster. The system uses the weave-net plugin for inter-node communication.
Access services from outside the cluster All services with a REST API must be accessed from outside the cluster using the Istio Ingress Gateway. This gateway can be accessed using a URL in the following format:
https://api.SYSTEM-NAME_DOMAIN-NAME The API requests then get routed to the appropriate node running that service.</description>
    </item>
    
    <item>
      <title>Kubernetes Storage</title>
      <link>/docs-csm/en-12/operations/kubernetes/kubernetes_storage/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/kubernetes_storage/</guid>
      <description>Kubernetes Storage Data belonging to micro-services in the management cluster is managed through persistent storage, which provides reliable and resilient data protection for containers running in the Kubernetes cluster.
The backing storage for this service is currently provided by JBOD disks that are spread across several nodes of the management cluster. These node disks are managed by Ceph, and are exposed to containers in the form of persistent volumes.</description>
    </item>
    
    <item>
      <title>Pod Resource Limits</title>
      <link>/docs-csm/en-12/operations/kubernetes/pod_resource_limits/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/pod_resource_limits/</guid>
      <description>Pod Resource Limits Kubernetes uses resource requests and Quality of Service (QoS) for scheduling pods. Resource requests can be provided explicitly for pods and containers, whereas pod QoS is implicit, based on the resource requests and limits of the containers in the pod. There are three types of QoS:
 Guaranteed: All containers in a pod have explicit memory and CPU resource requests and limits. For each resource, the limit equals the request.</description>
    </item>
    
    <item>
      <title>Rebalance Healthy Etcd Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/rebalance_healthy_etcd_clusters/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/rebalance_healthy_etcd_clusters/</guid>
      <description>Rebalance Healthy etcd Clusters Rebalance the etcd clusters. The clusters need to be in a healthy state, and there needs to be the same number of pods running on each worker node for the etcd clusters to be balanced.
Restoring the balance of etcd clusters will help with the storage of Kubernetes cluster data.
Prerequisites  etcd clusters are in a healthy state. etcd clusters do not have the same number of pods on each worker node.</description>
    </item>
    
    <item>
      <title>Disaster Recovery For Postgres</title>
      <link>/docs-csm/en-12/operations/kubernetes/disaster_recovery_postgres/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/disaster_recovery_postgres/</guid>
      <description>Disaster Recovery for Postgres In the event that the Postgres cluster has failed to the point that it must be recovered and there is no dump available to restore the data, a full service specific disaster recovery is needed.
Below are the service specific steps required to cleanup any existing resources, redeploy the resources and repopulate the data.
Disaster Recovery Procedures by Service:
 Restore HSM (Hardware State Manger) Postgres without a Backup Restore SLS (System Layout Service) Postgres without a Backup Restore Spire Postgres without a Backup Restore Keycloak Postgres without a Backup Restore Console Postgres without a Backup</description>
    </item>
    
    <item>
      <title>Increase Kafka Pod Resource Limits</title>
      <link>/docs-csm/en-12/operations/kubernetes/increase_kafka_pod_resource_limits/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/increase_kafka_pod_resource_limits/</guid>
      <description>Increase Kafka Pod Resource Limits For larger scale systems, the Kafka resource limits may need to be increased.See Increase Pod Resource Limits for details on how to increase limits.Increase Kafka Resource Limits Example
For a 1500 compute node system, increasing the cpu count to 6 and memory limits to 128G should be adequate.</description>
    </item>
    
    <item>
      <title>Increase Pod Resource Limits</title>
      <link>/docs-csm/en-12/operations/kubernetes/increase_pod_resource_limits/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/increase_pod_resource_limits/</guid>
      <description>Increase Pod Resource Limits Increase the appropriate resource limits for pods after determining if a pod is being CPU throttled or OOMKilled.
Return Kubernetes pods to a healthy state with resources available.
Prerequisites  kubectl is installed. The names of the pods hitting their resource limits are known. See Determine if Pods are Hitting Resource Limits.  Procedure   Determine the current limits of a pod.
In the example below, cray-hbtd-etcd-8r2scmpb58 is the POD_ID being used.</description>
    </item>
    
    <item>
      <title>Kubernetes</title>
      <link>/docs-csm/en-12/operations/kubernetes/kubernetes/</link>
      <pubDate>Sat, 13 Nov 2021 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/kubernetes/</guid>
      <description>Kubernetes The system management components are broken down into a series of micro-services. Each service is independently deployable, fine-grained, and uses lightweight protocols. As a result, the system&amp;rsquo;s micro-services are modular, resilient, and can be updated independently. Services within this architecture communicate via REST APIs.
About Kubernetes Kubernetes is a portable and extensible platform for managing containerized workloads and services. Kubernetes serves as a micro-services platform on the system that facilitates application deployment, scaling, and management.</description>
    </item>
    
    <item>
      <title>Containerd</title>
      <link>/docs-csm/en-12/operations/kubernetes/containerd/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:59 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/containerd/</guid>
      <description>Containerd Containerd is a daemonset that runs on the host. It is used to run containers on the Kubernetes platform.
Restarting containerd If the containerd service is restarted on a worker node, this may cause the sonar-jobs-watcher pod running on that worker node to fail when attempting to cleanup unneeded containers. For example:
ncn-w001 # systemctl restart containerd ncn-w001 # ncn-w001:~ # kubectl get pods -l name=sonar-jobs-watcher -n services -o wide | grep ncn-w001 sonar-jobs-watcher-8z6th 1/1 Running 0 95d 10.</description>
    </item>
    
    <item>
      <title>Create A Manual Backup Of A Healthy Etcd Cluster</title>
      <link>/docs-csm/en-12/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:59 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/</guid>
      <description>Create a Manual Backup of a Healthy etcd Cluster Manually create a backup of a healthy etcd cluster and check to see if the backup was created successfully.
Backups of healthy etcd clusters can be used to restore the cluster if it becomes unhealthy at any point.
The commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.
Prerequisites A healthy etcd cluster is available on the system.</description>
    </item>
    
    <item>
      <title>Determine If Pods Are Hitting Resource Limits</title>
      <link>/docs-csm/en-12/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:59 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/</guid>
      <description>Determine if Pods are Hitting Resource Limits Determine if a pod is being CPU throttled or hitting its memory limits (OOMKilled). Use the detect_cpu_throttling.sh script to determine if any pods are being CPU throttled, and check the Kubernetes events to see if any pods are hitting a memory limit.
IMPORTANT: The presence of CPU throttling does not always indicate a problem, but if a service is being slow or experiencing latency issues, this procedure can be used to evaluate if it is not performing well as a result of CPU throttling.</description>
    </item>
    
    <item>
      <title>Check The Health And Balance Of Etcd Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:58 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/</guid>
      <description>Check the Health and Balance of etcd Clusters Check to see if all of the etcd clusters have healthy pods, are balanced, and have a healthy cluster database. There needs to be the same number of pods running on each worker node for the etcd clusters to be balanced. If the number of pods is not the same for each worker node, the cluster is not balanced.
Any clusters that do not have healthy pods will need to be rebuilt.</description>
    </item>
    
    <item>
      <title>Clear Space In An Etcd Cluster Database</title>
      <link>/docs-csm/en-12/operations/kubernetes/clear_space_in_an_etcd_cluster_database/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:58 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/clear_space_in_an_etcd_cluster_database/</guid>
      <description>Clear Space in an etcd Cluster Database Use this procedure to clear the etcd cluster NOSPACE alarm. Once it is set it will remain set. If needed, defrag the database cluster before clearing the NOSPACE alarm.
Defragging the database cluster and clearing the etcd cluster NOSPACE alarm will free up database space.
Prerequisites  This procedure requires root privileges. The etcd clusters are in a healthy state.  Procedure   Clear up space when the etcd database space has exceeded and has been defragged, but the NOSPACE alarm remains set.</description>
    </item>
    
    <item>
      <title>Configure Kubectl Credentials To Access The Kubernetes Apis</title>
      <link>/docs-csm/en-12/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:58 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/</guid>
      <description>Configure kubectl Credentials to Access the Kubernetes APIs The credentials for kubectl are located in the admin configuration file on all non-compute node (NCN) master and worker nodes. They can be found at /etc/kubernetes/admin.conf for the root user. Use kubectl to access the Kubernetes cluster from a device outside the cluster.
For more information, refer to https://kubernetes.io/
Prerequisites This procedure requires administrative privileges and assumes that the device being used has:</description>
    </item>
    
    <item>
      <title>About Kubectl</title>
      <link>/docs-csm/en-12/operations/kubernetes/about_kubectl/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:57 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/about_kubectl/</guid>
      <description>About kubectl kubectl is a CLI that can be used to run commands against a Kubernetes cluster. The format of the kubectl command is shown below:
ncn# kubectl COMMAND RESOURCE_TYPE RESOURCE_NAME FLAGS An example of using kubectl to retrieve information about a pod is shown below:
ncn# kubectl get pod POD_NAME1 POD_NAME2 kubectl is installed by default on the non-compute node (NCN) image. To learn more about kubectl, refer to https://kubernetes.</description>
    </item>
    
    <item>
      <title>Backups For Etcd-operator Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/backups_for_etcd-operator_clusters/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:57 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/backups_for_etcd-operator_clusters/</guid>
      <description>Backups for etcd-operator Clusters Backups are periodically created for etcd clusters. These backups are stored in the Ceph Rados Gateway (S3). Not all services are backed up automatically. Services that are not backed up automatically will need to be manually rediscovered if the cluster is unhealthy.
Clusters with Automated Backups The following services are backed up (daily, one week&amp;rsquo;s worth of backups retained) as part of the automated solution:
 Boot Orchestration Service (BOS) Boot Script Service (BSS) Compute Rolling Upgrade Service (CRUS) External DNS Firmware Action Service (FAS)  Run the following command on any master node (ncn-mXXX) or the first worker node (ncn-w001) to list the backups for a specific project.</description>
    </item>
    
    <item>
      <title>Check For And Clear Etcd Cluster Alarms</title>
      <link>/docs-csm/en-12/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:57 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/</guid>
      <description>Check for and Clear etcd Cluster Alarms Check for any etcd cluster alarms and clear them as needed. An etcd cluster alarm must be manually cleared.
For example, a cluster&amp;rsquo;s database &amp;ldquo;NOSPACE&amp;rdquo; alarm is set when database storage space is no longer available. A subsequent defrag may free up database storage space, but writes to the database will continue to fail while the &amp;ldquo;NOSPACE&amp;rdquo; alarm is set.
Prerequisites  This procedure requires root privileges.</description>
    </item>
    
    <item>
      <title>Kubernetes And Bare Metal Etcd Certificate Renewal</title>
      <link>/docs-csm/en-12/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:57 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/</guid>
      <description>Kubernetes and Bare Metal EtcD Certificate Renewal Scope As part of the installation, Kubernetes generates certificates for the required subcomponents. This document will help walk through the process of renewing the certificates.
IMPORTANT: Depending on the version of Kubernetes, the command may or may not reside under the alpha category. Use kubectl certs --help and kubectl alpha certs --help to determine this. The overall command syntax should be the same and this is just whether or not the command structure will require alpha in it.</description>
    </item>
    
    <item>
      <title>About Etcd</title>
      <link>/docs-csm/en-12/operations/kubernetes/about_etcd/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:56 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/about_etcd/</guid>
      <description>About etcd The system uses etcd for storing all of its cluster data. It is an open source database that is excellent for maintaining the state of Kubernetes. Failures in the etcd cluster at the heart of Kubernetes will cause a failure of Kubernetes. To mitigate this risk, the system is deployed with etcd on dedicated disks and with a specific configuration to optimize Kubernetes workloads. The system also provides additional etcd cluster(s) as necessary to help maintain an operational state of services.</description>
    </item>
    
    <item>
      <title>About Kubernetes Taints And Labels</title>
      <link>/docs-csm/en-12/operations/kubernetes/about_kubernetes_taints_and_labels/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:56 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/about_kubernetes_taints_and_labels/</guid>
      <description>About Kubernetes Taints and Labels Kubernetes labels control node affinity, which is the property of pods that attracts them to a set of nodes. On the other hand, Kubernetes taints enable a node to repel a set of pods. In addition, pods can have tolerances for taints to allow them to run on nodes with certain taints.
Taints are controlled with the kubectl taint nodes command, while node labels for various nodes can be customized with a configmap that contains the desired values.</description>
    </item>
    
    <item>
      <title>About Postgres</title>
      <link>/docs-csm/en-12/operations/kubernetes/about_postgres/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:56 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/about_postgres/</guid>
      <description>About Postgres The system uses PostgreSQL (known as Postgres) as a database solution. Postgres databases use SQL language to store and manage databases on the system.
To learn more about Postgres, see https://www.postgresql.org/docs/.
The Patroni tool can be used to manage and maintain information in a Postgres database. It handles tasks such as listing cluster members and the replication status, configuring and restarting databases, and more. For more information about this tool, refer to Troubleshoot Postgres Database.</description>
    </item>
    
    <item>
      <title>Delete Or Recover Deleted IMS Content</title>
      <link>/docs-csm/en-12/operations/image_management/delete_or_recover_deleted_ims_content/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:55 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/delete_or_recover_deleted_ims_content/</guid>
      <description>Delete or Recover Deleted IMS Content The Image Management System (IMS) manages user supplied SSH public Keys, customizable image recipes, images, and IMS jobs that are used to build or customize images. In previous versions of IMS, deleting an IMS public key, recipe, or image resulted in that item being permanently deleted. Additionally, IMS recipes and images store linked artifacts in the Simple Storage Service (S3) datastore. These artifacts are referenced by the IMS recipe and image records.</description>
    </item>
    
    <item>
      <title>Image Management</title>
      <link>/docs-csm/en-12/operations/image_management/image_management/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:55 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/image_management/</guid>
      <description>Image Management The Image Management Service (IMS) uses the open source Kiwi-NG tool to build image roots from compressed Kiwi image descriptions. These compressed Kiwi image descriptions are referred to as &amp;ldquo;recipes.&amp;rdquo; Kiwi-NG builds images based on a variety of different Linux distributions, specifically SUSE, RHEL, and their derivatives. Kiwi image descriptions must follow the Kiwi development schema. More information about the development schema and the Kiwi-NG tool can be found in the documentation: https://doc.</description>
    </item>
    
    <item>
      <title>Image Management Workflows</title>
      <link>/docs-csm/en-12/operations/image_management/image_management_workflows/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:55 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/image_management_workflows/</guid>
      <description>Image Management Workflows Overview of how to create an image and how to customize and image.
The following workflows are intended to be high-level overviews of image management tasks. These workflows depict how services interact with each other during image management and help to provide a quicker and deeper understanding of how the system functions.
The workflows in this section include:
 Create a New Image Customize an Image  Create a New Image Use Case: The system administrator creates an image root from a customized recipe.</description>
    </item>
    
    <item>
      <title>Upload And Register An Image Recipe</title>
      <link>/docs-csm/en-12/operations/image_management/upload_and_register_an_image_recipe/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:55 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/upload_and_register_an_image_recipe/</guid>
      <description>Upload and Register an Image Recipe Download and expand recipe archives from S3 and IMS. Modify and upload a recipe archive, and then register that recipe archive with IMS.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployment:  cray-ims, the Image Management Service (IMS)   The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system.</description>
    </item>
    
    <item>
      <title>Configure IMS To Validate Rpms</title>
      <link>/docs-csm/en-12/operations/image_management/configure_ims_to_validate_rpms/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:54 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/configure_ims_to_validate_rpms/</guid>
      <description>Configure IMS to validate RPMS Configuring IMS to validate the GPG signatures of RPMs during IMS Build operations involves two steps.
  Create and update IMS to use a new Kiwi-NG Image with the Signing Keys embedded.
NOTE: The default IMS Kiwi-NG Image is already configured with the signing keys needed to validate HPE and SuSE RPMs and repositories.
  Update IMS Recipes to require GPG verification of RPMs/Repos.</description>
    </item>
    
    <item>
      <title>Convert Tgz Archives To SqUAShfs Images</title>
      <link>/docs-csm/en-12/operations/image_management/convert_tgz_archives_to_squashfs_images/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:54 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/convert_tgz_archives_to_squashfs_images/</guid>
      <description>Convert TGZ Archives to SquashFS Images If customizing a pre-built image root archive compressed as a txz or other non-SquashFS format, convert the image root to SquashFS and upload the SquashFS archive to S3.
The steps in this section only apply if the image root is not in SquashFS format.
Prerequisites There is a pre-built image that is not currently in SquashFS format.
Procedure   Locate the image root to be converted to SquashFS.</description>
    </item>
    
    <item>
      <title>Create UAN Boot Images</title>
      <link>/docs-csm/en-12/operations/image_management/create_uan_boot_images/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:54 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/create_uan_boot_images/</guid>
      <description>Create UAN Boot Images Update configuration management git repository to match the installed version of the UAN product. Then use that updated configuration to create UAN boot images and a BOS session template.
This is the overall workflow for preparing UAN images for booting UANs:
 Clone the UAN configuration git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch.</description>
    </item>
    
    <item>
      <title>Customize An Image Root Using IMS</title>
      <link>/docs-csm/en-12/operations/image_management/customize_an_image_root_using_ims/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:54 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/customize_an_image_root_using_ims/</guid>
      <description>Customize an Image Root Using IMS The Image Management Service (IMS) customization workflow sets up a temporary image customization environment within a Kubernetes pod and mounts the image to be customized in that environment. A system administrator then makes the desired changes to the image root within the customization environment.
Afterwards, the IMS customization workflow automatically copies the NCN CA public key to /etc/cray/ca/certificate_authority.crt within the image root being customized to enable secure communications between NCNs and client nodes.</description>
    </item>
    
    <item>
      <title>Adjust Hm Collector Resource Limits And Requests</title>
      <link>/docs-csm/en-12/operations/hmcollector/adjust_hmcollector_resource_limits_requests/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hmcollector/adjust_hmcollector_resource_limits_requests/</guid>
      <description>Adjust HM Collector resource limits and requests  Resource Limit Tuning Guidance Customize cray-hms-hmcollector resource limits and requests in customizations.yaml Redeploy cray-hms-hmcollector with new resource limits and requests  Resource Limit Tuning Guidance Inspect current resource usage in the cray-hms-hmcollector pod View resource usage of the containers in the cray-hms-hmcollector pod:
ncn-m001# kubectl -n services top pod -l app.kubernetes.io/name=cray-hms-hmcollector --containers POD NAME CPU(cores) MEMORY(bytes) cray-hms-hmcollector-7c5b797c5c-zxt67 istio-proxy 187m 275Mi cray-hms-hmcollector-7c5b797c5c-zxt67 cray-hms-hmcollector 4398m 296Mi The default resource limits for the cray-hms-hmcollector container are:</description>
    </item>
    
    <item>
      <title>Build A New UAN Image Using The Default Recipe</title>
      <link>/docs-csm/en-12/operations/image_management/build_a_new_uan_image_using_the_default_recipe/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/build_a_new_uan_image_using_the_default_recipe/</guid>
      <description>Build a New UAN Image Using the Default Recipe Build or rebuild the UAN image using either the default UAN image or image recipe. Both of these are supplied by the User Access Node (UAN) product stream installer.
Prerequisites  Both the Cray Operation System (COS) and UAN product streams must be installed. The Cray administrative CLI must be initialized.  Procedure The Cray EX User Access Node (UAN) recipe currently requires the Slingshot Diagnostics package, which is not installed with the UAN product itself.</description>
    </item>
    
    <item>
      <title>Build An Image Using IMS Rest Service</title>
      <link>/docs-csm/en-12/operations/image_management/build_an_image_using_ims_rest_service/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/build_an_image_using_ims_rest_service/</guid>
      <description>Build an Image Using IMS REST Service Create an image root from an IMS recipe.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments:  cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service   The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system.</description>
    </item>
    
    <item>
      <title>Restore Hardware State Manager (HSM) Postgres Database From Backup</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_from_backup/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_from_backup/</guid>
      <description>Restore Hardware State Manager (HSM) Postgres Database from Backup This procedure can be used to restore the HSM Postgres database from a previously taken backup. This can be a manual backup created by the Create a Backup of the HSM Postgres Database procedure, or an automatic backup created by the cray-smd-postgresql-db-backup Kubernetes cronjob.
Prerequisites   Healthy System Layout Service (SLS). Recovered first if also affected.
  Healthy HSM Postgres Cluster.</description>
    </item>
    
    <item>
      <title>Restore Hardware State Manager (HSM) Postgres Without An Existing Backup</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/</guid>
      <description>Restore Hardware State Manager (HSM) Postgres without an Existing Backup This procedure is intended to repopulate HSM in the event when no Postgres backup exists.
Prerequisite   Healthy System Layout Service (SLS). Recovered first if also affected.
  Healthy HSM service.
Verify all 3 HSM postgres replicas are up and running:
ncn# kubectl -n services get pods -l cluster-name=cray-smd-postgres NAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 18d cray-smd-postgres-1 3/3 Running 0 18d cray-smd-postgres-2 3/3 Running 0 18d   Procedure   Re-run the HSM loader job.</description>
    </item>
    
    <item>
      <title>Hardware State Manager (HSM) State And Flag Fields</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/</guid>
      <description>Hardware State Manager (HSM) State and Flag Fields HSM manages important information for hardware components in the system. Administrators can use the data returned by HSM to learn about the state of the system. To do so, it is critical that the State and Flag fields are understood, and the next steps to take are known when viewing output returned by HSM commands. It is also beneficial to understand what services can cause State or Flag changes in HSM.</description>
    </item>
    
    <item>
      <title>Lock And Unlock Management Nodes</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/lock_and_unlock_management_nodes/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/lock_and_unlock_management_nodes/</guid>
      <description>Lock and Unlock Management Nodes The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes and NCNs are also not locked by default. The administrator must lock the NCNs to prevent unwanted actions from affecting these nodes.
This section only covers using locks with the Hardware State Manager (HSM). For more information on ignoring nodes, refer to the following sections:
 Firmware Action Service (FAS): See Ignore Node within FAS.</description>
    </item>
    
    <item>
      <title>Manage Component Groups</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/manage_component_groups/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/manage_component_groups/</guid>
      <description>Manage Component Groups The creation, deletion, and modification of groups is enabled by the Hardware State Manager (HSM) APIs.
The following is an example group that contains the optional fields tags and exclusiveGroup:
{ &amp;#34;label&amp;#34; : &amp;#34;blue&amp;#34;, &amp;#34;description&amp;#34; : &amp;#34;blue node group&amp;#34;, &amp;#34;tags&amp;#34; : [ &amp;#34;tag1&amp;#34;, &amp;#34;tag2&amp;#34; ], &amp;#34;members&amp;#34; : { &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;, &amp;#34;x0c0s0b0n1&amp;#34;, &amp;#34;x0c0s0b1n0&amp;#34;, &amp;#34;x0c0s0b1n1&amp;#34; ] }, &amp;#34;exclusiveGroup&amp;#34; : &amp;#34;colors&amp;#34; } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.</description>
    </item>
    
    <item>
      <title>Manage Component Partitions</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/manage_component_partitions/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/manage_component_partitions/</guid>
      <description>Manage Component Partitions The creation, deletion, and modification of partitions is enabled by the Hardware State Manager (HSM) APIs.
The following is an example partition that contains the optional tags field:
{ &amp;quot;name&amp;quot; : &amp;quot;partition 1&amp;quot;, &amp;quot;description&amp;quot; : &amp;quot;partition 1&amp;quot;, &amp;quot;tags&amp;quot; : [ &amp;quot;tag2&amp;quot; ], &amp;quot;members&amp;quot; : { &amp;quot;ids&amp;quot; : [ &amp;quot;x0c0s0b0n0&amp;quot;, &amp;quot;x0c0s0b0n1&amp;quot;, &amp;quot;x0c0s0b1n0&amp;quot; ] }, } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.</description>
    </item>
    
    <item>
      <title>Manage Hms Locks</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/manage_hms_locks/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/manage_hms_locks/</guid>
      <description>Manage HMS Locks This section describes how to check the status of a lock, disable reservations, and repair reservations. The disable and repair operations only affect the ability to make reservations on hardware devices.
Some of the common scenarios an admin might encounter when working with the Hardware State Manager (HSM) Locking API are also described.
Check Lock Status Use the following command to verify if an xname is locked or not.</description>
    </item>
    
    <item>
      <title>Create A Backup Of The HSM Postgres Database</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:51 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/</guid>
      <description>Create a Backup of the HSM Postgres Database Perform a manual backup of the contents of the Hardware State Manager (HSM) Postgres database. This backup can be used to restore the contents of the HSM Postgres database at a later point in time using the Restore HSM Postgres from Backup procedure.
Prerequisites   Healthy HSM Postgres Cluster.
Use patronictl list on the HSM Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:</description>
    </item>
    
    <item>
      <title>Hardware Management Services (hms) Locking Api</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/hardware_management_services_hms_locking_api/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:51 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/hardware_management_services_hms_locking_api/</guid>
      <description>Hardware Management Services (HMS) Locking API The locking feature is a part of the Hardware State Manager (HSM) API. The locking API enables administrators to lock xnames on the system. Locking xnames ensures other system actors, such as administrators or running services, cannot perform a firmware update with the Firmware Action Service (FAS) or a power state change with the Cray Advanced Platform Monitoring and Control (CAPMC). Locks only constrain FAS and CAPMC from each other and help ensure that a firmware update action will not be interfered with by a request to power off the device through CAPMC.</description>
    </item>
    
    <item>
      <title>Hardware State Manager (HSM)</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:51 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager/</guid>
      <description>Hardware State Manager (HSM) The Hardware State Manager (HSM) monitors and interrogates hardware components in the HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur.
In the CSM 0.9.3 release, v1 of the HSM API has begun its deprecation process in favor of the new HSM v2 API. Refer to the HSM API documentation for more information on the changes.</description>
    </item>
    
    <item>
      <title>HSM Roles And Subroles</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/hsm_roles_and_subroles/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:51 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/hsm_roles_and_subroles/</guid>
      <description>HSM Roles and Subroles The Hardware State Manager (HSM) contains several pre-defined roles and subroles that can be assigned to components and used to target specific hardware devices.
Roles and subroles assignments come from the System Layout Service (SLS) and are applied by HSM when a node is discovered.
HSM Roles The following is a list of all pre-defined roles:
 Management Compute Application Service System Storage  The Management role refers to NCNs and will generally have the Master, Worker, or Storage subrole assigned.</description>
    </item>
    
    <item>
      <title>Add An NCN To The HSM Database</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/</guid>
      <description>Add an NCN to the HSM Database This procedure details how to customize the bare-metal non-compute node (NCN) on a system and add the NCN to the Hardware State Manager (HSM) database.
The examples in this procedure use ncn-w0003-nmn as the Customer Access Node (CAN). Use the correct CAN for the system.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. The initial software installation is complete.</description>
    </item>
    
    <item>
      <title>Component Group Members</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/component_group_members/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/component_group_members/</guid>
      <description>Component Group Members The members object in the group definition has additional actions available for managing the members after the group has been created.
The following is an example of group members:
{ &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;,&amp;#34;x0c0s0b0n1&amp;#34;,&amp;#34;x0c0s0b1n0&amp;#34; ] } Retrieve Group Members Retrieve just the members array for a group:
ncn-m# cray hsm groups members list GROUP_LABEL Retrieve only the members of a group that are also in a specific partition:</description>
    </item>
    
    <item>
      <title>Component Groups And Partitions</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/component_groups_and_partitions/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/component_groups_and_partitions/</guid>
      <description>Component Groups and Partitions The Hardware State Manager (HSM) provides the group and partition services. Both are means of grouping (also known as labeling) system components that are tracked by HSM. Components include the nodes, blades, controllers, and more on a system.
There is no limit to the number of members a group or partition contains. The only limitation is that all members must be actual members of the system. The HSM needs to know that the components exist.</description>
    </item>
    
    <item>
      <title>Component Memberships</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/component_memberships/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/component_memberships/</guid>
      <description>Component Memberships Memberships are a read-only resource that is generated automatically by changes to groups and partitions. Each component in /hsm/v2/State/Components is represented. Filter options are available to prune the list, or a specific xname ID can be given. All groups and the partition (if any) of each component are listed.
At this point in time, only information about node components is needed. The --type node filter option is used in the commands below to retrieve information about node memberships only.</description>
    </item>
    
    <item>
      <title>Component Partition Members</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/component_partition_members/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/component_partition_members/</guid>
      <description>Component Partition Members The members object in the partition definition has additional actions available for managing the members after the partition has been created.
The following is an example of partition members:
{ &amp;quot;ids&amp;quot; : [ &amp;quot;x0c0s0b0n0&amp;quot;,&amp;quot;x0c0s0b0n1&amp;quot;,&amp;quot;x0c0s0b1n0&amp;quot;,&amp;quot;x0c0s0b1n1&amp;quot; ] } Retrieve Partition Members Retrieving members of a partition is very similar to how group members are retrieved and modified. No filtering options are available in partitions. However, there are partition and group filtering parameters for the /hsm/v2/State/Components and /hsm/v2/memberships collections, with both essentially working the same way.</description>
    </item>
    
    <item>
      <title>Add A Switch To The HSM Database</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/</guid>
      <description>Add a Switch to the HSM Database Manually add a switch to the Hardware State Manager (HSM) database. Switches need to be in the HSM database in order to update their firmware with the Firmware Action Service (FAS).
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Add the switch to the HSM database.
The --rediscover-on-update true flag forces HSM to discover the switch.</description>
    </item>
    
    <item>
      <title>FAS Use Cases</title>
      <link>/docs-csm/en-12/operations/firmware/fas_use_cases/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_use_cases/</guid>
      <description>FAS Use Cases Use the Firmware Action Service (FAS) to update the firmware on supported hardware devices. Each procedure includes the prerequisites and example recipes required to update the firmware.
 Update Liquid-Cooled Node Update Chassis Management Module (CMM) Firmware Update NCN BIOS and BMC Firmware with FAS Update Liquid-Cooled Compute Node BIOS Firmware Compute Node BIOS Workaround for HPE CRAY EX425  NOTE to update Switch Controllers (sC) or RouterBMC refer to the Rosetta Documentation Update Liquid-Cooled Nodes Update a liquid-cooled node controller (nC) firmware using FAS.</description>
    </item>
    
    <item>
      <title>Recipes</title>
      <link>/docs-csm/en-12/operations/firmware/fas_recipes/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_recipes/</guid>
      <description>Recipes The following example JSON files are useful to reference when updating specific hardware components. In all of these examples, the overrideDryrun field will be set to false; set them to true to perform a live update.
When updating an entire system, walk down the device hierarchy component type by component type, starting first with &amp;lsquo;Routers&amp;rsquo; (switches), proceeding to Chassis, and then finally to Nodes. While this is not strictly necessary, it does help eliminate confusion.</description>
    </item>
    
    <item>
      <title>Update Firmware With FAS</title>
      <link>/docs-csm/en-12/operations/firmware/update_firmware_with_fas/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/update_firmware_with_fas/</guid>
      <description>Update Firmware with FAS The Firmware Action Service (FAS) provides an interface for managing firmware versions of Redfish-enabled hardware in the system. FAS interacts with the Hardware State Managers (HSM), device data, and image data in order to update firmware.
Reset Gigabyte node BMC to factory defaults if having problems with ipmitool, using Redfish, or when flashing procedures fail. See Set Gigabyte Node BMC to Factory Defaults.
FAS images contain the following information that is needed for a hardware device to update firmware versions:</description>
    </item>
    
    <item>
      <title>Upload BMC Recovery Firmware Into Tftp Server</title>
      <link>/docs-csm/en-12/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/</guid>
      <description>Upload BMC Recovery Firmware into TFTP Server cray-upload-recovery-images is a utility for uploading the BMC recovery files for ChassisBMCs, NodeBMCs, and RouterBMCs to be served by the cray-tftp service. The tool uses the cray cli (fas, artifacts) and cray-tftp to download the s3 recovery images (as remembered by FAS) then upload them into the PVC that is used by cray-tftp. cray-upload-recovery-images should be run on every system.
Procedure   Execute the cray-upload-recovery-images script.</description>
    </item>
    
    <item>
      <title>FAS Admin Procedures</title>
      <link>/docs-csm/en-12/operations/firmware/fas_admin_procedures/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_admin_procedures/</guid>
      <description>FAS Admin Procedures 
Procedures for leveraging the Firmware Action Service (FAS) CLI to manage firmware.
Topics  FAS Admin Procedures  Topics Warning for Non-Compute Nodes (NCNs) Ignore Nodes within FAS  Procedure   Override an Image for an Update Procedure Check for New Firmware Versions with a Dry-Run  Procedure   Load Firmware from Nexus Load Firmware from RPM or ZIP file     Warning for Non-Compute Nodes (NCNs) WARNING: NCNs should be locked with the HSM locking API to ensure they are not unintentionally updated by FAS.</description>
    </item>
    
    <item>
      <title>FAS Cli</title>
      <link>/docs-csm/en-12/operations/firmware/fas_cli/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_cli/</guid>
      <description>FAS CLI This section describes the basic capabilities of the Firmware Action Service (FAS) CLI commands. These commands can be used to manage firmware for system hardware supported by FAS. Refer to the prerequisites section before proceeding to any of the sections for the supported operations.
The following CLI operations are described:
 Action  Execute an Action Abort an Action Describe an Action   Snapshots  Create a Snapshot View a Snapshot List Snapshots   Update an Image FAS Loader Commands  Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    
    <item>
      <title>FAS Filters</title>
      <link>/docs-csm/en-12/operations/firmware/fas_filters/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_filters/</guid>
      <description>FAS Filters FAS uses five primary filters for actions and snapshots to determine what operations to create. The filters are listed below:
 Selection Filters - Determine what operations will be created. The following selection filters are available:  stateComponentFilter targetFilter inventoryHardwareFilter  imageFilter   Command Filters - Determine how the operations will be executed. The following command filters are available:  command    All filters are logically connected with AND logic.</description>
    </item>
    
    <item>
      <title>Troubleshoot Conman Failing To Connect To A Console</title>
      <link>/docs-csm/en-12/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/</guid>
      <description>Troubleshoot ConMan Failing to Connect to a Console There are many reasons that conman may not be able to connect to a specific console. This procedure outlines several things to check that may impact the connectivity with a console.
Prerequisites This procedure requires administrative privileges.
Procedure   Check for general network availability.
If the Kubernetes worker node the cray-console-node pod that is attempting to connect with a console cannot access that network address the connection will fail.</description>
    </item>
    
    <item>
      <title>Establish A Serial Connection To NCNs</title>
      <link>/docs-csm/en-12/operations/conman/establish_a_serial_connection_to_ncns/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/establish_a_serial_connection_to_ncns/</guid>
      <description>Establish a Serial Connection to NCNs The ConMan pod can be used to establish a serial console connection with each non-compute node (NCN) in the system. In the scenario of a power down or reboot of an NCN worker, it will be important to check which NCN the conman pod is running on, and to ensure that it is NOT running on the NCN worker that will be impacted. If the NCN worker (where ConMan is running) is powered down or rebooted, the ConMan pod will be unavailable until the node comes back up or until the ConMan pod has terminated on the downed NCN worker node and come up on another NCN worker node.</description>
    </item>
    
    <item>
      <title>Log In To A Node Using Conman</title>
      <link>/docs-csm/en-12/operations/conman/log_in_to_a_node_using_conman/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/log_in_to_a_node_using_conman/</guid>
      <description>Log in to a Node Using ConMan This procedure shows how to connect to the node&amp;rsquo;s Serial Over Lan (SOL) via ConMan.
Prerequisites The user performing this procedure needs to have access permission to the cray-console-operator and cray-console-node pods.
Procedure   Log in to a non-compute node (NCN) that acts as the Kubernetes master or worker. This procedure assumes that it is being carried out on an NCN acting as a Kubernetes master.</description>
    </item>
    
    <item>
      <title>Manage Node Consoles</title>
      <link>/docs-csm/en-12/operations/conman/manage_node_consoles/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/manage_node_consoles/</guid>
      <description>Manage Node Consoles ConMan is used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.
ConMan runs on the system in a set of containers within Kubernetes pods named cray-console-operator and cray-console-node.
The cray-console-operator and cray-console-node pods determine which nodes they should monitor by checking with the Hardware State Manager (HSM) service. They do this once when they starts.</description>
    </item>
    
    <item>
      <title>Troubleshoot Conman Asking For Password On SSH Connection</title>
      <link>/docs-csm/en-12/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/</guid>
      <description>Troubleshoot ConMan Asking for Password on SSH Connection If ConMan starts to ask for a password when there is an SSH connection to the node on liquid-cooled hardware, that usually indicates there is a problem with the SSH key that was established on the node BMC. The key may have been replaced or overwritten on the hardware.
Use this procedure to renew or reinstall the SSH key on the BMCs.</description>
    </item>
    
    <item>
      <title>Troubleshoot Conman Blocking Access To A Node BMC</title>
      <link>/docs-csm/en-12/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/</guid>
      <description>Troubleshoot ConMan Blocking Access to a Node BMC Disable ConMan if it is blocking access to a node by other means. ConMan runs on the system as a containerized service, and it is enabled by default. However, the use of ConMan to connect to a node blocks access to that node by other Serial over LAN (SOL) utilities or by a virtual KVM.
For information about how ConMan works, see ConMan.</description>
    </item>
    
    <item>
      <title>Access Compute Node Logs</title>
      <link>/docs-csm/en-12/operations/conman/access_compute_node_logs/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/access_compute_node_logs/</guid>
      <description>Access Compute Node Logs This procedure shows how the ConMan utility can be used to retrieve compute node logs.
Prerequisites The user performing this procedure needs to have access permission to the cray-console-operator pod.
Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.
Procedure   Log on to a Kubernetes master or worker node.
  Retrieve the cray-console-operator pod ID.</description>
    </item>
    
    <item>
      <title>Access Console Log Data Via The System Monitoring Framework (smf)</title>
      <link>/docs-csm/en-12/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/</guid>
      <description>Access Console Log Data Via the System Monitoring Framework (SMF) Console log data is collected by SMF and can be queried through the Kibana UI or Elasticsearch. Each line of the console logs are an individual record in the SMF database.
Prerequisites This procedure requires the Kibana service to be up and running on a non-compute node (NCN).
Procedure   Determine the external domain name by running the following command on any NCN:</description>
    </item>
    
    <item>
      <title>Configure The Cray Command Line Interface (`cray` Cli)</title>
      <link>/docs-csm/en-12/operations/configure_cray_cli/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configure_cray_cli/</guid>
      <description>Configure the Cray Command Line Interface (cray CLI) The cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.
Later procedures in the installation workflow use the cray CLI to interact with multiple services. The cray CLI configuration needs to be initialized for the Linux account, and the Keycloak user running the procedure needs to be authorized. This section describes how to initialize the cray CLI for use by a user and how to authorize that user.</description>
    </item>
    
    <item>
      <title>Conman</title>
      <link>/docs-csm/en-12/operations/conman/conman/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/conman/</guid>
      <description>ConMan ConMan is a tool used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.
ConMan runs on the system as a containerized service. It runs in a set of Docker containers within Kubernetes pods named cray-console-operator and cray-console-node. ConMan can be configured to encrypt usernames and passwords. Node console logs are stored locally within the cray-console-operator pod in the /var/log/conman/ directory, as well as being collected by the System Monitoring Framework (SMF).</description>
    </item>
    
    <item>
      <title>Disable Conman After The System Software Installation</title>
      <link>/docs-csm/en-12/operations/conman/disable_conman_after_system_software_installation/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/disable_conman_after_system_software_installation/</guid>
      <description>Disable ConMan After the System Software Installation The ConMan utility is enabled by default. This procedure provides instructions for disabling it after the system software has been installed.
Prerequisites This procedure requires administrative privileges.
Procedure   Log on to a non-compute node (NCN) that acts as a Kubernetes master. This procedure assumes that it is being carried out on an NCN acting as a Kubernetes master.
  Scale the cray-console-operator pods to 0 replicas.</description>
    </item>
    
    <item>
      <title>Version Control Service (vcs)</title>
      <link>/docs-csm/en-12/operations/configuration_management/version_control_service_vcs/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:45 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/version_control_service_vcs/</guid>
      <description>Version Control Service (VCS) The Version Control Service (VCS) includes a web interface for repository management, pull requests, and a visual view of all repositories and organizations. The following URL is for the VCS web interface:
https://vcs.SHASTA_CLUSTER_DNS_NAME On cluster nodes, the VCS service can be accessed through the gateway. VCS credentials for the crayvcs user are required before cloning a repository (see the &amp;ldquo;VCS Administrative User&amp;rdquo; section below). To clone a repository in the cray organization, use the following command:</description>
    </item>
    
    <item>
      <title>View Configuration Session Logs</title>
      <link>/docs-csm/en-12/operations/configuration_management/view_configuration_session_logs/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:45 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/view_configuration_session_logs/</guid>
      <description>View Configuration Session Logs Logs for the individual steps of a session are available via the kubectl log command for each container of a Configuration Framework Service (CFS) session. Refer to Configuration Sessions for more info about these containers.
To find the name of the Kubernetes pod that is running the CFS session:
ncn# kubectl get pods --no-headers -o \ custom-columns=&amp;#34;:metadata.name&amp;#34; -n services -l cfsession=example cfs-f9d18751-e6d1-4326-bf76-434293a7b1c5-q8tsc Store the returned pod name as the CFS_POD_NAME variable for future use:</description>
    </item>
    
    <item>
      <title>Write Ansible Code For CFS</title>
      <link>/docs-csm/en-12/operations/configuration_management/write_ansible_code_for_cfs/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:45 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/write_ansible_code_for_cfs/</guid>
      <description>Write Ansible Code for CFS Cray provides Ansible plays and roles for software products deemed necessary for the system to function. Customers are free to write their own Ansible plays and roles to augment what Cray provides or implement new features. Basic knowledge of Ansible is needed to write plays and roles. The information below includes recommendations and best practices for writing and running Ansible code on the system successfully with the Configuration Framework Service (CFS).</description>
    </item>
    
    <item>
      <title>Update The Privacy Settings For Gitea Configuration Content Repositories</title>
      <link>/docs-csm/en-12/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:44 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/</guid>
      <description>Update the Privacy Settings for Gitea Configuration Content Repositories Change the visibility of Gitea configuration content repositories from public to private. All Cray-provided repositories are created as private by default.
Procedure   Log in to the Version Control Service (VCS) as the crayvcs user.
Use the following URL to access the VCS web interface:
https://vcs.SYSTEM-NAME.DOMAIN-NAME   Navigate to the cray organization.
https://vcs.SYSTEM-NAME.DOMAIN-NAME/vcs/cray   Select the repository title for each repository listed on the page.</description>
    </item>
    
    <item>
      <title>Use A Custom Ansible-cfg File</title>
      <link>/docs-csm/en-12/operations/configuration_management/use_a_custom_ansible-cfg_file/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:44 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/use_a_custom_ansible-cfg_file/</guid>
      <description>Use a Custom ansible-cfg File The Configuration Framework Service (CFS) allows for flexibility with the Ansible Execution Environment (AEE) by allowing for changes to included ansible.cfg file. When installed, CFS imports a custom ansible.cfg file into the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.
Administrators who want to make changes to the ansible.cfg file on a per-session or system-wide basis can upload a new file to a new ConfigMap and direct CFS to use their file.</description>
    </item>
    
    <item>
      <title>Use A Specific Inventory In A Configuration Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:44 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/</guid>
      <description>Use a Specific Inventory in a Configuration Session A special repository can be added to a Configuration Framework Service (CFS) configuration to help with certain scenarios, specifically when developing Ansible plays for use on the system. A static inventory often changes along with the Ansible content, and CFS users may need to test different configuration values simultaneously and not be forced to use the global additionalInventoryUrl.
Therefore, an additional_inventory mapping can be added to the CFS configuration.</description>
    </item>
    
    <item>
      <title>Vcs Branching Strategy</title>
      <link>/docs-csm/en-12/operations/configuration_management/vcs_branching_strategy/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:44 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/vcs_branching_strategy/</guid>
      <description>VCS Branching Strategy Individual products import configuration content (Ansible plays, roles, and more) into a repository in the Version Control Service (VCS) through their installation process. Typically, this repository is of the following form and exists in the cray organization in VCS:
[product name]-config-management The import branch of the product is considered &amp;ldquo;pristine content&amp;rdquo; and is added to VCS in a read-only branch. This step is taken to ensure the future updates of the product&amp;rsquo;s configuration content can be based on a clean branch, and that the upgrade procedure can proceed without merging issues.</description>
    </item>
    
    <item>
      <title>Troubleshoot Ansible Play Failures In CFS Sessions</title>
      <link>/docs-csm/en-12/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/</guid>
      <description>Troubleshoot Ansible Play Failures in CFS Sessions View the Kubernetes logs for a Configuration Framework Service (CFS) pod in an error state to determine whether the error resulted from the CFS infrastructure or from an Ansible play that was run by a specific configuration layer in a CFS session.
Use this procedure to obtain important triage information for Ansible plays being called by CFS.
Prerequisites A configuration session exists for CFS.</description>
    </item>
    
    <item>
      <title>Troubleshoot CFS Session Failing To Complete</title>
      <link>/docs-csm/en-12/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/</guid>
      <description>Troubleshoot CFS Session Failing to Complete Troubleshoot issues where Configuration Framework Service (CFS) sessions/pods fail and Ansible hangs. These issues can be resolved by modifying Ansible to produce less output.
Prerequisites A CFS session or pod is failing to complete, and the Ansible logs are not showing progress or completion.
The following is an example of the error causing Ansible to hang:
PLAY [Compute] ***************************************************************** META: ran handlers META: ran handlers META: ran handlers PLAY [Compute] ***************************************************************** Using module file /usr/lib/python3.</description>
    </item>
    
    <item>
      <title>Update A CFS Configuration</title>
      <link>/docs-csm/en-12/operations/configuration_management/update_a_cfs_configuration/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/update_a_cfs_configuration/</guid>
      <description>Update a CFS Configuration Modify a Configuration Framework Service (CFS) configuration by specifying the JSON of the configuration and its layers. Use the cray cfs configurations update command, similar to creating a configuration.
Prerequisites  A CFS configuration has been created. The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Add and/or remove the configuration layers from an existing JSON configuration file.</description>
    </item>
    
    <item>
      <title>Set The `ansible.cfg` For A Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/set_the_ansible-cfg_for_a_session/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/set_the_ansible-cfg_for_a_session/</guid>
      <description>Set the ansible.cfg for a Session View and update the Ansible configuration used by CFS.
Ansible configuration is available through the ansible.cfg file. See the Configuring Ansible external documentation for more information about what values can be set.
The Configuration Framework Service (CFS) provides a default ansible.cfg file in the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.
To view the ansible.cfg file:
ncn# kubectl get cm -n services cfs-default-ansible-cfg \ -o json | jq -r &amp;#39;.</description>
    </item>
    
    <item>
      <title>Target Ansible Tasks For Image Customization</title>
      <link>/docs-csm/en-12/operations/configuration_management/target_ansible_tasks_for_image_customization/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/target_ansible_tasks_for_image_customization/</guid>
      <description>Target Ansible Tasks for Image Customization The Configuration Framework Service (CFS) enables Ansible playbooks to run against both running nodes and images. See the &amp;ldquo;Use Cases&amp;rdquo; header in the Configuration Management section for more information about image customization and when it should be used.
CFS uses the cray_cfs_image variable to distinguish between node personalization (running on live nodes) and image customization (configuring an image prior to boot). When this variable is set to true, it indicates that the CFS session is an image customization type and the playbook is targeting an image.</description>
    </item>
    
    <item>
      <title>Track The Status Of A Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/track_the_status_of_a_session/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/track_the_status_of_a_session/</guid>
      <description>Track the Status of a Session A configuration session can be a long-running process, and depends on many system factors, as well as the number of configuration layers and Ansible tasks that are run in each layer. The Configuration Framework Service (CFS) provides the session status through the session metadata to allow for tracking progress and session state.
To view the session status of a session named example, use the following command:</description>
    </item>
    
    <item>
      <title>Manage Multiple Inventories In A Single Location</title>
      <link>/docs-csm/en-12/operations/configuration_management/manage_multiple_inventories_in_a_single_location/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:41 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/manage_multiple_inventories_in_a_single_location/</guid>
      <description>Manage Multiple Inventories in a Single Location Many configuration layers may be present in a single configuration for larger systems that configure multiple Cray products. When values for each of these layers need to be customized, it can be tedious to override values in each of the respective repositories. The CFS additionalInventoryUrl option allows for static inventory files to be automatically added to the hosts directory of each configuration layer before it is applied by Ansible.</description>
    </item>
    
    <item>
      <title>Set Limits For A Configuration Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/set_limits_for_a_configuration_session/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:41 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/set_limits_for_a_configuration_session/</guid>
      <description>Set Limits for a Configuration Session The configuration layers and session hosts can be limited when running a Configuration Framework Service (CFS) session.
Limit CFS Session Hosts Subsets of nodes can be targeted in the inventory when running CFS sessions, which is useful specifically when running a session with dynamic inventory. Use the CFS --ansible-limit option when creating a session to apply the limits. The option directly corresponds to the --limit option offered by ansible-playbook, and can be used to specify hosts, groups, or combinations of them with patterns.</description>
    </item>
    
    <item>
      <title>Delete CFS Sessions</title>
      <link>/docs-csm/en-12/operations/configuration_management/delete_cfs_sessions/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:40 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/delete_cfs_sessions/</guid>
      <description>Delete CFS Sessions Delete an existing Configuration Framework Service (CFS) configuration session with the CFS delete command.
Use the session name to delete the session:
ncn# cray cfs sessions delete example &amp;lt;no output expected&amp;gt; To delete all completed CFS sessions, use the deleteall command. This command can also filter the sessions to delete based on tags, name, status, age, and success or failure. By default, if no other filter is specified, this command only deletes completed sessions.</description>
    </item>
    
    <item>
      <title>Enable Ansible Profiling</title>
      <link>/docs-csm/en-12/operations/configuration_management/enable_ansible_profiling/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:40 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/enable_ansible_profiling/</guid>
      <description>Enable Ansible Profiling Ansible tasks and playbooks can be profiled in order to determine execution times and single out poor performance in runtime. The default Configuration Framework Service (CFS) ansible.cfg in the cfs-default-ansible-cfg ConfigMap does not enable these profiling tools. If profiling tools are desired, modify the default Ansible configuration file to enable them.
Procedure   Edit the cfs-default-ansible-cfg ConfigMap.
ncn# kubectl edit cm cfs-default-ansible-cfg -n services   Uncomment the indicated line by removing the # character from the beginning of the line.</description>
    </item>
    
    <item>
      <title>Git Operations</title>
      <link>/docs-csm/en-12/operations/configuration_management/git_operations/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:40 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/git_operations/</guid>
      <description>Git Operations Use the git command to manage repository content in the Version Control Service (VCS).
Once a repository is cloned, the git command line tool is available to interact with a repository from the Version Control Service (VCS). The git command is used for making commits, creating new branches, and pushing new branches, tags, and commits to the remote repository stored in VCS.
When pushing changes to the VCS server using the crayvcs user, input the password retrieved from the Kubernetes secret as the credentials.</description>
    </item>
    
    <item>
      <title>Create And Populate A Vcs Configuration Repository</title>
      <link>/docs-csm/en-12/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:39 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/</guid>
      <description>Create and Populate a VCS Configuration Repository Create a new repository in the VCS and populate it with content for site customizations in a custom Configuration Framework Service (CFS) configuration layer.
Prerequisites  The Version Control Service (VCS) login credentials for the crayvcs user are set up. See the &amp;ldquo;VCS Administrative User&amp;rdquo; heading in Version Control Service (VCS) for more information.  Procedure   Create the empty repository in VCS.</description>
    </item>
    
    <item>
      <title>Customize Configuration Values</title>
      <link>/docs-csm/en-12/operations/configuration_management/customize_configuration_values/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:39 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/customize_configuration_values/</guid>
      <description>Customize Configuration Values In general, most systems will require some customization from the default values provided by Cray products. As stated in the previous section, these changes cannot be made on the pristine product branches that are imported during product installation and upgrades. Changes can only be made in Git branches that are based on the pristine branches.
Changing or overriding default values should be done in accordance with Ansible best practices (see the external Ansible best practices guide) and variable precedence (see the external Ansible variable guide) in mind.</description>
    </item>
    
    <item>
      <title>Create A CFS Configuration</title>
      <link>/docs-csm/en-12/operations/configuration_management/create_a_cfs_configuration/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:38 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/create_a_cfs_configuration/</guid>
      <description>Create a CFS Configuration Create a Configuration Framework Service (CFS) configuration, which contains an ordered list of layers. Each layer is defined by a Git repository clone URL, a Git commit, a name, and the path in the repository to an Ansible playbook to execute.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Create a JSON file to hold data about the CFS configuration.</description>
    </item>
    
    <item>
      <title>Create A CFS Session With Dynamic Inventory</title>
      <link>/docs-csm/en-12/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:38 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/</guid>
      <description>Create a CFS Session with Dynamic Inventory A Configuration Framework Service (CFS) session using dynamic inventory is used to configure live nodes. To create a CFS session using the default dynamic inventory, simply provide a session name and the name of the configuration to apply:
ncn# cray cfs sessions create --name example \ --configuration-name configurations-example { &amp;#34;ansible&amp;#34;: { &amp;#34;config&amp;#34;: &amp;#34;cfs-default-ansible-cfg&amp;#34;, &amp;#34;limit&amp;#34;: &amp;#34;, &amp;#34;verbosity&amp;#34;: 0 }, &amp;#34;configuration&amp;#34;: { &amp;#34;limit&amp;#34;: &amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;configurations-example&amp;#34; }, &amp;#34;name&amp;#34;: &amp;#34;example&amp;#34;, &amp;#34;status&amp;#34;: { &amp;#34;artifacts&amp;#34;: [], &amp;#34;session&amp;#34;: { &amp;#34;status&amp;#34;: &amp;#34;pending&amp;#34;, &amp;#34;succeeded&amp;#34;: &amp;#34;none&amp;#34; } }, &amp;#34;tags&amp;#34;: {}, &amp;#34;target&amp;#34;: { &amp;#34;definition&amp;#34;: &amp;#34;dynamic&amp;#34;, &amp;#34;groups&amp;#34;: null } } Add the --target-definition dynamic parameter to the create command to explicitly define the inventory type to be dynamic.</description>
    </item>
    
    <item>
      <title>Create An Image Customization CFS Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/create_an_image_customization_cfs_session/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:38 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/create_an_image_customization_cfs_session/</guid>
      <description>Create an Image Customization CFS Session A configuration session that is meant to customize image roots tracked by the Image Management Service (IMS) can be created using the --target-definition image option. This option will instruct the Configuration Framework Service (CFS) to prepare the image IDs specified and assign them to the groups specified in Ansible inventory. IMS will then provide SSH connection information to each image root that CFS will use to configure Ansible.</description>
    </item>
    
    <item>
      <title>Configuration Management Of System Components</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_management_of_system_components/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:37 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_management_of_system_components/</guid>
      <description>Configuration Management of System Components The configuration of individual system components is managed with the cray cfs components command. The Configuration Framework Service (CFS) contains a database of the configuration state of available hardware known to the Hardware State Manager (HSM). When new nodes are added to the HSM database, a CFS Hardware Sync Agent enters the component into the CFS database with a null state of configuration.
Administrators are able to set a desired CFS configuration for each component, and the CFS Batcher ensures the desired configuration state and the current configuration state match.</description>
    </item>
    
    <item>
      <title>Configuration Management With The CFS Batcher</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_management_with_the_cfs_batcher/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:37 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_management_with_the_cfs_batcher/</guid>
      <description>Configuration Management with the CFS Batcher Creating configuration sessions with the Configuration Framework Service (CFS) enables remote execution for configuring live nodes and boot images prior to booting. CFS also provides its Batcher component for configuration management of registered system components. The CFS Batcher periodically examines the aggregated configuration state of registered components and schedules CFS sessions against those that have not been configured to their desired state. The frequency of scheduling, the maximum number of components to schedule in the same CFS session, and the expiration time for scheduling less than full sessions are configurable.</description>
    </item>
    
    <item>
      <title>Configuration Sessions</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_sessions/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:37 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_sessions/</guid>
      <description>Configuration Sessions Once configurations have been created with the required layers and values set in the configuration repositories (or the additional inventory repository), create a Configuration Framework Session (CFS) session to apply the configuration to the targets.
Sessions are created via the Cray CLI or through the CFS REST API. A session stages Ansible inventory (whether dynamic, static, or image customization), launches Ansible Execution Environments (AEE) in order for each configuration layer in the service mesh, tears down the environments as required, and reports the session status to the CFS API.</description>
    </item>
    
    <item>
      <title>Configuration Layers</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_layers/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:36 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_layers/</guid>
      <description>Configuration Layers The Configuration Framework Service (CFS) uses configuration layers to specify the location of configuration content that will be applied. Configurations may include one or more layers. Each layer is defined by a Git repository clone URL, a Git commit, a name (optional), and the path in the repository to an Ansible playbook to execute.
Configurations with a single layer are useful when testing out a new configuration on targets, or when configuring system components with one product at a time.</description>
    </item>
    
    <item>
      <title>Configuration Management</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_management/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:36 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_management/</guid>
      <description>Configuration Management The Configuration Framework Service (CFS) is available on systems for remote execution and configuration management of nodes and boot images. This includes nodes available in the Hardware State Manager (HSM) inventory (compute, non-compute, application, and user access nodes), and boot images hosted by the Image Management Service (IMS).
CFS configures nodes and images via a gitops methodology. All configuration content is stored in a version control service (VCS), and is managed by authorized system administrators.</description>
    </item>
    
    <item>
      <title>Automatic Session Deletion With `sessionttl`</title>
      <link>/docs-csm/en-12/operations/configuration_management/automatic_session_deletion_with_sessionttl/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:35 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/automatic_session_deletion_with_sessionttl/</guid>
      <description>Automatic Session Deletion with sessionTTL By default, when a session completes, the Configuration Framework Service (CFS) will delete sessions and Kubernetes jobs associated with the session when the start date was more than seven days prior. This is done to ensure CFS sessions do not accumulate and eventually adversely affect the performance of the Kubernetes cluster.
For larger systems or systems that do frequent reboots of nodes that are configured with CFS sessions, this setting may need to be reduced.</description>
    </item>
    
    <item>
      <title>CFS Global Options</title>
      <link>/docs-csm/en-12/operations/configuration_management/cfs_global_options/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:35 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/cfs_global_options/</guid>
      <description>CFS Global Options The Configuration Framework Service (CFS) provides a global service options endpoint for modifying the base configuration of the service itself.
View the options with the following command:
ncn# cray cfs options list --format json { &amp;#34;additionalInventoryUrl&amp;#34;: &amp;#34;, &amp;#34;batchSize&amp;#34;: 25, &amp;#34;batchWindow&amp;#34;: 60, &amp;#34;batcherCheckInterval&amp;#34;: 10, &amp;#34;defaultAnsibleConfig&amp;#34;: &amp;#34;cfs-default-ansible-cfg&amp;#34;, &amp;#34;defaultBatcherRetryPolicy&amp;#34;: 1, &amp;#34;defaultPlaybook&amp;#34;: &amp;#34;site.yml&amp;#34;, &amp;#34;hardwareSyncInterval&amp;#34;: 10, &amp;#34;sessionTTL&amp;#34;: &amp;#34;7d&amp;#34; } The following are the CFS global options:
  additionalInventoryUrl
A Git clone URL to supply additional inventory content to all CFS sessions.</description>
    </item>
    
    <item>
      <title>Change The Ansible VerBOSity Logs</title>
      <link>/docs-csm/en-12/operations/configuration_management/change_the_ansible_verbosity_logs/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:35 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/change_the_ansible_verbosity_logs/</guid>
      <description>Change the Ansible Verbosity Logs It is useful to view the Ansible logs in a Configuration Framework Session (CFS) session with greater verbosity than the default. CFS sessions are able to set the Ansible verbosity from the command line when the session is created. The verbosity will apply to all configuration layers in the session.
Specify an integer using the &amp;ndash;ansible-verbosity option, where 1 = -v, 2 = -vv, and so on.</description>
    </item>
    
    <item>
      <title>Ansible Execution Environments</title>
      <link>/docs-csm/en-12/operations/configuration_management/ansible_execution_environments/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:34 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/ansible_execution_environments/</guid>
      <description>Ansible Execution Environments Configuration Framework Service (CFS) sessions are comprised of a single Kubernetes pod with several containers. Inventory and Git clone setup containers run first, and a teardown container runs last (if the session is running an image customization).
The containers that run the Ansible code cloned from the Git repositories in the configuration layers are Ansible Execution Environments (AEE). The AEE is provided as a SLES-based docker image, which includes Ansible version 2.</description>
    </item>
    
    <item>
      <title>Ansible Inventory</title>
      <link>/docs-csm/en-12/operations/configuration_management/ansible_inventory/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:34 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/ansible_inventory/</guid>
      <description>Ansible Inventory The Configuration Framework Service (CFS) provides several options for targeting nodes or boot images for configuration by Ansible. The contents of the Ansible inventory determine which nodes are available for configuration in each CFS session and how default configuration values can be customized.
The following are the inventory options provided by CFS:
 Dynamic inventory Static inventory Image Customization  Dynamic Inventory and Host Groups Dynamic inventory is the default inventory when creating a CFS session.</description>
    </item>
    
    <item>
      <title>Troubleshoot A Failed CRUS Session Because Of Unmet Conditions</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:33 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/</guid>
      <description>Troubleshoot a Failed CRUS Session Because of Unmet Conditions If a CRUS session has any unmet conditions, adding or fixing them will cause the session to continue from wherever it got stuck. Updating other parts of the system to meet the required conditions of a CRUS session will unblock the upgrade session.
The following are examples of unmet conditions:
 Undefined groups in the Hardware State Manager (HSM). No predefined Boot Orchestration Service (BOS) session template exists that describes the desired states of the nodes being upgraded.</description>
    </item>
    
    <item>
      <title>Upgrade Compute Nodes With CRUS</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:33 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/</guid>
      <description>Upgrade Compute Nodes with CRUS Upgrade a set of compute nodes with the Compute Rolling Upgrade Service (CRUS). Manage the workload management status of nodes and quiesce each node before taking the node out of service and upgrading it. Then reboot it into the upgraded state and return it to service within the workload manager (WLM).
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    
    <item>
      <title>Compute Rolling Upgrades</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/compute_rolling_upgrades/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:32 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/compute_rolling_upgrades/</guid>
      <description>Compute Rolling Upgrades The Compute Rolling Upgrade Service (CRUS) upgrades sets of compute nodes without requiring an entire set of nodes to be out of service at once. CRUS manages the workload management status of nodes, handling each of the following steps required to upgrade compute nodes:
 Quiesce each node before taking the node out of service. Upgrade the node. Reboot the node into the upgraded state. Return the node to service within its respective workload manager.</description>
    </item>
    
    <item>
      <title>CRUS Workflow</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/crus_workflow/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:32 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/crus_workflow/</guid>
      <description>CRUS Workflow The following workflow is intended to be a high-level overview of how to upgrade compute nodes. This workflow depicts how services interact with each other during the compute node upgrade process, and helps to provide a quicker and deeper understanding of how the system functions.
Upgrade Compute Nodes Use Cases: Administrator upgrades select compute nodes (around 500) to a newer compute image by using Compute Rolling Upgrade Service (CRUS).</description>
    </item>
    
    <item>
      <title>Troubleshoot A Failed CRUS Session Because Of Bad Parameters</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:32 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/</guid>
      <description>Troubleshoot a Failed CRUS Session Because of Bad Parameters A CRUS session must be deleted and recreated if it does not start or complete because of parameters having incorrect values.
The following are examples of incorrect parameters:
 Choosing the wrong Boot Orchestration Service (BOS) session template. Choosing the wrong group labels. Improperly defined BOS session template. For example, specifying nodes in the template instead of using the label of the upgrading group.</description>
    </item>
    
    <item>
      <title>Troubleshoot Nodes Failing To Upgrade In A CRUS Session</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:32 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/</guid>
      <description>Troubleshoot Nodes Failing to Upgrade in a CRUS Session Troubleshoot compute nodes failing to upgrade during a Compute Rolling Upgrade Service (CRUS) session and rerun the session on the failed nodes.
When a nodes are marked as failed they are added to the failed node group associated with the upgrade session, and the nodes are marked as down in the workload manager (WLM). If the WLM supports some kind of reason string, that string contains the cause of the down status.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAN Boot Issues</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_uan_boot_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:31 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_uan_boot_issues/</guid>
      <description>Troubleshoot UAN Boot Issues Use this topic to guide troubleshooting of UAN boot issues.
The UAN boot process BOS boots UANs. BOS uses session templates to define various parameters such as:
 Which nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration (Node Personalization) of the nodes by CFS. Which CFS configuration to use if Node Personalization is enabled.  UAN boots are performed in three phases:</description>
    </item>
    
    <item>
      <title>Upload Node Boot Information To Boot Script Service (bss)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:31 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/</guid>
      <description>Upload Node Boot Information to Boot Script Service (BSS) The following information must be uploaded to BSS as a prerequisite to booting a node via iPXE:
 The location of an initrd image in the artifact repository The location of a kernel image in the artifact repository Kernel boot parameters The node(s) associated with that information, using either host name or NID  BSS manages the iPXE boot scripts that coordinate the boot process for nodes, and it enables basic association of boot scripts with nodes.</description>
    </item>
    
    <item>
      <title>View The Status Of A BOS Session</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/view_the_status_of_a_bos_session/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:31 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/view_the_status_of_a_bos_session/</guid>
      <description>View the Status of a BOS Session The Boot Orchestration Service (BOS) supports a status endpoint that reports the status for individual BOS sessions. The status can be retrieved for each boot set within the session, as well as the individual items within a boot set.
BOS sessions contain one or more boot sets. Each boot set contains one or more phases, depending upon the operation for that session. For example, a reboot operation would have a shutdown, boot, and possibly configuration phase, but a shutdown operation would only have a shutdown phase.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To The Boot Script Service (bss)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:30 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to the Boot Script Service (BSS) Boot Script Service (BSS) delivers a boot script to a node based on its MAC address. This boot script tells the node where to obtain its boot artifacts, which include:
 kernel initrd  In addition, the boot script also contains the kernel boot parameters. This procedure helps resolve issues related to missing boot artifacts.
Prerequisites This procedure requires administrative privileges.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Unified Extensible Firmware Interface (uefi)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:30 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI) If a node is stuck in the UEFI shell, ConMan will be able to connect to it, but nothing else will appear in its logs. The node&amp;rsquo;s logs will look similar to the following, indicating that ConMan is updating its log hourly:
&amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 20:00:00 CDT. &amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 21:00:00 CDT. &amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 22:00:00 CDT.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Using Kubernetes</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kuberentes/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:30 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kuberentes/</guid>
      <description>Troubleshoot Compute Node Boot Issues Using Kubernetes A number of Kubernetes commands can be used to debug issues related to the node boot process. All of the traffic bound for the DHCP server, TFTP server, and Boot Script Service (BSS) is sent on the Node Management Network (NMN).
In the current arrangement, all three services are located on a non-compute node (NCN). Thus, traffic must first travel through the NCN to reach these services inside their pods.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Using Kubernetes</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:30 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/</guid>
      <description>Troubleshoot Compute Node Boot Issues Using Kubernetes A number of Kubernetes commands can be used to debug issues related to the node boot process. All of the traffic bound for the DHCP server, TFTP server, and Boot Script Service (BSS) is sent on the Node Management Network (NMN).
In the current arrangement, all three services are located on a non-compute node (NCN). Thus, traffic must first travel through the NCN to reach these services inside their pods.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Dynamic Host Configuration Protocol (DHCP)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:29 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP) DHCP issues can result in node boot failures. This procedure helps investigate and resolve such issues.
Prerequisites  This procedure requires administrative privileges. kubectl is installed.  Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.
Procedure   Log in to a non-compute node (NCN) as root.
  Check that the DHCP service is running.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Slow Boot Times</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:29 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Slow Boot Times Inspect BOS, the Boot Orchestration Agent (BOA) job logs, and the Configuration Framework Service (CFS) job logs to obtain information that is critical for boot troubleshooting. Use this procedure to determine why compute nodes are booting slower than expected.
Prerequisites A boot session has been created with the Boot Orchestration Service (BOS).
Procedure   View the BOA logs.
  Find the BOA job from the boot session.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Trivial File Transfer Protocol (tftp)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:29 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP) TFTP issues can result in node boot failures. Use this procedure to investigate and resolve such issues.
Prerequisites This procedure requires administrative privileges.
Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.
  Log onto a non-compute node (NCN) as root.
  Check that the TFTP service is running.</description>
    </item>
    
    <item>
      <title>BOS Sessions</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/sessions/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/sessions/</guid>
      <description>BOS Sessions Overview of BOS Session operations and limitations.
The Boot Orchestration Service (BOS) creates a session when it is asked to perform one of these operations:
 Boot - Boot a designated collection of nodes. Shutdown - Shutdown a designated collection of nodes. Reboot - Reboot a designated collection of nodes. Configure - Configure a designated collection of booted nodes.  BOS sessions can be used to boot compute nodes with customized image roots.</description>
    </item>
    
    <item>
      <title>Stage Changes Without BOS</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/stage_changes_without_bos/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/stage_changes_without_bos/</guid>
      <description>Stage Changes Without BOS Sometimes there is a need to stages changes to take place on a reboot, without immediately rebooting a node. When this is called for, users can bypass BOS, and set boot artifacts or configuration that will only take place when a node is later booted, whether that occurs manually, or triggered by a task manager.
Stage Boot Artifacts For information on staging boot artifacts, see the section Upload Node Boot Information to Boot Script Service (BSS).</description>
    </item>
    
    <item>
      <title>Tools For Resolving Compute Node Boot Issues</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/tools_for_resolving_boot_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/tools_for_resolving_boot_issues/</guid>
      <description>Tools for Resolving Compute Node Boot Issues A number of tools can be used to analyze and debug issues encountered during the compute node boot process. The underlying issue and symptoms dictate the type of tool required.
nmap Use nmap to send out DHCP Discover requests to test DHCP. nmap can be installed using the following command:
ncn-m001# zypper install nmap To reach the DHCP server, the request generally needs to be sent over the Node Management network (NMN) from the non-compute node (NCN).</description>
    </item>
    
    <item>
      <title>Troubleshoot Booting Nodes With Hardware Issues</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/</guid>
      <description>Troubleshoot Booting Nodes with Hardware Issues How to identify a node with hardware issues and how to disable is via the HSM.
If a node included in a Boot Orchestration Service (BOS) session template is having hardware issues, it can prevent the node from powering back up correctly. The entire BOS session will fail with a timeout error waiting for the node to become ready.
The following is example log output from a node with hardware issues, resulting in a failed BOS session:</description>
    </item>
    
    <item>
      <title>BOS Session Templates</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/session_templates/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/session_templates/</guid>
      <description>BOS Session Templates Describes the contents of a BOS session template.
A session template can be created by specifying parameters as part of the call to the Boot Orchestration Service (BOS). When calling BOS directly, JSON is passed as part of the call.
Session templates can be used to boot images that are customized with the Image Management Service (IMS). A session template has a collection of one or more boot set objects.</description>
    </item>
    
    <item>
      <title>Node Boot Root Cause Analysis</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/node_boot_root_cause_analysis/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/node_boot_root_cause_analysis/</guid>
      <description>Node Boot Root Cause Analysis The first step in debugging compute node boot-related issues is to determine the underlying cause, and the stage that the issue was encountered at.
The ConMan tool collects compute node logs. To learn more about ConMan, refer to ConMan.
A node&amp;rsquo;s console data can be accessed through its log file, as described in Access Compute Node Logs). This information can also be accessed by connecting to the node&amp;rsquo;s console withipmitool.</description>
    </item>
    
    <item>
      <title>Redeploy The IPXE And Tftp Services</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/</guid>
      <description>Redeploy the iPXE and TFTP Services Redeploy the iPXE and TFTP services if a pod with a ceph-fs Process Virtualization Service (PVS) on a Kubernetes worker node is causing a HEALTH_WARN error.
Resolve issues with ceph-fs and ceph-mds by restarting the iPXE and TFTP services. The Ceph cluster will return to a healthy state after this procedure.
Prerequisites This procedure requires administrative privileges.
Procedure   Find the iPXE and TFTP deployments.</description>
    </item>
    
    <item>
      <title>Log File Locations And Ports Used In Compute Node Boot Troubleshooting</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/</guid>
      <description>Log File Locations and Ports Used in Compute Node Boot Troubleshooting This section includes the port IDs and log file locations of components associated with the node boot process.
Log File Locations The log file locations for ConMan, DHCP, and TFTP.
  ConMan logs are located within the conman pod at /var/log/conman.log.
  DHCP:
ncn-m001# kubectl logs DHCP_POD_ID   TFTP:
ncn-m001# kubectl logs -n services TFTP_POD_ID   Port IDs The following table includes the port IDs for DHCP and TFTP.</description>
    </item>
    
    <item>
      <title>Manage A BOS Session</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/manage_a_bos_session/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/manage_a_bos_session/</guid>
      <description>Manage a BOS Session Once there is a Boot Orchestration Service (BOS) session template created, users can perform operations on nodes, such as boot, reboot, configure, and shutdown. Managing sessions through the Cray CLI can be accomplished using the cray bos session commands.
Create a New Session Creating a new BOS session requires the following command-line options:
 --template-uuid: Use this option to specify the name value returned in the cray bos sessiontemplate list command.</description>
    </item>
    
    <item>
      <title>Manage A Session Template</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/manage_a_session_template/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/manage_a_session_template/</guid>
      <description>Manage a Session Template A session template must be created before starting a session with the Boot Orchestration Service (BOS). Session templates are managed via the Cray CLI with the cray bos sessiontemplate commands.
Get the Framework for a Session Template When creating a new BOS session template, it can be helpful to start with a framework and then edit it as needed. Use the following command to retrieve the BOS session template framework:</description>
    </item>
    
    <item>
      <title>BOS Limitations For Gigabyte BMC Hardware</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/</guid>
      <description>BOS Limitations for Gigabyte BMC Hardware Special steps need to be taken when using the Boot Orchestration Service (BOS) to boot, reboot, shutdown, or configure Gigabyte hardware. Gigabyte hardware treats power off and power on requests as successful, regardless of if actually successfully completed. The power on/off requests are ignored by Cray Advanced Platform Monitoring and Control (CAPMC) if they are received within a short period of time, which is typically around 60 seconds per operation.</description>
    </item>
    
    <item>
      <title>Kernel Boot Parameters</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/kernel_boot_parameters/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/kernel_boot_parameters/</guid>
      <description>Kernel Boot Parameters The Image Management Service (IMS) extracts kernel boot parameters from the /boot/kernel-parameters file in the image, if that file exists, and stores them in S3. IMS already stores the other boot artifacts (kernel, initrd, and rootfs) in S3. When told to boot an image, the Boot Orchestration Service (BOS) will extract these parameters and deliver them to the Boot Script Service (BSS) so they can be used during the next boot of a node.</description>
    </item>
    
    <item>
      <title>Limit The Scope Of A BOS Session</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/limit_the_scope_of_a_bos_session/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/limit_the_scope_of_a_bos_session/</guid>
      <description>Limit the Scope of a BOS Session The Boot Orchestration Service (BOS) supports an optional &amp;ndash;limit parameter when creating a session. This parameter can be used to further limit the nodes that BOS runs against, and is applied to all boot sets.
The --limit parameter takes a comma-separated list of nodes, groups, or roles in any combination. The BOS session will be limited to run against components that match both the boot set information and one or more of the nodes, groups, or roles listed in the limit.</description>
    </item>
    
    <item>
      <title>Create A Session Template To Boot Compute Nodes With Cps</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/</guid>
      <description>Create a Session Template to Boot Compute Nodes with CPS When compute nodes are booted, the Content Projection Service (CPS) and Data Virtualization Service (DVS) project the root file system (rootfs) over the network to the compute nodes by default.
Another option when compute nodes are booted is to download their rootfs into RAM.
Procedure   Use either the cray bos session create CLI command or the bash script to create a session template.</description>
    </item>
    
    <item>
      <title>Edit The IPXE Embedded Boot Script</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/</guid>
      <description>Edit the iPXE Embedded Boot Script Manually adjust the iPXE embedded boot script to change the order of network interfaces for DHCP request. Changing the order of network interfaces for DHCP requests helps improve boot time performance.
Prerequisites This procedure requires administrative privileges.
Procedure   Edit the ConfigMap using one of the following options.
NOTE: Save a backup of the ConfigMap before making any changes.
The following is an example of creating a backup:</description>
    </item>
    
    <item>
      <title>Healthy Compute Node Boot Process</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/healthy_compute_node_boot_process/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/healthy_compute_node_boot_process/</guid>
      <description>Healthy Compute Node Boot Process In order to investigate node boot-related issues, it is important to understand the flow of a healthy boot process and the associated components. This section outlines the normal flow of components that play a role in booting compute nodes, including DHCP, BSS, and TPTP.
DHCP A healthy DHCP exchange between server and client looks like the following:
   Traffic Description Sender     DHCP Discover A broadcast request from the client requesting an IP address.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Sequence</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_sequence/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_sequence/</guid>
      <description>Compute Node Boot Sequence Provides an overview of the compute node boot process and touches upon the fact that issues can be encountered during this process.
The following is a high-level overview of the boot sequence for compute nodes:
  The compute node is powered on.
  The BIOS issues a DHCP discover request.
  DHCP responds with the following:
 next-server, which is the IP address of the TFTP server.</description>
    </item>
    
    <item>
      <title>Configure The BOS Timeout When Booting Compute Nodes</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/</guid>
      <description>Configure the BOS Timeout When Booting Compute Nodes Manually update the boa-job-template ConfigMap to tune the timeout and sleep intervals for the Boot Orchestration Agent (BOA). Correcting the timeout value is a good troubleshooting option for when BOS sessions hang waiting for nodes to be in a Ready state.
If the BOS timeout occurs when booting compute nodes, the system will be unable to boot via BOS.
Prerequisites A Boot Orchestration Service (BOS) session was run and compute nodes are failing to move to a Ready state.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Duplicate Address Warnings And Declined DHCP Offers In Logs</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/</guid>
      <description>Compute Node Boot Issue Symptom: Duplicate Address Warnings and Declined DHCP Offers in Logs If the DHCP and node logs show duplicate address warnings and indicate declined DHCP offers, it may be because another component owns the IP address that DHCP is trying to assign to a node. If this happens, the node will not accept the IP address and will repeatedly submit a DHCP discover request. As a result, the node and DHCP become entangled in a loop of requesting and rejecting.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Message About Invalid Eeprom Checksum In Node Console Or Log</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/</guid>
      <description>Compute Node Boot Issue Symptom: Message About Invalid EEPROM Checksum in Node Console or Log On rare occasions, the processor hardware may lose the Serial Over Lan (SOL) connections and may need to be reseated to allow the node to successfully boot.
Symptoms This issue can be identified if the following is displayed in the node&amp;rsquo;s console or log:
console.38:2018-09-08 04:54:51 [16.721165] ixgbe 0000:18:00.0: The EEPROM Checksum Is Not Valid console.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Node Is Not Able To Download The Required Artifacts</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/</guid>
      <description>Compute Node Boot Issue Symptom: Node is Not Able to Download the Required Artifacts If either or both of the kernel or the initrd boot artifacts are missing from the artifact repository, Boot Script Service (BSS), or both, the node will not be able to download the required boot artifacts and will fail to boot.
Symptoms The node&amp;rsquo;s console or log will display lines beginning with, &amp;lsquo;&#39;Could not start download&#39;&amp;rsquo;. Refer to the image below for an example of this error message.</description>
    </item>
    
    <item>
      <title>Check The Progress Of BOS Session Operations</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/check_the_progress_of_bos_session_operations/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/check_the_progress_of_bos_session_operations/</guid>
      <description>Check the Progress of BOS Session Operations Describes how to view the logs of BOS operations with Kubernetes.
When a Boot Orchestration Service (BOS) session is created, it will return a job ID. This ID can be used to locate the Boot Orchestration Agent (BOA) Kubernetes job that executes the session. For example:
ncn-m001# cray bos session create --template-uuid SESSIONTEMPLATE_NAME --operation Boot operation = &amp;#34;Boot&amp;#34; templateUuid = &amp;#34;TEMPLATE_UUID&amp;#34; [[links]] href = &amp;#34;foo-c7faa704-3f98-4c91-bdfb-e377a184ab4f&amp;#34; jobId = &amp;#34;boa-a939bd32-9d27-433f-afc2-735e77ec8e58&amp;#34; rel = &amp;#34;session&amp;#34; type = &amp;#34;GET&amp;#34; All BOS Kubernetes pods operate in the services namespace.</description>
    </item>
    
    <item>
      <title>Clean Up After A BOS/boa Job Is Completed Or CANcelled</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/</guid>
      <description>Clean Up After a BOS/BOA Job is Completed or Cancelled When a BOS session is created, there are a number of items created on the system. When a session is cancelled or completed, these items need to be cleaned up to ensure there is not lingering content from the session on the system.
When a session is launched, the items below are created:
 Boot Orchestration Agent (BOA) job: The Kubernetes job that runs and handles the BOS session.</description>
    </item>
    
    <item>
      <title>Clean Up Logs After A Boa Kubernetes Job</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/</guid>
      <description>Clean Up Logs After a BOA Kubernetes Job Delete log entries from previous boot orchestration jobs. The Boot Orchestration Service (BOS) launches a Boot Orchestration Agent (BOA) Kubernetes job. BOA then launches a Configuration Framework Service (CFS) session, resulting in a CFS-BOA Kubernetes job. Thus, there are two separate sets of jobs that can be removed.
Deleting log entries creates more space and helps improve the usability of viewing logs.</description>
    </item>
    
    <item>
      <title>Boot Issue Symptom Node HSN Interface Does Not Appear Or Show Detected Links Detected</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/</guid>
      <description>Boot Issue Symptom: Node HSN Interface Does Not Appear or Show Detected Links Detected A node may fail to boot if the HSN interface is experiencing issues, or if it is not able to detect any links.
Symptom The node&amp;rsquo;s HSN interface does not appear in the output of the ip addr command or the output of the ethtool interface command shows no link detected.
Resolution Reseat the node&amp;rsquo;s PCIe card.</description>
    </item>
    
    <item>
      <title>Boot Orchestration</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/boot_orchestration/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/boot_orchestration/</guid>
      <description>Boot Orchestration The Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. This is accomplished using BOS components, such as boot orchestration session templates and sessions, as well as launching a Boot Orchestration Agent (BOA) that fulfills boot requests.
BOS users create a BOS session template via the REST API. A session template is a collection of metadata for a group of nodes and their desired boot artifacts and configuration.</description>
    </item>
    
    <item>
      <title>Boot UANs</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/boot_uans/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/boot_uans/</guid>
      <description>Boot UANs Boot UANs with an image so that they are ready for user logins.
Prerequisites UAN boot images and a BOS session template have been created. See Create UAN Boot Images.
Procedure   Create a BOS session to boot the UAN nodes.
ncn-m001# cray bos session create --template-uuid uan-sessiontemplate-PRODUCT_VERSION \ --operation reboot --format json | tee session.json { &amp;#34;links&amp;#34;: [ { &amp;#34;href&amp;#34;: &amp;#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d&amp;#34;, &amp;#34;jobId&amp;#34;: &amp;#34;boa-89680d0a-3a6b-4569-a1a1-e275b71fce7d&amp;#34;, &amp;#34;rel&amp;#34;: &amp;#34;session&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;GET&amp;#34; }, { &amp;#34;href&amp;#34;: &amp;#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d/status&amp;#34;, &amp;#34;rel&amp;#34;: &amp;#34;status&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;GET&amp;#34; } ], &amp;#34;operation&amp;#34;: &amp;#34;reboot&amp;#34;, &amp;#34;templateUuid&amp;#34;: &amp;#34;uan-sessiontemplate-PRODUCT_VERSION&amp;#34; } The first attempt to reboot the UANs will most likely fail.</description>
    </item>
    
    <item>
      <title>BOS Workflows</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/bos_workflows/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/bos_workflows/</guid>
      <description>BOS Workflows The following workflows present a high-level overview of common Boot Orchestration Service (BOS) operations. These workflows depict how services interact with each other when booting, configuring, or shutting down nodes. They also help provide a quicker and deeper understanding of how the system functions.
The following workflows are included in this section:
 Boot and Configure Nodes Reconfigure Nodes Power Off Nodes  Boot and Configure Nodes Use Case: Administrator powers on and configures select compute nodes.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Node Console Or Logs Indicate That The Server Response Has Timed Out</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/</guid>
      <description>Compute Node Boot Issue Symptom: Node Console or Logs Indicate that the Server Response has Timed Out If the TFTP request is able to access the TFTP service pod but is unable to find its way back to the node, it may be because the kernel is not tracking established TFTP connections.
Symptoms The following image, which is tcpdump data from within the TFTP pod, shows what happens when the TFTP request cannot find a route back to the node that sent the request.</description>
    </item>
    
    <item>
      <title>Use S3 Libraries And Clients</title>
      <link>/docs-csm/en-12/operations/artifact_management/use_s3_libraries_and_clients/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/artifact_management/use_s3_libraries_and_clients/</guid>
      <description>Use S3 Libraries and Clients Several command line clients and language-specific libraries are available in addition to the Simple Storage Service (S3) RESTful API. Developers and system administrators can interact with artifacts in the S3 object store with these tools.
To learn more, refer to the following links:
 https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html - S3 Python client https://docs.aws.amazon.com/sdk-for-go/api/service/s3/ - Go client https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html - Amazon Web Services (AWS) S3 CLI  </description>
    </item>
    
    <item>
      <title>Artifact Management</title>
      <link>/docs-csm/en-12/operations/artifact_management/artifact_management/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/artifact_management/artifact_management/</guid>
      <description>Artifact Management The Ceph Object Gateway Simple Storage Service (S3) API is used for artifact management. The RESTful API that Ceph provides via the gateway is compatible with the basic data access model of the Amazon S3 API. See the https://docs.ceph.com/docs/mimic/radosgw/s3/ for more information about compatibility. The object gateway is also referred to as the RADOS gateway or simply RGW.
S3 is an object storage service that provides high-level performance, scalability, security, and data availability.</description>
    </item>
    
    <item>
      <title>Generate Temporary S3 Credentials</title>
      <link>/docs-csm/en-12/operations/artifact_management/generate_temporary_s3_credentials/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/artifact_management/generate_temporary_s3_credentials/</guid>
      <description>Generate Temporary S3 Credentials Cray provides a simple token service (STS) via the API gateway for administrators to generate temporary Simple Storage Service (S3) credentials for use with S3 buckets. Temporary S3 credentials are generated using either cURL or Python.
The generated S3 credentials will expire after one hour.
Procedure   Retrieve temporary S3 credentials with cURL.
  Obtain a JWT token.
See Retrieve an Authentication Token for more information.</description>
    </item>
    
    <item>
      <title>Manage Artifacts With The Cray Cli</title>
      <link>/docs-csm/en-12/operations/artifact_management/manage_artifacts_with_the_cray_cli/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/artifact_management/manage_artifacts_with_the_cray_cli/</guid>
      <description>Manage Artifacts with the Cray CLI The artifacts (objects) available for use on the system are created and managed with the Cray CLI. The cray artifacts command provides the ability to manage any given artifact. The Cray CLI automatically authenticates users and provides Simple Storage Service (S3) credentials.
All operations with the cray artifacts command assume that the user has already been authenticated. If the user has not been authenticated with the Cray CLI, run the following command and enter the appropriate credentials:</description>
    </item>
    
    <item>
      <title>Update A UAS Volume</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/update_a_uas_volume/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/update_a_uas_volume/</guid>
      <description>Update a UAS Volume Modify the configuration of an already-registered UAS volume. Almost any part of the configuration of a UAS volume can be modified.
Prerequisites  Install and initialize the cray administrative CLI. Obtain the UAS volume ID of a volume. Perform List Volumes Registered in UAS if needed. Read Add a Volume to UAS. The options and caveats for updating volumes are the same as for creating volumes.  Procedure   Modify the configuration of a UAS volume.</description>
    </item>
    
    <item>
      <title>User Access Service (UAS)</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/user_access_service_uas/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/user_access_service_uas/</guid>
      <description>User Access Service (UAS) The User Access Service (UAS) is a containerized service managed by Kubernetes that enables application developers to create and run user applications. UAS runs on a non-compute node (NCN) that is acting as a Kubernetes worker node.
Users launch a User Access Instance (UAI) using the cray command. Users can also transfer data between the Cray system and external systems using the UAI.
When a user requests a new UAI, the UAS service returns status and connection information to the newly created UAI.</description>
    </item>
    
    <item>
      <title>View A UAI Class</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/view_a_uai_class/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/view_a_uai_class/</guid>
      <description>View a UAI Class Display all the information for a specific UAI class by referencing its class ID.
Prerequisites  Install and initialize the cray administrative CLI. Obtain the ID of a UAI class.  Procedure   View all the information about a specific UAI class.
To examine an existing UAI class, use a command of the following form:
ncn-m001-pit# cray uas admin config classes describe &amp;lt;class-id&amp;gt; The following example uses the --format yaml option to display the UAI class configuration in YAML format.</description>
    </item>
    
    <item>
      <title>Volumes</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/volumes/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/volumes/</guid>
      <description>Volumes Volumes provide a way to connect UAIs to external data, whether they be Kubernetes managed objects, external file systems or files, host node files and directories, or remote networked data to be used within the UAI.
The following are examples of how volumes are commonly used by UAIs:
 To connect UAIs to configuration files like /etc/localtime maintained by the host node To connect end-user UAIs to Slurm or PBS Professional Workload Manager configuration shared through Kubernetes To connect end-user UAIs to Programming Environment libraries and tools hosted on the UAI host nodes To connect end-user UAIs to Lustre or other external storage for user data To connect broker UAIs to a directory service (see Configure a Broker UAI Class) or SSH configuration (see Customize the Broker UAI Image) needed to authenticate and redirect user sessions  Any kind of volume recognized by the Kubernetes installation can be installed as a volume within UAS and will be used when creating UAIs.</description>
    </item>
    
    <item>
      <title>UAS And UAI Health Checks</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/uas_and_uai_health_checks/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/uas_and_uai_health_checks/</guid>
      <description>UAS and UAI Health Checks Initialize and authorize the CLI so a user may run procedures on any given node.
Initialize and Authorize the CLI The procedures below use the CLI as an authorized user and run on two separate node types. The first part runs on the LiveCD node while the second part runs on a non-LiveCD Kubernetes master or worker node. When using the CLI on either node, the CLI configuration must be initialized and the user running the procedure must be authorized.</description>
    </item>
    
    <item>
      <title>Update A Resource Specification</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/update_a_resource_specification/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/update_a_resource_specification/</guid>
      <description>Update a Resource Specification Modify a specific UAI resource specification using the resource_id of that specification.
Prerequisites  Install and initialize the cray administrative CLI. Verify that the resource specification to be updated exists within UAS. Perform eitherList UAI Resource Specifications or Retrieve Resource Specification Details.  Procedure To modify a particular resource specification, use a command of the following form:
ncn-m001-pit# cray uas admin config resources update [OPTIONS] RESOURCE_ID The [OPTIONS] used by this command are the same options used to create resource specifications.</description>
    </item>
    
    <item>
      <title>Update A UAI Image Registration</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/update_a_uai_image_registration/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/update_a_uai_image_registration/</guid>
      <description>Update a UAI Image Registration Modify the UAS registration information of a UAI image.
Prerequisites Verify that the image to be updated is registered with UAS. Refer to Retrieve UAI Image Registration Information.
Procedure Once an allowable UAI image has been created, it may be necessary to change its attributes. For example, the default image may need to change.
  Modify the registration information of a UAI image by using a command of the form:</description>
    </item>
    
    <item>
      <title>UAI Macvlans Network Attachments</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_macvlans_network_attachments/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_macvlans_network_attachments/</guid>
      <description>UAI macvlans Network Attachments UAIs need to be able to reach compute nodes across the node management network (NMN). When the compute node NMN is structured as multiple subnets, this requires routing form the UAIs to those subnets. The default route in a UAI goes to the public network through the Customer Access Network (CAN) so that will not work for reaching compute nodes. To solve this problem, UAS installs Kubernetes network attachments within the Kubernetes user namespace, one of which is used by UAIs.</description>
    </item>
    
    <item>
      <title>UAI Network Attachments</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_network_attachments/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_network_attachments/</guid>
      <description>UAI Network Attachments The UAI network attachment configuration flows from the CRAY Site Initializer (CSI) localization data through customizations.yaml into the UAS Helm chart and, ultimately, into Kubernetes in the form of a &amp;ldquo;network-attachment-definition&amp;rdquo;.
This section describes the data at each of those stages to show how the final network attachment gets created.
CSI Localization Data The details of CSI localization are beyond the scope of this guide, but here are the important settings, and the values used in the following examples:</description>
    </item>
    
    <item>
      <title>UAS Limitations</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/uas_limitations/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/uas_limitations/</guid>
      <description>UAS Limitations Functionality that is currently not supported while using UAS.
Functionality Not Currently Supported by the User Access Service  Lustre (lfs) commands within the UAS service pod Executing Singularity containers within the UAS service Building Docker containers within the UAS environment Building containerd containers within the UAS environment dmesg cannot run inside a UAI because of container security limitations Users cannot ssh from ncn-w001 to a UAI. This is because UAIs use LoadBalancer IP addresses on the Customer Access Network (CAN) instead of NodePorts and the LoadBalancer IP addresses are not accessible from ncn-w001.</description>
    </item>
    
    <item>
      <title>UAI Host Node Selection</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_host_node_selection/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_host_node_selection/</guid>
      <description>UAI Host Node Selection When selecting UAI host nodes, it is a good idea to take into account the amount of combined load users and system services will bring to those nodes. UAIs run by default at a lower priority than system services on worker nodes which means that, if the combined load exceeds the capacity of the nodes, Kubernetes will eject UAIs and/or refuse to schedule them to protect system services.</description>
    </item>
    
    <item>
      <title>UAI Host Nodes</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_host_nodes/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_host_nodes/</guid>
      <description>UAI Host Nodes UAIs run on Kubernetes worker nodes. There is a mechanism using Kubernetes labels to prevent UAIs from running on a specific worker node, however. Any Kubernetes node that is not labeled to prevent UAIs from running on it is considered to be a UAI host node. The administrator of a given site may control the set of UAI host nodes by labeling Kubernetes worker nodes appropriately.</description>
    </item>
    
    <item>
      <title>UAI Images</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_images/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_images/</guid>
      <description>UAI Images There are three kinds of UAI images used by UAS:
 A pre-packaged broker UAI image provided with the UAS A pre-packaged basic end-user UAI Image provided with the UAS Custom end-user UAI images created on site, usually based on compute node contents  UAS provides two stock UAI images when installed. The first is a standard end-user UAI Image that has the necessary software installed in it to support a basic Linux distribution login experience.</description>
    </item>
    
    <item>
      <title>UAI Management</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_management/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_management/</guid>
      <description>UAI Management UAS supports two manual methods and one automated method of UAI management:
 Direct administrative UAI management Legacy mode user driven UAI management UAI broker mode UAI management  Direct administrative UAI management is available mostly to allow administrators to set up UAI brokers for the UAI broker mode of UAI management and to control UAIs that are created under one of the other two methods. It is unlikely that a site will choose to create end-user UAIs this way, but it is possible to do.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAS By Viewing Log Output</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uas_by_viewing_log_output/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uas_by_viewing_log_output/</guid>
      <description>Troubleshoot UAS by Viewing Log Output At times there will be problems with UAS. Usually this takes the form of errors showing up on CLI commands that are not immediately interpretable as some sort of input error. It is sometimes useful to examine the UAS service logs to find out what is wrong.
The first thing to do is to find out the names of the Kubernetes pods running UAS:</description>
    </item>
    
    <item>
      <title>UAI Classes</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_classes/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/uai_classes/</guid>
      <description>UAI Classes This topic explains all the fields in a User Access Instance (UAI) class and gives guidance on setting them when creating UAI classes.
Example Listing and Overview The following is JSON-formatted example output from the cray uas admin config classes list command (see List Available UAI Classes). This output contains examples of three UAI classes:
 A UAI broker class A brokered end-user UAI class A non-brokered end-user UAI class  This topic uses the end-user UAI class section to explain each of fields within a UAI class.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAIs By Viewing Log Output</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uais_by_viewing_log_output/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uais_by_viewing_log_output/</guid>
      <description>Troubleshoot UAIs by Viewing Log Output Sometimes a UAI will come up and run but will not work correctly. It is possible to see errors reported by elements of the UAI entrypoint script using the kubectl logs command. First find the UAI of interest. This starts by identifying the UAI name using the CLI:
ncn-m001-pit# cray uas admin uais list [[results]] uai_age = &amp;quot;4h30m&amp;quot; uai_connect_string = &amp;quot;ssh broker@10.103.13.162&amp;quot; uai_host = &amp;quot;ncn-w001&amp;quot; uai_img = &amp;quot;dtr.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAIs With Administrative Access</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uais_with_administrative_access/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uais_with_administrative_access/</guid>
      <description>Troubleshoot UAIs with Administrative Access Sometimes there is no better way to figure out a problem with a UAI than to get inside it and look around as an administrator. This is done using kubectl exec to start a shell inside the running container as &amp;ldquo;root&amp;rdquo; (in the container). With this an administrator can diagnose problems, make changes to the running UAI and find solutions. It is important to remember that any change made inside a UAI is transitory.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAS Issues</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uas_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uas_issues/</guid>
      <description>Troubleshoot UAS Issues This section provides examples of some commands that can be used to troubleshoot UAS-related issues.
Troubleshoot Connection Issues packet_write_wait: Connection to 203.0.113.0 port 30841: Broken pipe If an error message related to broken pipes returns, enable keep-alives on the client side. The admin should update the /etc/ssh/sshd_config and /etc/ssh/ssh_config files to add the following:
TCPKeepAlive yes ServerAliveInterval 120 ServerAliveCountMax 720 Invalid Credentials ncn-w001 # cray auth login --username USER --password WRONGPASSWORD Usage: cray auth login [OPTIONS] Try &amp;#34;cray auth login --help&amp;#34; for help.</description>
    </item>
    
    <item>
      <title>Troubleshoot Missing Or Incorrect UAI Images</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_missing_or_incorrect_uai_images/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_missing_or_incorrect_uai_images/</guid>
      <description>Troubleshoot Missing or Incorrect UAI Images If a UAI shows a uai_status of Waiting and a uai_msg of ImagePullBackOff, that indicates that the UAI or the UAI class is configured to use an image that is not in the image registry.
Either obtaining and pushing the image to the image registry, or correcting the name or version of the image in the UAS configuration will usually resolve this.</description>
    </item>
    
    <item>
      <title>Troubleshoot Stale Brokered UAIs</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_stale_brokered_uais/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_stale_brokered_uais/</guid>
      <description>Troubleshoot Stale Brokered UAIs When a broker UAI terminates and restarts, the SSH key used to forward SSH sessions to end-user UAIs changes (this is a known problem) and subsequent broker UAIs are unable to forward sessions to end-user UAIs. The symptom of this is that a user logging into a broker UAI will receive a password prompt from the end-user UAI and be unable to log in even if providing the correct password.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAI Authentication Issues</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uai_authentication_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uai_authentication_issues/</guid>
      <description>Troubleshoot UAI Authentication Issues Several troubleshooting steps related to authentication in a UAI.
Internal Server Error An error was encountered while accessing Keycloak because of an invalid token.
# cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try &amp;#34;cray uas create --help&amp;#34; for help. Error: Internal Server Error: An error was encountered while accessing Keycloak The uas-mgr logs show:
2020-03-06 18:52:07,642 - uas_auth - ERROR - &amp;lt;class &amp;#39;requests.exceptions.HTTPError&amp;#39;&amp;gt; HTTPError(&amp;#39;401 Client Error: Unauthorized for url: https://api-gw-service-nmn.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAI Stuck In &#34;containercreating&#34;</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uai_stuck_in_containercreating/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_uai_stuck_in_containercreating/</guid>
      <description>Troubleshoot UAI Stuck in &amp;ldquo;ContainerCreating&amp;rdquo; Resolve an issue causing UAIs to show a uai_status field of Waiting, and a uai_msg field of ContainerCreating. It is possible that this is just a matter of starting the UAI taking longer than normal, perhaps as it pulls in a new UAI image from a registry. If the issue persists for a long time, it is worth investigating.
Prerequisites The UAI has been in the ContainerCreating status for several minutes.</description>
    </item>
    
    <item>
      <title>Start A Broker UAI</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/start_a_broker_uai/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/start_a_broker_uai/</guid>
      <description>Start a Broker UAI Create a broker UAI after a broker UAI class has been created.
Prerequisites A broker UAI class has been set up. See Configure a Broker UAI Class.
Procedure Use the following command to create a broker UAI:
ncn-m001-pit# cray uas admin uais create --class-id &amp;lt;class-id&amp;gt; [--owner &amp;lt;name&amp;gt;] To make the broker obvious in the list of UAIs, giving it an owner name of broker is handy. The owner name on a broker is used for naming and listing, but nothing else, so this is a convenient convention.</description>
    </item>
    
    <item>
      <title>Troubleshoot Common Mistakes When Creating A Custom End-user UAI Image</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_common_mistakes_when_creating_a_custom_end-user_uai_image/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_common_mistakes_when_creating_a_custom_end-user_uai_image/</guid>
      <description>Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image There a several problems that may occur while making or working with a custom end-user UAI images. The following are some basic troubleshooting questions to ask:
 Does SESSION_NAME match an actual entry in cray bos sessiontemplate list? Is the SESSION_ID set to an appropriate uuid format? Did the awk command not parse the uuid correctly? Did the file /etc/security/limits.d/99-slingshot-network.conf get removed from the tarball correctly?</description>
    </item>
    
    <item>
      <title>Troubleshoot Duplicate Mount Paths In A UAI</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_duplicate_mount_paths_in_a_uai/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/troubleshoot_duplicate_mount_paths_in_a_uai/</guid>
      <description>Troubleshoot Duplicate Mount Paths in a UAI If a user attempts to create a UAI in the legacy mode and cannot create the UAI at all, a good place to look is at volumes. Duplicate mount_path specifications in the list of volumes in a UAI will cause a failure that looks like this:
ncn-m001-pit# cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try &#39;cray uas create --help&#39; for help.</description>
    </item>
    
    <item>
      <title>Retrieve UAI Image Registration Information</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/retrieve_uai_image_registration_information/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:09 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/retrieve_uai_image_registration_information/</guid>
      <description>Retrieve UAI Image Registration Information Use this procedure to obtain the default and imagename values for a UAI image that has been registered with UAS. This procedure can also be used to confirm that a specific image ID is still registered with UAS.
This procedure returns the same information as List Registered UAI Images, but only for one image.
Prerequisites Obtain a valid UAS image ID.
Procedure   Obtain the image ID for a UAI that has been registered with UAS.</description>
    </item>
    
    <item>
      <title>Select And Configure Host Nodes For UAIs</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/select_and_configure_host_nodes_for_uais/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:09 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/select_and_configure_host_nodes_for_uais/</guid>
      <description>Select and Configure Host Nodes for UAIs Site administrators can control the set of UAI host nodes by labeling Kubernetes worker nodes appropriately.
UAIs run on NCNs that function as Kubernetes worker nodes. Use Kubernetes labels to prevent UAIs from running on one or more specific worker nodes. Any Kubernetes node that is not labeled to prevent UAIs from running on it is considered to be a UAI host node. In other words, UAI host node selection is an exclusive activity, not an inclusive one.</description>
    </item>
    
    <item>
      <title>Special Purpose UAIs</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/special_purpose_uais/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:09 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/special_purpose_uais/</guid>
      <description>Special Purpose UAIs Even though most UAIs are end-user UAIs, UAI classes make it possible to construct UAIs to serve special purposes that are not strictly end-user oriented.
One kind of special purpose UAI is the broker UAI, which provides on demand end-user UAI launch and management (see Broker Mode UAI Management). While no other specialty UAI types currently exist, other applications are expected to arise and users are encouraged to innovate as needed.</description>
    </item>
    
    <item>
      <title>Register A UAI Image</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/register_a_uai_image/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:08 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/register_a_uai_image/</guid>
      <description>Register a UAI Image Register a UAI image with UAS. Registration tells UAS where to locate the image and whether to use the image as the default for UAIs.
Prerequisites  Initialize cray administrative CLI. Create a UAI image and upload it to the container registry. See Customize End-User UAI Images.  Procedure   Register a UAI image with UAS.
The following is the minimum required CLI command form:</description>
    </item>
    
    <item>
      <title>Reset The UAS Configuration To Original Installed Settings</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/reset_the_uas_configuration_to_original_installed_settings/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:08 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/reset_the_uas_configuration_to_original_installed_settings/</guid>
      <description>Reset the UAS Configuration to Original Installed Settings How to remove a customized UAS configuration and restore the base installed configuration.
The configuration set up using the Cray CLI to interact with UAS persists as long as UAS remains installed and survives upgrades. This is called the running configuration and it is both persistent and malleable. During installation and localization, however, the installer creates a base installed configuration. It may be necessary to return to this base configuration.</description>
    </item>
    
    <item>
      <title>Resource Specifications</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/resource_specifications/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:08 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/resource_specifications/</guid>
      <description>Resource Specifications Kubernetes uses resource limits and resource requests, to manage the system resources available to pods. Because UAIs run as pods under Kubernetes, UAS takes advantage of Kubernetes to manage the system resources available to UAIs.
In the UAS configuration, resource specifications contain that configuration. A UAI that is assigned a resource specification will use that instead of the default resource limits or requests on the Kubernetes namespace containing the UAI.</description>
    </item>
    
    <item>
      <title>Retrieve Resource Specification Details</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/retrieve_resource_specification_details/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:08 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/retrieve_resource_specification_details/</guid>
      <description>Retrieve Resource Specification Details Display a specific resource specification using the resource_id of that specification.
Prerequisites Install and initialize the cray administrative CLI.
Procedure   Print out a resource specification.
To examine a particular resource specification, use a command of the following form:
ncn-m001-pit# cray uas admin config resources describe RESOURCE_ID For example:
ncn-m001-pit# cray uas admin config resources describe 85645ff3-1ce0-4f49-9c23-05b8a2d31849 comment = &amp;#34;my first example resource specification&amp;#34; limit = &amp;#34;{&amp;#34;cpu&amp;#34;: &amp;#34;300m&amp;#34;, &amp;#34;memory&amp;#34;: &amp;#34;250Mi&amp;#34;}&amp;#34; request = &amp;#34;{&amp;#34;cpu&amp;#34;: &amp;#34;300m&amp;#34;, &amp;#34;memory&amp;#34;: &amp;#34;250Mi&amp;#34;}&amp;#34; resource_id = &amp;#34;85645ff3-1ce0-4f49-9c23-05b8a2d31849&amp;#34;   </description>
    </item>
    
    <item>
      <title>Log In To A User&#39;s UAI To Troubleshoot Issues</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/login_to_a_users_uai_to_troubleshoot_issues/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:07 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/login_to_a_users_uai_to_troubleshoot_issues/</guid>
      <description>Log in to a User&amp;rsquo;s UAI to Troubleshoot Issues Log in to a user&amp;rsquo;s User Access Instance (UAI) to help the user troubleshoot issues.
Prerequisites This procedure requires root access.
Limitations This procedure does not work if the pod is in either &amp;ldquo;Error&amp;rdquo; or &amp;ldquo;Terminating&amp;rdquo; states.
Procedure   Log in to the first NCN acting as a Kubernetes master node (ncn-m001) as root.
  Find and record the name of the UAI.</description>
    </item>
    
    <item>
      <title>Modify A UAI Class</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/modify_a_uai_class/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:07 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/modify_a_uai_class/</guid>
      <description>Modify a UAI Class Update a UAI class with a modified configuration.
Prerequisites  Install and initialize the cray administrative CLI. Obtain the ID of the UAI class that will be modified.  Limitations The ID of the UAI class cannot be modified.
Procedure To update an existing UAI class, use a command of the following form:
cray uas admin config classes update OPTIONS UAI_CLASS_ID OPTIONS are the same options supported for UAI class creation (see Create a UAI Class) and UAI_CLASS_ID is the ID of the UAI class.</description>
    </item>
    
    <item>
      <title>Obtain The Configuration Of A UAS Volume</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/obtain_configuration_of_a_uas_volume/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:07 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/obtain_configuration_of_a_uas_volume/</guid>
      <description>Obtain the Configuration of a UAS Volume View the configuration information of a specific UAS volume. This procedure requires the volume_ID of that volume.
Prerequisites  Install and initialize the cray administrative CLI. Obtain the UAS volume ID of a volume. Perform List Volumes Registered in UAS if needed.  Procedure   View the configuration of a specific UAS volume.
This command returns output in TOML format by default. JSON or YAML formatted output can be obtained by using the --format json or --format yaml options respectively.</description>
    </item>
    
    <item>
      <title>List And Delete All UAIs</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_and_delete_all_uais/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:06 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_and_delete_all_uais/</guid>
      <description>List and Delete All UAIs Delete all UAIs currently on the system.
Prerequisites At least one UAI is running.
Procedure   Log in to an NCN as root.
  List all the UAIs on the system.
ncn-m001# cray uas uais list [[results]] username = &amp;#34;uastest&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_status = &amp;#34;Running: Ready&amp;#34; uai_connect_string = &amp;#34;ssh uastest@203.0.113.0 -p 32486 -i ~/.ssh/id_rsa&amp;#34; uai_img = &amp;#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest&amp;#34; uai_age = &amp;#34;2m&amp;#34; uai_name = &amp;#34;uai-uastest-f488eef6&amp;#34; [[results]] username = &amp;#34;uastest&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_status = &amp;#34;Running: Ready&amp;#34; uai_connect_string = &amp;#34;ssh uastest@203.</description>
    </item>
    
    <item>
      <title>List UAS Information</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_uas_information/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:06 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_uas_information/</guid>
      <description>List UAS Information Use the cray uas command to gather information about the User Access Service&amp;rsquo;s version, images, and running User Access Instances (UAIs).
List UAS Version with cray uas mgr-info list ncn-w001# cray uas mgr-info list service_name = &amp;quot;cray-uas-mgr&amp;quot;, version = &amp;quot;0.11.3&amp;quot; List Available UAS Images with cray uas images list ncn-w001# cray uas images list default_image = &amp;#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest&amp;#34; image_list = [ &amp;#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest&amp;#34;, &amp;#34;registry.local/cray/cray-uas-sles15sp1:latest&amp;#34;,] List All Running UAIs with cray uas uais list ncn-w001# cray uas uais list [[results]] username = &amp;#34;user&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_status = &amp;#34;Running: Ready&amp;#34; uai_connect_string = &amp;#34;ssh user@203.</description>
    </item>
    
    <item>
      <title>List Volumes Registered In UAS</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_volumes_registered_in_uas/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:06 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_volumes_registered_in_uas/</guid>
      <description>List Volumes Registered in UAS List the details of all volumes registered in UAS with the cray uas admin config volumes list command. Use this command to obtain the volume_id value of volume, which is required for other UAS administrative commands.
Prerequisites Install and initialize the cray administrative CLI.
Procedure   List the details of all the volumes registered in UAS.
  Print out the list in TOML.</description>
    </item>
    
    <item>
      <title>Log In To A Broker UAI</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/log_in_to_a_broker_uai/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:06 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/log_in_to_a_broker_uai/</guid>
      <description>Log in to a Broker UAI SSH to log into a broker UAI and reach the end-user UAIs on demand.
Prerequisites The broker UAI is running. See Start a Broker UAI.
Procedure   Log in to the broker UAI.
The following example is the first login for the vers user:
vers&amp;gt; ssh vers@10.103.13.162 The authenticity of host &#39;10.103.13.162 (10.103.13.162)&#39; can&#39;t be established. ECDSA key fingerprint is SHA256:k4ef6vTtJ1Dtb6H17cAFh5ljZYTl4IXtezR3fPVUKZI. Are you sure you want to continue connecting (yes/no/[fingerprint])?</description>
    </item>
    
    <item>
      <title>List Registered UAI Images</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_registered_uai_images/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:05 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_registered_uai_images/</guid>
      <description>List Registered UAI Images Administrators can use the cray uas admin config images list command to see the list of registered images. This command also displays the UAS registration information about each image.
Registering a UAI image name is insufficient to make that image available for UAIs. UAI images must also be registered, but also created and stored in the container registry need to link to procedure on how to do that.</description>
    </item>
    
    <item>
      <title>List UAI Resource Specifications</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_uai_resource_specifications/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:05 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_uai_resource_specifications/</guid>
      <description>List UAI Resource Specifications Obtain a list of all the UAI resource specifications registered with UAS.
Prerequisites The cray administrative CLI must be installed and initialized.
Procedure   List all the resource specifications registered in UAS.
The resource specifications returned by the following command are available for UAIs to use:
ncn-m001-pit# cray uas admin config resources list [[results]] comment = &amp;quot;my first example resource specification&amp;quot; limit = &amp;quot;{\&amp;quot;cpu\&amp;quot;: \&amp;quot;300m\&amp;quot;, \&amp;quot;memory\&amp;quot;: \&amp;quot;250Mi\&amp;quot;}&amp;quot; request = &amp;quot;{\&amp;quot;cpu\&amp;quot;: \&amp;quot;300m\&amp;quot;, \&amp;quot;memory\&amp;quot;: \&amp;quot;250Mi\&amp;quot;}&amp;quot; resource_id = &amp;quot;85645ff3-1ce0-4f49-9c23-05b8a2d31849&amp;quot; [[results]] comment = &amp;quot;my second example resource specification&amp;quot; limit = &amp;quot;{\&amp;quot;cpu\&amp;quot;: \&amp;quot;4\&amp;quot;, \&amp;quot;memory\&amp;quot;: \&amp;quot;1Gi\&amp;quot;}&amp;quot; request = &amp;quot;{\&amp;quot;cpu\&amp;quot;: \&amp;quot;4\&amp;quot;, \&amp;quot;memory\&amp;quot;: \&amp;quot;1Gi\&amp;quot;}&amp;quot; resource_id = &amp;quot;eff9e1f2-3560-4ece-a9ce-8093e05e032d&amp;quot; The following are the configurable parts of a resource specification:</description>
    </item>
    
    <item>
      <title>List UAIs</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_uais/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:05 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_uais/</guid>
      <description>List UAIs View the details of every UAI that is running by using a direct UAS administrative command.
Prerequisites  This procedure requires administrative privileges. Install and initialize the cray administrative CLI.  Procedure   List the existing UAIs.
Use a command of the following form:
ncn-m001-pit# cray uas admin uais list [options] The [options] parameter includes the following selection options:
 --owner &#39;&amp;lt;user-name&amp;gt;&#39; show only UAIs owned by the named user --class-id &#39;&amp;lt;class-id&#39; show only UAIs of the specified UAI class  For example:</description>
    </item>
    
    <item>
      <title>List Available UAI Classes</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_available_uai_classes/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:04 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_available_uai_classes/</guid>
      <description>List Available UAI Classes View all the details of every available UAI class. Use this information to select a class to apply to one or more UAIs.
Prerequisites Install and initialize the cray administrative CLI.
Procedure   List all available UAI classes.
To list available UAI classes, use the following command:
ncn-m001-pit# cray uas admin config classes list The cray uas admin config classes list command supports the same --format options as the cray uas admin config volumes list command.</description>
    </item>
    
    <item>
      <title>List Available UAI Images In Legacy Mode</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_available_uai_images_in_legacy_mode/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:04 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/list_available_uai_images_in_legacy_mode/</guid>
      <description>List Available UAI Images in Legacy Mode A user can list the UAI images available for creating a UAI with a command of the form:
user&amp;gt; cray uas images list For example:
vers&amp;gt; cray uas images list default_image = &amp;quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest&amp;quot; image_list = [ &amp;quot;dtr.dev.cray.com/cray/cray-uai-broker:latest&amp;quot;, &amp;quot;dtr.dev.cray.com/cray/cray-uas-sles15:latest&amp;quot;, &amp;quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest&amp;quot;,] </description>
    </item>
    
    <item>
      <title>Elements Of A UAI</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/elements_of_a_uai/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/elements_of_a_uai/</guid>
      <description>Elements of a UAI All UAIs can have the following attributes associated with them:
 A required container image An optional set of volumes An optional resource specification An optional collection of other configuration items  This topic explains each of these attributes.
UAI container image The container image for a UAI (UAI image) defines and provides the basic environment available to the user. This environment includes, among other things:</description>
    </item>
    
    <item>
      <title>End-user UAIs</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/end_user_uais/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/end_user_uais/</guid>
      <description>End-User UAIs UAIs used for interactive logins are called end-user UAIs. End-user UAIs can be seen as lightweight User Access Nodes (UANs), but there are important differences between UAIs and UANs. First, end-user UAIs are not dedicated hardware like UANs. They are implemented as containers orchestrated by Kubernetes, which makes them subject to Kubernetes scheduling and resource management rules. One key element of Kubernetes orchestration is impermanence. While end-user UAIs are often long running, Kubernetes can reschedule or recreate them as needed to meet resource and node availability constraints.</description>
    </item>
    
    <item>
      <title>Examine A UAI Using A Direct Administrative Command</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/examine_a_uai_using_a_direct_administrative_command/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/examine_a_uai_using_a_direct_administrative_command/</guid>
      <description>Examine a UAI Using a Direct Administrative Command Print out information about a UAI.
Prerequisites Install and initialize the cray administrative CLI.
Procedure   Print out information about a UAI.
To examine an existing UAI use a command of the following form:
cray uas admin uais describe &amp;lt;uai-name&amp;gt; For example:
ncn-m001-pit# cray uas admin uais describe uai-vers-715fa89d uai_age = &amp;#34;2d23h&amp;#34; uai_connect_string = &amp;#34;ssh vers@10.28.212.166&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_img = &amp;#34;registry.</description>
    </item>
    
    <item>
      <title>Legacy Mode User-driven UAI Management</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/legacy_mode_user-driven_uai_management/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/legacy_mode_user-driven_uai_management/</guid>
      <description>Legacy Mode User-Driven UAI Management In the legacy mode, users create and manage their own UAIs through the Cray CLI. A user may create, list and delete only UAIs owned by the user. The user may not create a UAI for another user, nor may the user see or delete UAIs owned by another user. Once created, the information describing the UAI gives the user the information needed to reach the UAI using SSH and log into it.</description>
    </item>
    
    <item>
      <title>Delete A UAI Resource Specification</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_resource_specification/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:02 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_resource_specification/</guid>
      <description>Delete a UAI Resource Specification Delete a specific UAI resource specification using the resource_id of that specification. Once deleted, UAIs will no longer be able to use that specification.
Prerequisites Install and initialize the cray administrative CLI.
Prerequisites To delete a particular resource specification, use a command of the following form:
ncn-m001-pit# cray uas admin config resources delete RESOURCE_ID   Remove a UAI resource specification from UAS.
ncn-m001-pit# cray uas admin config resources delete 7c78f5cf-ccf3-4d69-ae0b-a75648e5cddb   </description>
    </item>
    
    <item>
      <title>Delete A UAI Using An Administrative Command</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_using_an_administrative_command/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:02 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_using_an_administrative_command/</guid>
      <description>Delete a UAI Using an Administrative Command Manually delete one or more UAIs.
Prerequisites Install and initialize the cray administrative CLI.
Procedure   Delete one or more UAIs using a command of the following form:
ncn-m001-pit# cray uas admin uais delete OPTIONS OPTIONS is one or more of the following:
 --owner USERNAME: delete all UAIs owned by the named user --class-id CLASS_ID: delete all UAIs of the specified UAI class --uai-list LIST_OF_UAI_NAMES: delete all the listed UAIs  The following example deletes two UAIs by name:</description>
    </item>
    
    <item>
      <title>Delete A Volume Configuration</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_volume_configuration/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:02 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_volume_configuration/</guid>
      <description>Delete a Volume Configuration Delete an existing volume configuration. This procedure does not delete the underlying object referred to by the UAS volume configuration.
Prerequisites  Install and initialize the cray administrative CLI. Obtain the volume_id of the UAS volume to delete. Perform List Volumes Registered in UAS if necessary.  Procedure   Delete the target volume configuration.
To delete a UAS Volume, use a command of the following form:</description>
    </item>
    
    <item>
      <title>Customize The Broker UAI Image</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/customize_the_broker_uai_image/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/customize_the_broker_uai_image/</guid>
      <description>Customize the Broker UAI Image The broker UAI image that comes with UAS is the image used to construct broker UAIs.
The key pieces of the broker UAI image are:
 An entrypoint shell script that initializes the container and starts the SSH daemon running. An SSH configuration that forces logged in users into the switchboard command which creates / selects end-user UAIs and redirects connections.  The primary way to customize the broker UAI image is by defining volumes and connecting them to the broker UAI class for a given broker.</description>
    </item>
    
    <item>
      <title>Delete A UAI</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai/</guid>
      <description>Delete a UAI The cray uas command allows users to manage UAIs. This procedure deletes one of the user&amp;rsquo;s UAIs. To delete all UAIs on the system, see List and Delete All UAIs for more information.
Prerequisites A UAI is up and running.
Limitations Currently, the user must SSH to the system as root.
Procedure   Log in to an NCN as root.
  List existing UAIs.
ncn-w001# cray uas list username = &amp;#34;user&amp;#34; uai_host = &amp;#34;ncn-w001&amp;#34; uai_status = &amp;#34;Running: Ready&amp;#34; uai_connect_string = &amp;#34;ssh user@203.</description>
    </item>
    
    <item>
      <title>Delete A UAI Class</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_class/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_class/</guid>
      <description>Delete a UAI Class Delete a UAI class. After deletion, the class will no longer be available for new UAIs.
Prerequisites  Install and initialize the cray administrative CLI. Obtain the ID of the UAI class that will be deleted.  Procedure Delete a UAI by using a command of the following form:
cray uas admin config classes delete UAI_CLASS_ID UAI_CLASS_ID is the UAS ID of the UAI class.
  Delete a UAI class.</description>
    </item>
    
    <item>
      <title>Delete A UAI Image Registration</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_image_registration/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/delete_a_uai_image_registration/</guid>
      <description>Delete a UAI Image Registration Unregister a UAI image from UAS.
Prerequisites Verify that the UAI image to be deleted is registered with UAS. See Retrieve UAI Image Registration Information for instructions.
Procedure Deleting a UAI image from UAS effectively unregisters the UAI image from UAS. This procedure does delete the actual UAI image artifact.
  Delete a UAS image registration by using a command of the following form:</description>
    </item>
    
    <item>
      <title>Create And Register A Custom UAI Image</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_and_register_a_custom_uai_image/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:00 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_and_register_a_custom_uai_image/</guid>
      <description>Create and Register a Custom UAI Image Create a custom UAI image based on the current compute node image. This UAI image can then be used to build compute node software with the Cray Programming Environment (PE).
Prerequisites  This procedure requires administrator privileges. Log into either a master or worker NCN node (not a LiveCD node).  Procedure The default end-user UAI is not suitable for use with the Cray PE.</description>
    </item>
    
    <item>
      <title>Create And Use Default UAIs In Legacy Mode</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_and_use_default_uais_in_legacy_mode/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:00 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_and_use_default_uais_in_legacy_mode/</guid>
      <description>Create and Use Default UAIs in Legacy Mode Create a UAI using the default UAI image or the default UAI class in legacy mode.
Procedure   Create a UAI with a command of the following form:
user&amp;gt; cray uas create --public-key &#39;&amp;lt;path&amp;gt;&#39; &amp;lt;path&amp;gt; is the path to a file containing an SSH public-key matched to the SSH private key belonging to the user.
  Watch the UAI and see when it is ready for logins.</description>
    </item>
    
    <item>
      <title>Customize End-user UAI Images</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/customize_end-user_uai_images/</link>
      <pubDate>Sat, 13 Nov 2021 03:16:00 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/customize_end-user_uai_images/</guid>
      <description>Customize End-User UAI Images The provided end-user UAI image is a basic UAI image that includes an up-to-date version of the Sles Linux Distribution and client support for both the Slurm and PBS Professional workload managers. It provides an entrypoint to using UAIs and doing workload management from UAIs. This UAI image is not suitable for use with the Cray PE because it cannot be assured of being up-to-date with what is running on Shasta compute nodes at a given site.</description>
    </item>
    
    <item>
      <title>Create A UAI Resource Specification</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_resource_specification/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:59 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_resource_specification/</guid>
      <description>Create a UAI Resource Specification Add a resource specification to UAS. Once added, a resource specification can be used to limit UAI resource consumption on host nodes and enable UAIs to access external data.
Prerequisites Install and initialize the cray administrative CLI.
Procedure   Add a resource specification.
Use a command of the following form:
ncn-m001-pit # cray uas admin config resources create [--limit &amp;lt;k8s-resource-limit&amp;gt;] [--request &amp;lt;k8s-resource-request&amp;gt;] [--comment &#39;&amp;lt;string&amp;gt;&#39;] For example:</description>
    </item>
    
    <item>
      <title>Create A UAI Using A Direct Administrative Command</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_using_a_direct_administrative_command/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:59 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_using_a_direct_administrative_command/</guid>
      <description>Create a UAI Using a Direct Administrative Command Administrators can use this method to manually create UAIs. This method is intended more for creating broker UAIs than for creating end-user UAIs.
Prerequisites Install and initialize the cray administrative CLI.
Procedure This method is intended more for creating broker UAIs than for creating end-user UAIs. Administrators can, however, create end-user UAIs using this method.
  Create a UAI manually with a command of the form:</description>
    </item>
    
    <item>
      <title>Create A UAI With Additional Ports</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_with_additional_ports/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:59 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_with_additional_ports/</guid>
      <description>Create a UAI with Additional Ports An option is available to expose UAI ports to the customer user network in addition to the the port used for SSH access. These ports are restricted to ports 80, 443, and 8888. This procedure allows a user or administrator to create a new UAI with these additional ports.
Prerequisites A public SSH key
Limitations Only ports 80, 443, and 8888 can be exposed. Attempting to open any other ports will result in an error.</description>
    </item>
    
    <item>
      <title>Create A UAI</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:58 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai/</guid>
      <description>Create a UAI It is rare that an an administrator would hand-craft a UAI in this way, but it is possible. This is the mechanism used to create broker UAIs for the broker mode of UAI management.
Refer to Broker Mode UAI Management for more information.
Prerequisites This procedure requires administrative privileges.
Procedure   Create a UAI manually.
Use a command of the following form:
cray uas admin uais create [options] The following options are available for use:</description>
    </item>
    
    <item>
      <title>Create A UAI Class</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_class/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:58 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_a_uai_class/</guid>
      <description>Create a UAI Class Add a new User Access Instance (UAI) class to the User Access Service (UAS) so that the class can be used to configure UAIs.
Prerequisites Install and initialize the cray administrative CLI.
Procedure   Add a UAI class by using the command in the following example.
ncn-m001-pit# cray uas admin config classes create --image-id &amp;lt;image-id&amp;gt; [options] --image-id &amp;lt;image-id&amp;gt; specifies the UAI image identifier of the UAI image to be used in creating UAIs of the new class.</description>
    </item>
    
    <item>
      <title>Configure A Broker UAI Class</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_a_broker_uai_class/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:57 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_a_broker_uai_class/</guid>
      <description>Configure a Broker UAI Class Configuring a broker UAI class consists of the following:
 Create volumes to hold any site-specific authentication, SSH, or other configuration required Choose the end-user UAI class for which the broker UAI will serve instances Create a UAI Class with (at a minimum):  namespace set to uas default set to false volume_mounts set to the list of customization volume-ids created above public_ip set to true uai_compute_network set to false uai_creation_class set to the class-id of the end-user UAI class    Example of Volumes to Connect Broker UAIs to LDAP Broker UAIs authenticate users in SSH, and pass the SSH connection on to the selected or created end-user UAI.</description>
    </item>
    
    <item>
      <title>Configure A Default UAI Class For Legacy Mode</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_a_default_uai_class_for_legacy_mode/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:57 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_a_default_uai_class_for_legacy_mode/</guid>
      <description>Configure a Default UAI Class for Legacy Mode Using a default UAI class is optional but recommended for any site using the legacy UAI management mode that wants to have some control over UAIs created by users. UAI classes used for this purpose need to have certain minimum configuration in them:
 The image_id field set to identify the image used to construct UAIs The volume_list field set to the list of volumes to mount in UAIs The public_ip field set to true The uai_compute_network flag set to true (if workload management will be used) The default flag set to true to make this the default UAI class  To make UAIs useful, there is a minimum set of volumes that should be defined in the UAS configuration:</description>
    </item>
    
    <item>
      <title>Create UAIs From Specific Uai Images In Legacy Mode</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_uais_from_specific_uai_images_in_legacy_mode/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:57 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/create_uais_from_specific_uai_images_in_legacy_mode/</guid>
      <description>Create UAIs From Specific UAI Images in Legacy Mode A user can create a UAI from a specific UAI image (assuming no default UAI class exists) using a command of the form:
user&amp;gt; cray uas create --publickey &amp;lt;path&amp;gt; --imagename &amp;lt;image-name&amp;gt; &amp;lt;image-name&amp;gt; is the name shown above in the list of UAI images.
For example:
vers&amp;gt; cray uas images list default_image = &amp;quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest&amp;quot; image_list = [ &amp;quot;dtr.dev.cray.com/cray/cray-uai-broker:latest&amp;quot;, &amp;quot;dtr.dev.cray.com/cray/cray-uas-sles15:latest&amp;quot;, &amp;quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest&amp;quot;,] vers&amp;gt; cray uas create --publickey ~/.</description>
    </item>
    
    <item>
      <title>Broker Mode UAI Management</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/broker_mode_uai_management/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:56 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/broker_mode_uai_management/</guid>
      <description>Broker Mode UAI Management A UAI broker is a special kind of UAI whose job is not to host users directly but to field attempts to reach a UAI, locate or create a UAI for the user making the attempt, and then pass the connection on to the correct UAI. Multiple UAI brokers can be created, each serving a UAI of a different class, making it possible to set up UAIs for varying workflows and environments as needed.</description>
    </item>
    
    <item>
      <title>Configure End-user UAI Classes For Broker Mode</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_end-user_uai_classes_for_broker_mode/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:56 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_end-user_uai_classes_for_broker_mode/</guid>
      <description>Configure End-User UAI Classes for Broker Mode Each UAI broker will create and manage a single class of end-user UAIs. Setting up UAI classes for this is similar to Configure a Default UAI Class for Legacy Mode with the following exceptions:
 The public_ip flag for brokered UAI classes should be set to false The default flag for brokered UAI classes may be set to true or false but should, most likely, be set to false.</description>
    </item>
    
    <item>
      <title>Configure UAIs In UAS</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_uais_in_uas/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:56 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/configure_uais_in_uas/</guid>
      <description>Configure UAIs in UAS The four main items of UAI configuration in UAS. Links to procedures for listing, adding, examining, updating, and deleting each item.
Options for the elements of a UAI are maintained in the UAS configuration. The following can be configured in UAS:
 UAI images Volumes Resource specifications UAI Classes  Only users who are defined as administrators in an HPE Cray EX system and are logged in using the administrative CLI (cray command) can configure UAS.</description>
    </item>
    
    <item>
      <title>Add A Volume To UAS</title>
      <link>/docs-csm/en-12/operations/uas_user_and_admin_topics/add_a_volume_to_uas/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:55 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/uas_user_and_admin_topics/add_a_volume_to_uas/</guid>
      <description>Add a Volume to UAS This procedure registers and configures a volume in UAS so that the volume can be mounted in UAIs.
See List Volumes Registered in UAS for examples of valid volume configurations. Refer to Elements of a UAI for descriptions of the volume configuration fields and values.
Note the following caveats about adding volumes to UAS:
 A volume description may specify an underlying directory that is NFS-mounted on the UAI host nodes.</description>
    </item>
    
    <item>
      <title>Component Names (xnames)</title>
      <link>/docs-csm/en-12/operations/component_names_xnames/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:55 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/component_names_xnames/</guid>
      <description>Component Names (xnames) Component names (xnames) identify the geolocation for hardware components in the HPE Cray EX system. Every component is uniquely identified by these component names. Some, like the system cabinet number or the CDU number, can be changed by site needs. There is no geolocation encoded within the cabinet number, such as an X-Y coordinate system to relate to the floor layout of the cabinets. Other component names refer to the location within a cabinet and go down to the port on a card or switch or the socket holding a processor or a memory DIMM location.</description>
    </item>
    
    <item>
      <title>Validate Signed Rpms</title>
      <link>/docs-csm/en-12/operations/csm_product_management/validate_signed_rpms/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:55 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/validate_signed_rpms/</guid>
      <description>Validate Signed RPMs The HPE Cray EX system signs RPMs to provide an extra level of security. Use the following procedure to import a key from either CrayPort or a Kubernetes Secret, and then use that key to validate the RPM package signatures on each node type.
The RPMs will vary on compute, application, worker, master, and storage nodes. Check each node type to ensure the RPMs are correctly signed.</description>
    </item>
    
    <item>
      <title>Post-install Customizations</title>
      <link>/docs-csm/en-12/operations/csm_product_management/post_install_customizations/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:54 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/post_install_customizations/</guid>
      <description>Post-Install Customizations Post-install customizations may be needed as systems scale. These customizations also need to persist across future installs or upgrades. Not all resources can be customized post-install; common scenarios are documented in the following sections.
The following is a guide for determining where issues may exist, how to adjust the resources, and how to ensure the changes will persist. Different values may be be needed for systems as they scale.</description>
    </item>
    
    <item>
      <title>Remove Artifacts From Product Installations</title>
      <link>/docs-csm/en-12/operations/csm_product_management/remove_artifacts_from_product_installations/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:54 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/remove_artifacts_from_product_installations/</guid>
      <description>Remove Artifacts from Product Installations Remove product artifacts that were imported from various Cray products. These instructions provide guidance for removing Image Management Service (IMS) images, IMS recipes, and Git repositories present in the Cray Product Catalog from the system.
The examples in this procedure show how to remove the product artifacts for the Cray System Management (CSM) product.
WARNING: If individual Cray products have removal procedures, those instructions supersede this procedure.</description>
    </item>
    
    <item>
      <title>Configure Keycloak Account</title>
      <link>/docs-csm/en-12/operations/csm_product_management/configure_keycloak_account/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:53 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/configure_keycloak_account/</guid>
      <description>Configure Keycloak Account Installation of CSM software includes a default account for administrative access to keycloak.
Depending on choices made during the installation, there may be a federated connection to an external Identity Provider (IdP), such as an LDAP or AD server, which enables the use of external accounts in keycloak.
However, if the external accounts are not available, then an &amp;ldquo;internal user account&amp;rdquo; could be created in keycloak. Having a usable account in keycloak with administrative authorization enables the use of the cray CLI for many administrative commands, such as those used to Validate CSM Health and general operation of the management services via the API gateway.</description>
    </item>
    
    <item>
      <title>Configure Non-compute Nodes With CFS</title>
      <link>/docs-csm/en-12/operations/csm_product_management/configure_non-compute_nodes_with_cfs/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:53 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/configure_non-compute_nodes_with_cfs/</guid>
      <description>Configure Non-Compute Nodes with CFS Non-compute node (NCN) personalization applies post-boot configuration to the HPE Cray EX management nodes. Several HPE Cray EX product environments outside of CSM require NCN personalization to function. Consult the manual for each product to configure them on NCNs by referring to the 1.5 HPE Cray EX System Software Getting Started Guide S-8000 on the HPE Customer Support Center.
This procedure defines the NCN personalization process for the CSM product using the Configuration Framework Service (CFS).</description>
    </item>
    
    <item>
      <title>Perform NCN Personalization</title>
      <link>/docs-csm/en-12/operations/csm_product_management/perform_ncn_personalization/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:53 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/perform_ncn_personalization/</guid>
      <description>Perform NCN Personalization NCN personalization is the process of applying product-specific configuration to NCNs post-boot.
Prerequisites Prior to running this procedure, gather the following information required by CFS to create a configuration layer:
 HTTP clone URL for the configuration repository in VCS Path to the Ansible play to run in the repository Commit ID in the repository for CFS to pull and run on the nodes  Products may supply multiple plays to run, in which case multiple configuration layers must be created.</description>
    </item>
    
    <item>
      <title>Accessing Livecd Usb Device After Reboot</title>
      <link>/docs-csm/en-12/operations/access_livecd_usb_device_after_reboot/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:52 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/access_livecd_usb_device_after_reboot/</guid>
      <description>Accessing LiveCD USB Device After Reboot This is an procedure that only applies to the LiveCD USB device after the PIT node has been rebooted.
 USB ONLY If the installation above was done from a Remote ISO.
 After deploying the LiveCD&amp;rsquo;s NCN, the LiveCD USB itself is unharmed and available to an administrator.
Procedure   Mount and view the USB device.
ncn-m001# mkdir -pv /mnt/{cow,pitdata} ncn-m001# mount -vL cow /mnt/cow ncn-m001# mount -vL PITDATA /mnt/pitdata ncn-m001# ls -ld /mnt/cow/rw/* Example output:</description>
    </item>
    
    <item>
      <title>Change Passwords And Credentials</title>
      <link>/docs-csm/en-12/operations/csm_product_management/change_passwords_and_credentials/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:52 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/change_passwords_and_credentials/</guid>
      <description>Change Passwords and Credentials This is an overarching procedure to change all credentials managed by Cray System Management (CSM) in HPE Cray EX system to new values.
There are many passwords and credentials used in different contexts to manage the system. These can be changed as needed. Their initial settings are documented, so it is recommended to change them during or soon after a CSM software installation.
Prerequisites  Review procedures in Manage System Passwords.</description>
    </item>
    
    <item>
      <title>Scenarios For Shasta V1.5</title>
      <link>/docs-csm/en-12/introduction/scenarios/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:52 +0000</pubDate>
      
      <guid>/docs-csm/en-12/introduction/scenarios/</guid>
      <description>Scenarios for Shasta v1.5 There are multiple scenarios for installing CSM software which are described in this documentation with many supporting procedures.
 Scenarios for Shasta v1.5  Installation Upgrade Migration    Installation There are two ways to install the CSM software. There are some differences between a first time install which must create the initial configuration payload and configure the management network switches, whereas a reinstall can reuse a previous configuration payload and skip the configuration of management network switches.</description>
    </item>
    
    <item>
      <title>Differences From Previous Release</title>
      <link>/docs-csm/en-12/introduction/differences/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:51 +0000</pubDate>
      
      <guid>/docs-csm/en-12/introduction/differences/</guid>
      <description>Differences from Previous Release The most noteworthy changes since the previous release are described here.
Topics:  New Features Deprecating Features Deprecated Features Other Changes  Details New Features  Scaling improvements for larger systems  BOS CAPMC FAS   New hardware supported in this release:  Compute nodes  Milan-Based Grizzly Peak with A100 40 GB GPU Milan-Based Windom Liquid Cooled System Rome-Based HPE Apollo 6500 XL675d Gen10+ with A100 40 GB GPU Rome-Based HPE Apollo 6500 XL645d Gen10+ with A100 40 GB GPU   User Access Nodes (UANs)  Milan-Based HPE DL 385(v2) Gen10+ Rome-Based HPE DL 385(v1) Gen10     Node consoles are now managed by cray-console-node which is based on conman.</description>
    </item>
    
    <item>
      <title>Documentation Conventions</title>
      <link>/docs-csm/en-12/introduction/documentation_conventions/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:51 +0000</pubDate>
      
      <guid>/docs-csm/en-12/introduction/documentation_conventions/</guid>
      <description>Documentation Conventions Several conventions have been used in the preparation of this documentation.
 Markdown Format File Formats Typographic Conventions Command Prompt Conventions which describe the context for user, host, directory, chroot environment, or container environment  Markdown Format This documentation is in Markdown format. Although much of it can be viewed with any text editor, a richer experience will come from using a tool which can render the Markdown to show different font sizes, the use of bold and italics formatting, inclusion of diagrams and screen shots as image files, and to follow navigational links within a topic file and to other files.</description>
    </item>
    
    <item>
      <title>CAPMC Deprecation Notice Many Capmc V1 Features Are Being Partially Deprecated</title>
      <link>/docs-csm/en-12/introduction/capmc_deprecation/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:50 +0000</pubDate>
      
      <guid>/docs-csm/en-12/introduction/capmc_deprecation/</guid>
      <description>CAPMC Deprecation Notice: many CAPMC v1 features are being partially deprecated Deprecated Features in CSM 1.0 Many CAPMC v1 REST API and CLI features are being deprecated as part of CSM version 1.0; Full removal of the following deprecated CAPMC features will happen in CSM version 1.3. Further development of CAPMC service or CLI has stopped. CAPMC has entered end-of-life but will still be generally available. CAPMC is going to be replaced with the Power Control Service (PCS) in a future release.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>/docs-csm/en-12/introduction/csm_overview/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:50 +0000</pubDate>
      
      <guid>/docs-csm/en-12/introduction/csm_overview/</guid>
      <description>CSM Overview This CSM Overview describes the Cray System Management ecosystem with its hardware, software, and network. It describes how to access these services and components.
The CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage. These nodes perform their function as Kubernetes master nodes, Kubernetes worker nodes, or utility storage nodes with the Ceph storage.</description>
    </item>
    
    <item>
      <title>PowerDNS Migration Notice</title>
      <link>/docs-csm/en-12/introduction/powerdns_migration/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:50 +0000</pubDate>
      
      <guid>/docs-csm/en-12/introduction/powerdns_migration/</guid>
      <description>PowerDNS Migration Notice The migration to PowerDNS as the authoritative DNS source and the introduction of Bifurcated CAN (Customer Access Network) will result in some changes to the node and service naming conventions.
DNS Record Naming Changes Fully qualified domain names will be introduced for all DNS records.
Canonical name: hostname.network-path.system-name.tld
 hostname - The hostname of the node or service network-path - The network path used to access the node  .</description>
    </item>
    
    <item>
      <title>Wipe NCN Disks For Reinstallation</title>
      <link>/docs-csm/en-12/install/wipe_ncn_disks_for_reinstallation/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:50 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/wipe_ncn_disks_for_reinstallation/</guid>
      <description>Wipe NCN Disks for Reinstallation This page will detail how disks are wiped and includes workarounds for wedged disks. Any process covered on this page will be covered by the installer.
 Everything in this section should be considered DESTRUCTIVE.
 After following these procedures an NCN can be rebooted and redeployed.
Ideally the Basic Wipe is enough, and should be tried first. All types of disk wipe can be run from Linux or an initramFS/initrd emergency shell.</description>
    </item>
    
    <item>
      <title>Switch PXE Boot From Onboard NIC To Pcie</title>
      <link>/docs-csm/en-12/install/switch_pxe_boot_from_onboard_nic_to_pcie/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:49 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/switch_pxe_boot_from_onboard_nic_to_pcie/</guid>
      <description>Switch PXE Boot from Onboard NIC to PCIe This section details how to migrate NCNs from using their onboard NICs for PXE booting to booting over the PCIe cards.
 Switch PXE Boot from Onboard NIC to PCIe  Enabling UEFI PXE Mode  Mellanox  Print Current UEFI and SR-IOV State Setting Expected Values High-Speed Network  Obtaining Mellanox Tools     QLogic FastLinq  Kernel Modules     Disabling or Removing On-Board Connections    This applies to Newer systems (Spring 2020 or newer) where onboard NICs are still used.</description>
    </item>
    
    <item>
      <title>Troubleshooting Installation Problems</title>
      <link>/docs-csm/en-12/install/troubleshooting_installation/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:49 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/troubleshooting_installation/</guid>
      <description>Troubleshooting Installation Problems The installation of the Cray System Management (CSM) product requires knowledge of the various nodes and switches for the HPE Cray EX system. The procedures in this section should be referenced during the CSM install for additional information on system hardware, troubleshooting, and administrative tasks related to CSM.
Topics:  Reset root Password on LiveCD Reinstall LiveCD PXE Boot Troubleshooting Wipe NCN Disks for Reinstallation Restart Network Services and Interfaces on NCNs Utility Storage Node Installation Troubleshooting Ceph CSI Troubleshooting Safeguards for CSM NCN Upgrades  Details</description>
    </item>
    
    <item>
      <title>Utility Storage Installation Troubleshooting</title>
      <link>/docs-csm/en-12/install/utility_storage_node_installation_troubleshooting/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:49 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/utility_storage_node_installation_troubleshooting/</guid>
      <description>Utility Storage Installation Troubleshooting If there is a failure in the creation of Ceph storage on the utility storage nodes for one of these scenarios, the Ceph storage might need to be reinitialized.
Topics  Scenario 1 (Shasta v1.4 only) Scenario 2 (Shasta v1.5 only)  Details Scenario 1 (Shasta 1.4 only) IMPORTANT (FOR NODE INSTALLS/REINSTALLS ONLY): If the Ceph install failed, check the following:
ncn-s# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 83.</description>
    </item>
    
    <item>
      <title>Validate Management Network Cabling</title>
      <link>/docs-csm/en-12/install/validate_management_network_cabling/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:49 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/validate_management_network_cabling/</guid>
      <description>Validate Management Network Cabling This page is designed to be a guide on how all nodes in a Shasta system are wired to the management network.
The Shasta Cabling Diagram (SHCD) for this system describes how the cables connect the nodes to the management network switches and the connections between the different types of management network switches. Having SHCD data which matches how the physical system is cabled will be needed later when preparing the hmn_connections.</description>
    </item>
    
    <item>
      <title>Safeguards For</title>
      <link>/docs-csm/en-12/install/safeguards_for_csm_ncn_upgrades/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:48 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/safeguards_for_csm_ncn_upgrades/</guid>
      <description>Safeguards for CSM This page covers safe-guards for preventing destructive behaviors on management nodes.
If reinstalling or upgrading, run through these safe-guards on a by-case basis:
 Whether or not CEPH should be preserved. Whether or not the RAIDs should be protected.  Safeguard CEPH OSDs   Edit /var/www/ephemeral/configs/data.json and align the following options:
{ .. // Disables Ceph wipe: &amp;#34;wipe-ceph-osds&amp;#34;: &amp;#34;no&amp;#34; .. } { .. // Restores default behavior: &amp;#34;wipe-ceph-osds&amp;#34;: &amp;#34;yes&amp;#34; .</description>
    </item>
    
    <item>
      <title>Set Gigabyte Node BMC To Factory Defaults</title>
      <link>/docs-csm/en-12/install/set_gigabyte_node_bmc_to_factory_defaults/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:48 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/set_gigabyte_node_bmc_to_factory_defaults/</guid>
      <description>Set Gigabyte Node BMC to Factory Defaults Prerequisites Use the management scripts and text files to reset Gigabyte BMC to factory default settings. Set the BMC to the factory default settings in the following cases:
 There are problems using the ipmitool command and Redfish does not respond There are problems using the ipmitool command and Redfish is running When BIOS or BMC flash procedures fail using Redfish  Run the do_bmc_factory_default.</description>
    </item>
    
    <item>
      <title>Shcd Hmn Tab/hmn Connections Rules</title>
      <link>/docs-csm/en-12/install/shcd_hmn_connections_rules/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:48 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/shcd_hmn_connections_rules/</guid>
      <description>SHCD HMN Tab/HMN Connections Rules Table of contents:  Introduction Compute Node  Dense 4 node chassis - Gigabyte or Intel chassis Single node chassis - Apollo 6500 XL675D Dual node chassis - Apollo 6500 XL645D   Chassis Management Controller (CMC) Management Node  Master Worker Storage   Application Node  Single Node Chassis  Building xnames for nodes in a single application node chassis   Dual Node Chassis  Building xnames for nodes in a dual application node chassis     Columbia Slingshot Switch PDU Cabinet Controller Cooling Door Management Switches  Introduction The HMN tab of the SHCD describes the air-cooled hardware present in the system and how these devices are connected to the Hardware Management Network (HMN).</description>
    </item>
    
    <item>
      <title>Redeploy Pit Node</title>
      <link>/docs-csm/en-12/install/redeploy_pit_node/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:47 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/redeploy_pit_node/</guid>
      <description>Redeploy PIT Node The following procedure contains information for rebooting and deploying the management node that is currently hosting the LiveCD. This assists with remote-console setup to aid in observing the reboot. At the end of this procedure, the LiveCD will no longer be active. The node it was using will join the Kubernetes cluster as the final of three master nodes forming a quorum.
IMPORTANT: While the node is rebooting, it will only be available through Serial-over-LAN and local terminals.</description>
    </item>
    
    <item>
      <title>Reinstall Livecd</title>
      <link>/docs-csm/en-12/install/reinstall_livecd/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:47 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/reinstall_livecd/</guid>
      <description>Reinstall LiveCD Setup a re-install of LiveCD on a node using the previous configuration.
  Backup to the data partition:
pit# mkdir -pv /var/www/ephemeral/backup pit# pushd /var/www/ephemeral/backup pit# tar -czvf &amp;#34;dnsmasq-data-$(date &amp;#39;+%Y-%m-%d_%H-%M-%S&amp;#39;).tar.gz&amp;#34; /etc/dnsmasq.* pit# tar -czvf &amp;#34;network-data-$(date &amp;#39;+%Y-%m-%d_%H-%M-%S&amp;#39;).tar.gz&amp;#34; /etc/sysconfig/network/* pit# cp -pv /etc/hosts ./ pit# popd pit# umount -v /var/www/ephemeral   Unplug the USB device.
The USB device should now contain all the information already loaded, as well as the backups of the initialized files.</description>
    </item>
    
    <item>
      <title>Reset Root Password On Livecd</title>
      <link>/docs-csm/en-12/install/reset_root_password_on_livecd/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:47 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/reset_root_password_on_livecd/</guid>
      <description>Reset root Password on LiveCD It may become desirable to clear the password on the LiveCD.
The root password is preserved within the COW partition at cow:rw/etc/shadow. This is the modified copy of the /etc/shadow file used by the operating system.
If a site/user needs to reset/clear the password for root, they can mount their USB on another machine and remove this file from the COW partition. When next booting from the USB it will reinitialize to an empty password for root, and again at next login it will require the password to be changed.</description>
    </item>
    
    <item>
      <title>Restart Network Services And Interfaces On NCNs</title>
      <link>/docs-csm/en-12/install/restart_network_services_and_interfaces_on_ncns/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:47 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/restart_network_services_and_interfaces_on_ncns/</guid>
      <description>Restart Network Services and Interfaces on NCNs Interfaces within the network stack can be reloaded or reset to fix wedged interfaces. The NCNs have network device names set during first boot. The names vary based on the available hardware. For more information, see NCN Networking. Any process covered on this page will be covered by the installer.
The use cases for resetting services:
 Interfaces not showing up IP Addresses not applying Member/children interfaces not being included  Topics:  Restart Network Services and Interfaces))) Command Reference  Check interface status (up/down/broken) Show routing and status for all devices Print real devices ( ignore no-device ) Show the currently enabled network service (Wicked or Network Manager)    Restart Network Services There are a few daemons that make up the SUSE network stack.</description>
    </item>
    
    <item>
      <title>Prepare Management Nodes</title>
      <link>/docs-csm/en-12/install/prepare_management_nodes/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:46 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/prepare_management_nodes/</guid>
      <description>Prepare Management Nodes The procedures described on this page are being done before any node is booted with the Cray Pre-Install Toolkit. When the PIT node is referenced during these procedures, it means the node that will be be booted as the PIT node.
Topics:  Quiesce Compute and Application Nodes Disable DHCP Service (if any management nodes are booted) Wipe Disks on Booted Nodes Power Off Booted Nodes Set Node BMCs to DHCP Wipe USB Device on PIT Node (Only if switching from USB LiveCD method to RemoteISO LiveCD method) Power Off PIT Node  Quiesce compute nodes and application nodes.</description>
    </item>
    
    <item>
      <title>Prepare Site Init</title>
      <link>/docs-csm/en-12/install/prepare_site_init/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:46 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/prepare_site_init/</guid>
      <description>Prepare Site Init These procedures guide administrators through setting up the site-init directory which contains important customizations for various products. The appendix is informational only; it does not include any default install procedures.
Topics:  Background Create and Initialize Site-Init Directory Create Baseline System Customizations Generate Sealed Secrets Version Control Site-Init Files  Push to a Remote Repository   Patch cloud-init with the CA Customer-Specific Customizations  Details 1. Background The shasta-cfg directory included in CSM includes relatively static, installation-centric artifacts such as:</description>
    </item>
    
    <item>
      <title>PXE Boot Troubleshooting</title>
      <link>/docs-csm/en-12/install/pxe_boot_troubleshooting/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:46 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/pxe_boot_troubleshooting/</guid>
      <description>PXE Boot Troubleshooting This page is designed to cover various issues that arise when trying to PXE boot nodes in an HPE Cray EX system.
 PXE Boot Troubleshooting  Configuration required for PXE booting Switch Configuration  Aruba Configuration Mellanox Configuration   Next steps  Restart BSS Restart KEA Missing BSS Data      In order for PXE booting to work successfully, the management network switches need to be configured correctly.</description>
    </item>
    
    <item>
      <title>Install Services</title>
      <link>/docs-csm/en-12/install/install_csm_services/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:45 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/install_csm_services/</guid>
      <description>Install CSM Services This procedure will install CSM applications and services into the CSM Kubernetes cluster.
 Node: Check the information in Known Issues before starting this procedure to be warned about possible problems.
 Topics:  Initialize Bootstrap Registry Create Site-Init Secret Deploy Sealed Secret Decryption Key Deploy CSM Applications and Services Setup Nexus Set NCNs to use Unbound Apply Pod Priorities Apply After Sysmgmt Manifest Workarounds Known Issues  install.</description>
    </item>
    
    <item>
      <title>Prepare Compute Nodes</title>
      <link>/docs-csm/en-12/install/prepare_compute_nodes/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:45 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/prepare_compute_nodes/</guid>
      <description>Prepare Compute nodes Some compute nodes types have special preparation steps, but most compute nodes are ready to be used now These nodes have an additional procedure before they can be booted.
Topics:  Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes  Prerequisites The time for Gigabyte compute nodes is synced with the rest of the system. See Update the Gigabyte Server BIOS Time.
Details 1. Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes The HPE Apollo 6500 XL645d Gen10 Plus compute node uses a NIC/shared iLO network port.</description>
    </item>
    
    <item>
      <title>Prepare Configuration Payload</title>
      <link>/docs-csm/en-12/install/prepare_configuration_payload/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:45 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/prepare_configuration_payload/</guid>
      <description>Prepare Configuration Payload The configuration payload consists of the information which must be known about the HPE Cray EX system so it can be passed to the csi (Cray Site Init) program during the CSM installation process.
Information gathered from a site survey is needed to feed into the CSM installation process, such as system name, system size, site network information for the CAN, site DNS configuration, site NTP configuration, network information for the node used to bootstrap the installation.</description>
    </item>
    
    <item>
      <title>Create NCN Metadata Csv</title>
      <link>/docs-csm/en-12/install/create_ncn_metadata_csv/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:44 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/create_ncn_metadata_csv/</guid>
      <description>Create NCN Metadata CSV The information in the ncn_metadata.csv file identifies each of the management nodes, assigns the function as a master, worker, or storage node, and provides the MAC address information needed to identify the BMC and the NIC which will be used to boot the node.
Some of the data in the ncn_metadata.csv can be found in the SHCD. However, the hardest data to collect is the MAC addresses for the node&amp;rsquo;s BMC, the node&amp;rsquo;s bootable network interface, and the pair of network interfaces which will become the bonded interface bond0.</description>
    </item>
    
    <item>
      <title>Create Switch Metadata Csv</title>
      <link>/docs-csm/en-12/install/create_switch_metadata_csv/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:44 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/create_switch_metadata_csv/</guid>
      <description>Create Switch Metadata CSV This page provides directions on constructing the switch_metadata.csv file.
This file is manually created to include information about all spine, leaf, CDU, and aggregation switches in the system. None of the Slingshot switches for the HSN should be included in this file.
The file should have the following format, in ascending order by Xname:
Switch Xname,Type,Brand d0w1,CDU,Dell d0w2,CDU,Dell x3000c0w38,Leaf,Dell x3000c0w36,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h34s1,Spine,Mellanox The above file would lead to this pairing between component name and hostname:</description>
    </item>
    
    <item>
      <title>Deploy Management Nodes</title>
      <link>/docs-csm/en-12/install/deploy_management_nodes/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:44 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/deploy_management_nodes/</guid>
      <description>Deploy Management Nodes The following procedure deploys Linux and Kubernetes software to the management NCNs. Deployment of the nodes starts with booting the storage nodes followed by the master nodes and worker nodes together. After the operating system boots on each node, there are some configuration actions which take place. Watching the console or the console log for certain nodes can help to understand what happens and when. When the process completes for all nodes, the Ceph storage is initialized and the Kubernetes cluster is created and ready for a workload.</description>
    </item>
    
    <item>
      <title>Connect To Switch Over Usb-serial Cable</title>
      <link>/docs-csm/en-12/install/connect_to_switch_over_usb_serial_cable/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/connect_to_switch_over_usb_serial_cable/</guid>
      <description>Connect to Switch over USB-Serial Cable In the event that network plumbing is lacking, down, or unconfigured for procuring devices, then it is recommended to use the Serial/COM ports on the spine and leaf switches.
This guide will instruct the user on procuring MAC addresses for the NCNs metadata files with the serial console.
Mileage may vary, as some obstacles such as BAUDRATE and terminal usage vary per manufacturer.
Common Manufacturers Refer to the external support/documentation portals for more information:</description>
    </item>
    
    <item>
      <title>Create Application Node Config Yaml</title>
      <link>/docs-csm/en-12/install/create_application_node_config_yaml/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/create_application_node_config_yaml/</guid>
      <description>Create Application Node Config YAML This topic provides directions on constructing the application_node_config.yaml file. This file controls how the csi config init command finds and treats application nodes discovered in the hmn_connections.json file when generating configuration files for the system.
 Prerequisites Background Directions  Prerequisites The application_node_config.yaml file can be constructed from information from one of the following sources:
 The SHCD Excel spreadsheet for the system The hmn_connections.json file generated from the system&amp;rsquo;s SHCD  Background SHCD and hmn_connections.</description>
    </item>
    
    <item>
      <title>Create Cabinets Yaml</title>
      <link>/docs-csm/en-12/install/create_cabinets_yaml/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/create_cabinets_yaml/</guid>
      <description>Create Cabinets YAML This page provides directions on constructing the optional cabinets.yaml file. This file lists cabinet IDs for any systems with non-contiguous cabinet ID numbers and controls how the csi config init command treats cabinet IDs.
The following example file is manually created and follows this format. Eeach &amp;ldquo;type&amp;rdquo; of cabinet can have several fields: total_number of cabinets of this type, starting_id for this cabinet type, and a list of the IDs.</description>
    </item>
    
    <item>
      <title>Create Hmn Connections Json</title>
      <link>/docs-csm/en-12/install/create_hmn_connections_json/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:43 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/create_hmn_connections_json/</guid>
      <description>Create HMN Connections JSON About this task Use this procedure shows to generate the hmn_connections.json from the system&amp;rsquo;s SHCD Excel document. This process is typically needed when generating the hmn_connections.json file for a new system, or regenerating it when system&amp;rsquo;s SHCD is changed (specifically the HMN tab). The hms-shcd-parser tool can be used to generate the hmn_connections.json file.
The SHCD/HMN Connections Rules document explains the expected naming conventions and rules for the HMN tab of the SHCD, and the hmn_connections.</description>
    </item>
    
    <item>
      <title>Configure Dell Cdu Switch</title>
      <link>/docs-csm/en-12/install/configure_dell_cdu_switch/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_dell_cdu_switch/</guid>
      <description>Configure Dell CDU switch This page describes how Dell CDU switches are configured.
CDU switches are located in liquid-cooled cabinets and provide connectivity to MTN (Mountain) components. CDU switches act as leaf switches in the architecture. They run in a high availability pair and use VLT to provide redundancy.
Prerequisites  Two uplinks from each CDU switch to the upstream switch, this is normally a spine switch. Connectivity to the switch is established.</description>
    </item>
    
    <item>
      <title>Configure Dell Leaf Switch</title>
      <link>/docs-csm/en-12/install/configure_dell_leaf_switch/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_dell_leaf_switch/</guid>
      <description>Configure Dell Leaf Switch This page describes how Dell leaf switches are configured.
Leaf switches are located in air-cooled cabinets and provide connectivity to components in those cabinets.
Prerequisites  Connectivity to the switch is established. There are two uplinks from the switch to the upstream switch; this is can be an aggregation switch or a spine switch.  Here are example snippets from a leaf switch in the SHCD.</description>
    </item>
    
    <item>
      <title>Configure Management Network Switches</title>
      <link>/docs-csm/en-12/install/configure_management_network/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_management_network/</guid>
      <description>Configure Management Network Switches HPE Cray EX systems can have network switches in many roles: spine switches, leaf switches, aggregation switches, and CDU switches. Newer systems have HPE Aruba switches, while older systems have Dell and Mellanox switches. Switch IP addresses are generated by Cray Site Init (CSI).
The configuration steps are different for these switch vendors. The switch configuration procedures for HPE Aruba will be grouped separately from the switch configuration procedures for other vendors.</description>
    </item>
    
    <item>
      <title>Configure Mellanox Spine Switch</title>
      <link>/docs-csm/en-12/install/configure_mellanox_spine_switch/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:42 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_mellanox_spine_switch/</guid>
      <description>Configure Mellanox Spine Switch This page describes how Mellanox spine switches are configured.
Depending on the size of the HPE Cray EX system, the spine switches will serve different purposes. On TDS systems, the NCNs will plug directly into the spine switches. On larger systems with aggregation switches, the spine switches will provide connection between the aggregation switches.
Prerequisites  One connection between the switches is used for the Inter switch link (ISL).</description>
    </item>
    
    <item>
      <title>Configure Aruba Management Network Base</title>
      <link>/docs-csm/en-12/install/configure_aruba_management_network_base/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:41 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_aruba_management_network_base/</guid>
      <description>Configure Aruba Management Network Base This page provides instructions on how to setup the base network configuration of the Shasta management network.
After applying the base configuration, all management switches will be accessible to apply the remaining configuration.
Prerequisites  Console access to all of the switches SHCD available  Configuration The base configuration can be applied once console access to the switches has been established. The purpose of this configuration is to have an IPv6 underlay that provides access to the management switches.</description>
    </item>
    
    <item>
      <title>Configure Aruba Spine Switch</title>
      <link>/docs-csm/en-12/install/configure_aruba_spine_switch/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:41 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_aruba_spine_switch/</guid>
      <description>Configure Aruba Spine Switch This page describes how Aruba spine switches are configured.
Depending on the size of the HPE Cray EX system, the spine switches will serve different purposes. On TDS systems, the NCNs will plug directly into the spine switches. On larger systems with aggregation switches, the spine switches will provide connection between the aggregation switches.
Switch models used: JL635A Aruba 8325-48Y8C and JL636A Aruba 8325-32C
They run in a high availability pair and use VSX to provide redundancy.</description>
    </item>
    
    <item>
      <title>Configure Dell Aggregation Switch</title>
      <link>/docs-csm/en-12/install/configure_dell_aggregation_switch/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:41 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_dell_aggregation_switch/</guid>
      <description>Configure Dell Aggregation Switch This page describes how Dell aggregation switches are configured.
Management nodes and Application nodes will be plugged into aggregation switches.
They run in a high availability pair and use VLT to provide redundancy.
Prerequisites   Three connections between the switches, two of these are used for the Inter switch link (ISL), and one used for the keepalive.
  Connectivity to the switch is established.</description>
    </item>
    
    <item>
      <title>Configure Aruba Aggregation Switch</title>
      <link>/docs-csm/en-12/install/configure_aruba_aggregation_switch/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:40 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_aruba_aggregation_switch/</guid>
      <description>Configure Aruba Aggregation Switch This page describes how Aruba aggregation switches are configured.
Management nodes and Application nodes will be plugged into aggregation switches.
Switch models used: JL635A Aruba 8325-48Y8C
They run in a high availability pair and use VSX to provide redundancy.
Prerequisites   Three connections between the switches, two of these are used for the Inter switch link (ISL), and one used for the keepalive.
  The ISL uses 100GB ports and the keepalive will be a 25 GB port.</description>
    </item>
    
    <item>
      <title>Configure Aruba Cdu Switch</title>
      <link>/docs-csm/en-12/install/configure_aruba_cdu_switch/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:40 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_aruba_cdu_switch/</guid>
      <description>Configure Aruba CDU Switch This page describes how Aruba CDU switches are configured.
CDU switches are located in liquid-cooled cabinets and provide connectivity to MTN (Mountain) components. CDU switches act as leaf switches in the architecture. Aruba JL720A 8360-48XT4 is the model used. They run in a high availability pair and use VSX to provide redundancy.
Prerequisites  There are two uplinks from each CDU switch to the upstream switch. This is normally a spine switch.</description>
    </item>
    
    <item>
      <title>Configure Aruba Leaf Switch</title>
      <link>/docs-csm/en-12/install/configure_aruba_leaf_switch/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:40 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_aruba_leaf_switch/</guid>
      <description>Configure Aruba Leaf Switch This page describes how Aruba leaf switches are configured.
Leaf switches are located in air-cooled cabinets and provide connectivity to components in those cabinets. Aruba JL762A 6300M 48G 4SFP56 is the model used.
Prerequisites  Connectivity to the switch is established. The Configure Aruba Management Network Base procedure has been run. There are two uplinks from the switch to the upstream switch; this is can be an aggregation switch or a spine switch.</description>
    </item>
    
    <item>
      <title>Collecting NCN Mac Addresses</title>
      <link>/docs-csm/en-12/install/collecting_ncn_mac_addresses/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:39 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/collecting_ncn_mac_addresses/</guid>
      <description>Collecting NCN MAC Addresses This procedure will detail how to collect the NCN MAC addresses from an HPE Cray EX system. The MAC addresses needed for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns in ncn_metadata.csv will be collected.
The Bootstrap MAC address will be used for identification of this node during the early part of the PXE boot process before the bonded interface can be established.
The Bond0 MAC0 and Bond0 MAC1 are the MAC addresses for the physical interfaces that the node will use for the various VLANs.</description>
    </item>
    
    <item>
      <title>Collecting The BMC Mac Addresses</title>
      <link>/docs-csm/en-12/install/collecting_bmc_mac_addresses/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:39 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/collecting_bmc_mac_addresses/</guid>
      <description>Collecting the BMC MAC Addresses This guide will detail how to collect BMC MAC Addresses from an HPE Cray EX system with configured switches. The BMC MAC Address is the exclusive, dedicated LAN for the onboard BMC.
Results may vary if an unconfigured switch is being used.
Prerequisites  There is a configured switch with SSH access or unconfigured with COM access (serial-over-lan/DB-9). Another file is available to record the collected BMC information.</description>
    </item>
    
    <item>
      <title>Configure Administrative Access</title>
      <link>/docs-csm/en-12/install/configure_administrative_access/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:39 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/configure_administrative_access/</guid>
      <description>Configure Administrative Access There are several operations which configure administrative access to different parts of the system. Ensuring that the cray CLI can be used by administrative credentials enables use of many management services via commands. The management nodes can be locked from accidental manipulation by the cray capmc and cray fas commands when the intent is to work on the entire system except the management nodes. The cray scsd command can change the SSH keys, NTP server, syslog server, and BMC/controller passwords.</description>
    </item>
    
    <item>
      <title>Clear Gigabyte Cmos</title>
      <link>/docs-csm/en-12/install/clear_gigabyte_cmos/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:38 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/clear_gigabyte_cmos/</guid>
      <description>Clear Gigabyte CMOS Because of a bug in the Gigabyte firmware, the Shasta 1.5 install may negatively impact Gigabyte motherboards when attempting to boot using bonded Mellanox network cards. The result is a board that is unusable until a CMOS clear is physically done via a jumper on the board itself.
A patched firmware release (newer than C20 BIOS) is expected to be available for a future release of Shasta. It is recommended that Gigabyte users wait for this new firmware before attempting an installation of Shasta 1.</description>
    </item>
    
    <item>
      <title>Collect Mac Addresses For NCNs</title>
      <link>/docs-csm/en-12/install/collect_mac_addresses_for_ncns/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:38 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/collect_mac_addresses_for_ncns/</guid>
      <description>Collect MAC Addresses for NCNs Now that the PIT node has been booted with the LiveCD and the management network switches have been configured, the actual MAC addresses for the management nodes can be collected. This process will include repetition of some of the steps done up to this point because csi config init will need to be run with the proper MAC addresses and some services will need to be restarted.</description>
    </item>
    
    <item>
      <title>Bootstrap Pit Node From Livecd Usb</title>
      <link>/docs-csm/en-12/install/bootstrap_livecd_usb/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:37 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/bootstrap_livecd_usb/</guid>
      <description>Bootstrap PIT Node from LiveCD USB The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node&amp;ndash;the RemoteISO or a bootable USB device. This procedure describes using the USB device. If not using the USB device, see Bootstrap Pit Node from LiveCD Remote ISO.
There are 5 overall steps that provide a bootable USB with SSH enabled, capable of installing Shasta v1.</description>
    </item>
    
    <item>
      <title>Cable Management Network Servers</title>
      <link>/docs-csm/en-12/install/cable_management_network_servers/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:37 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/cable_management_network_servers/</guid>
      <description>Cable Management Network Servers This topic describes nodes in the air-cooled cabinet with diagrams and pictures showing where to find the ports on the nodes and how to cable the nodes to the management network switches.
 HPE Hardware  HPE DL385 HPE DL325 HPE Worker Node Cabling HPE Master Node Cabling HPE Storage Node Cabling HPE UAN Cabling HPE Apollo 6500 XL645D HPE Apollo 6500 XL675D   Gigabyte/Intel Hardware  Worker Node Cabling Master Node Cabling Storage Node Cabling UAN Cabling    HPE Hardware HPE DL385  The OCP Slot is noted (number 7) in the image above.</description>
    </item>
    
    <item>
      <title>Ceph Csi Troubleshooting</title>
      <link>/docs-csm/en-12/install/ceph_csi_troubleshooting/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:37 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/ceph_csi_troubleshooting/</guid>
      <description>Ceph CSI Troubleshooting If there has been a failure to initialize all Ceph CSI components on ncn-s001, then the storage node cloud-init may need to be rerun.
Topics:  Verify Ceph CSI Rerun Storage Node cloud-init  Details 1. Verify Ceph CSI Verify that the ceph-csi requirements are in place
  Log in to ncn-s00 and run this command
ncn-s001# ceph -s If it returns a connection error then assume Ceph is not installed.</description>
    </item>
    
    <item>
      <title>Aruba Snmp Known Issue</title>
      <link>/docs-csm/en-12/install/aruba_snmp_known_issue_10_06_0010/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:36 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/aruba_snmp_known_issue_10_06_0010/</guid>
      <description>Aruba SNMP Known Issue Affected Devices 8320/8325/8360 Aruba CX
Aruba Defect CR 153440
Aruba Public Documentation/Images Aruba support portal:
 https://asp.arubanetworks.com/
 Aruba 8325 release notes 10.06.0120:
 https://www.arubanetworks.com/techdocs/AOS-CX/10.06/RN/5200-8179.pdf
 Aruba 8360 release notes 10.06.0120:
 https://www.arubanetworks.com/techdocs/AOS-CX/10.06/RN/5200-8180.pdf
 Where the Issue May Occur During Install During initial network discovery of Nodes, SNMP may not accurately report MAC-address from Aruba Leaf/Spine switches. This will lead to a situation where not all connected devices are discovered as expected.</description>
    </item>
    
    <item>
      <title>Boot Livecd Virtual Iso</title>
      <link>/docs-csm/en-12/install/boot_livecd_virtual_iso/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:36 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/boot_livecd_virtual_iso/</guid>
      <description>Boot LiveCD Virtual ISO This page will walk-through booting the LiveCD .iso file directly onto a BMC.
Topics  Boot LiveCD Virtual ISO  Topics Details  Prerequisites BMCs&#39; Virtual Mounts  HPE iLO BMCs Gigabyte BMCs   Configuring  Backing up the Overlay COW FS Restoring from an Overlay COW FS Backup        Details Prerequisites A Cray Pre-Install Toolkit ISO is required for this process.</description>
    </item>
    
    <item>
      <title>Bootstrap Pit Node From Livecd Remote Iso</title>
      <link>/docs-csm/en-12/install/bootstrap_livecd_remote_iso/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:36 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/bootstrap_livecd_remote_iso/</guid>
      <description>Bootstrap PIT Node from LiveCD Remote ISO The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node&amp;ndash;the RemoteISO or a bootable USB device. This procedure describes using the RemoteISO. If not using the RemoteISO, see Bootstrap PIT Node from LiveCD USB
The installation process is similar to the USB based installation with adjustments to account for the lack of removable storage.</description>
    </item>
    
    <item>
      <title>Hotfix To Workaround Known Mac-learning Issue With 8325</title>
      <link>/docs-csm/en-12/install/8325_mac_learning_hotfix/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:35 +0000</pubDate>
      
      <guid>/docs-csm/en-12/install/8325_mac_learning_hotfix/</guid>
      <description>Hotfix to workaround known mac-learning issue with 8325 Issue description  Aruba CR: 90598
Affected platform: 8325
Symptom: MAC learning stops.
Scenario: Under extremely rare DMA stress conditions, anL2 learning thread may timeout and exit preventing future MAC learning.
Workaround: Reboot the switch or monitor the L2 thread and restart it with an NAE script.
Fixed in: 10.06.0130, 10.7.0010 and above.
Aruba release notes
 To fix the issue without upgrading software You can run a NAE script on the 8325 platform switches to resolve mac learning issue.</description>
    </item>
    
    <item>
      <title>Glossary</title>
      <link>/docs-csm/en-12/glossary/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/glossary/</guid>
      <description>Glossary Glossary of terms used in CSM documentation.
 Application Node (AN) Baseboard Management Controller (BMC) Blade Switch Controller (sC) Cabinet Cooling Group Cabinet Environmental Controller (CEC) CEC microcontroller (eC) Customer Access Network Chassis Management Module (CMM) Compute Node (CN) Cray Site Init (CSI) Cray System Management (CSM) EX Compute Cabinet EX TDS Cabinet Fabric Floor Standing CDU Hardware Management Network (HMN) High Speed Network (HSN) Kubernetes NCNs LiveCD Management Cabinet Management Nodes NIC Mezzanine Card (NMC) Node Controller (nC) Node Management Network Non-Compute Node (NCN) Power Distribution Unit (PDU) Pre-Install Toolkit (PIT) node Rack-Mounted CDU Rack System Compute Cabinet Rosetta ASIC Service/IO Cabinet Slingshot Slingshot Blade Switch Slingshot Top of Rack (ToR) Switch Shasta Cabling Diagram (SHCD) Supply/Return Cutoff Valves System Management Network (SMNet) System Management Services (SMS) System Management Services (SMS) nodes Top of Rack Switch Controller (sC-ToR) User Access Instance (UAI) User Access Node (UAN) xname  Application Node (AN) An application node (AN) is an NCN which is not providing management functions for the Cray EX system.</description>
    </item>
    
    <item>
      <title>NCN Packages</title>
      <link>/docs-csm/en-12/background/ncn_packages/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/background/ncn_packages/</guid>
      <description>NCN Packages The management nodes boot from images which have many (RPM) packages installed. Lists of the packages for these images are generated on running nodes. A list of images can be collected by running a zypper command on one of the storage, worker, or master nodes.
Kubernetes Images The Kubernetes image is used to boot the master nodes and worker nodes.
Collection ncn-w002# zypper --disable-repositories se --installed-only | grep i+ | tr -d &amp;#39;|&amp;#39; | awk &amp;#39;{print $2}&amp;#39; The List  SLE_HPC SLE_HPC-release acpid apparmor-profiles arptables at autoyast2 autoyast2-installation base bc bind-utils biosdevname ceph-common cfs-state-reporter cloud-init conntrack-tools cpupower crash cray-cos-release cray-cps-utils cray-dvs-kmp-default cray-dvs-service cray-heartbeat cray-lustre-client cray-lustre-client-devel cray-lustre-client-kmp-default cray-network-config cray-node-identity cray-orca cray-power-button cray-sat-podman craycli-wrapper createrepo_c dnsmasq dosfstools dracut-kiwi-live dracut-metal-mdsquash dump e2fsprogs ebtables emacs ethtool expect fping gdb genders git-core glibc gnuplot gperftools gptfdisk grub2 haproxy hpe-csm-scripts hplip ipmitool iproute2-bash-completion ipset ipvsadm irqbalance java-1_8_0-ibm java-1_8_0-ibm-plugin kdump kdumpid keepalived kernel-mft-mlnx-kmp-default kernel-source kernel-syms kexec-tools kmod-bash-completion kubeadm kubectl kubelet less libcanberra-gtk0 libecpg6 libpq5 libunwind-devel ltrace lvm2 man mariadb mariadb-tools mdadm minicom mlocate nmap numactl open-lldp patterns-base-base pdsh perf perl-MailTools perl-doc pixz podman podman-cni-config postgresql postgresql-contrib postgresql-docs postgresql-server python python-urlgrabber python3-Jinja2 python3-MarkupSafe python3-click python3-colorama python3-curses python3-ipaddr python3-lxml python3-netaddr python3-pexpect python3-pip python3-rpm python3-sip rarpd rasdaemon rsync rsyslog rzsz screen slingshot-network-config smartmontools socat spire-agent squashfs squid strace sudo sysstat systemtap tar tcpdump unixODBC usbutils vim wget wireshark xfsdump yast2-add-on yast2-bootloader yast2-country yast2-network yast2-services-manager yast2-users yum-metadata-parser zip  CEPH The Ceph image is used to boot the utility storage nodes.</description>
    </item>
    
    <item>
      <title>NCN Operating System Releases</title>
      <link>/docs-csm/en-12/background/ncn_operating_system_releases/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/background/ncn_operating_system_releases/</guid>
      <description>NCN Operating System Releases The NCNs define their products per image layer:
 Management node SquashFS images are always SLE_HPC (SuSE High Performance Computing) Utility Storage nodes Ceph Images are always SLE_HPC (SuSE High Performance Computing) with SES (SuSE Enterprise Storage)  The sles-release RPM is uninstalled for NCNs, and instead, the sle_HPC-release RPM is installed. These both provide the same files, but differ for os-release and /etc/product.d/baseproduct.
The ses-release RPM is installed on top of the sle_HPC-release RPM in the Ceph images.</description>
    </item>
    
    <item>
      <title>NCN Networking</title>
      <link>/docs-csm/en-12/background/ncn_networking/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/background/ncn_networking/</guid>
      <description>NCN Networking Non-compute nodes and compute nodes have different network interfaces used for booting, this topic focuses on the network interfaces for management nodes.
Topics:  NCN Network Interfaces Device Naming Vendor and Bus ID Identification  Details NCN Network Interfaces The following table includes information about the different NCN network interfaces:
   Name Type MTU     mgmt0 Port 1 Slot 1 on the SMNET card.</description>
    </item>
    
    <item>
      <title>NCN Mounts And File Systems</title>
      <link>/docs-csm/en-12/background/ncn_mounts_and_file_systems/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/background/ncn_mounts_and_file_systems/</guid>
      <description>NCN Mounts and File Systems The management nodes use drive storage for persistence and block storage. This page outlines reference information for these disks, their partition tables, and their management.
Topics:  What Controls Partitioning? Plan of Record / Baseline  Problems When Above/Below Baseline Worker Nodes with ETCD  Disable Luks Expand the RAID     Disk Layout Quick-Reference Tables OverlayFS and Persistence  OverlayFS Example Persistent Directories  Layering - Upperdir and Lowerdir(s) Layering Real World Example   OverlayFS Control  Reset Toggles Reset On Next Boot Reset on Every Boot Re-sizing the Persistent Overlay Thin Overlay Feature     SystemD MetalFS Old/Retired FS-Labels  Details What Controls Partitioning?</description>
    </item>
    
    <item>
      <title>NCN Images</title>
      <link>/docs-csm/en-12/background/ncn_images/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/background/ncn_images/</guid>
      <description>NCN Images The management nodes boot from NCN images which are created from layers on top of a common base image. The common image is customized with a Kubernetes layer for the master nodes and worker nodes. The common image is customized with a storage-ceph layer for the utility storage nodes..
Topics:  Overview of NCN Images LiveCD Server  Details Overview of NCN Images There are several flavors of NCN images, each share a common base image.</description>
    </item>
    
    <item>
      <title>NCN Boot Workflow</title>
      <link>/docs-csm/en-12/background/ncn_boot_workflow/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/background/ncn_boot_workflow/</guid>
      <description>NCN Boot Workflow Non-compute nodes boot two ways:
 Network/PXE booting Disk Booting  Topics:  Determine the Current Boot Order Reasons to Change the Boot Order After CSM Install Determine if NCNs Booted via Disk or PXE Set BMCs to DHCP Boot Order Overview Setting Order Trimming Boot Order Example Boot Orders Reverting Changes Locating USB Device  Determine the Current Boot Order Under normal operations, the NCNs use the following boot order:</description>
    </item>
    
    <item>
      <title>Non-compute Node Bios</title>
      <link>/docs-csm/en-12/background/ncn_bios/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/background/ncn_bios/</guid>
      <description>Non-Compute Node BIOS This page denotes BIOS settings that are desirable for non-compute nodes.
 NOTE Any tunables in this page are in the interest of performance and stability. If either of those facets seem to be infringed by any of the content on this page, please contact Cray System Management for reconciliation.
  NOTE The table below declares desired settings; unlisted settings should remain at vendor-default. This table may be expanded as new settings are adjusted.</description>
    </item>
    
    <item>
      <title>Certificate Authority</title>
      <link>/docs-csm/en-12/background/certificate_authority/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/background/certificate_authority/</guid>
      <description>Certificate Authority While a system is being installed for the first time, a certificate authority (CA) is needed. This can be generated for a system, or one can be supplied from a customer intermediate CA. Outside of a new installation, there is no supported method to rotate or change the platform CA in this release.
Topics:  Overview Use Default Platform Generated CA Customize Platform Generated CA Use External CA  Overview At install time, a PKI certificate authority (CA) can either be generated for a system, or a customer can opt to supply their own (intermediate) CA.</description>
    </item>
    
    <item>
      <title>Cray System Management - Readme</title>
      <link>/docs-csm/en-12/readme/</link>
      <pubDate>Sat, 13 Nov 2021 03:15:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/readme/</guid>
      <description>Cray System Management (CSM) - README The documentation included here describes how to install or upgrade the Cray System Management (CSM) software and related supporting operational procedures. CSM software is the foundation upon which other software product streams for the HPE Cray EX system depend.
This documentation is in Markdown format. Although much of it can be viewed with any text editor, a richer experience will come from using a tool which can render the Markdown to show different font sizes, the use of bold and italics formatting, inclusion of diagrams and screen shots as image files, and to follow navigational links within a topic file and to other files.</description>
    </item>
    
  </channel>
</rss>
